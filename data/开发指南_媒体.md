# 合并文件
合并时间: 2025-04-29 06:50:09

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/multimedia-hdr-vivid-V14
爬取时间: 2025-04-28 19:45:54
来源: Huawei Developer
HDR Vivid是UWA认证的动态HDR视频标准，在HarmonyOS平台上，开发者能够利用HDR Vivid的特性，开发媒体类应用，为用户呈现高动态范围和广色域的视觉体验。作为新一代高动态范围图像标准，HDR Vivid贯穿内容创作、平台支持和设备显示，为用户带来更宽广的色彩范围、更细腻的层次表现、更显著的明暗对比，以及更智能的动态元数据处理，助力用户领略世界的真实色彩。
HDR Vivid视频
应用只需调用媒体领域提供的API，即可接入HarmonyOS的HDR Vivid视频采集、转码和解码显示功能，基于HDR Vivid标准，制作出高质量的视频。
| 类别  | 开发指导  | 提供能力的Kit  |
| --- | --- | --- |
| 采集  | HDR Vivid相机录像  | Camera Kit  |
| 编码  | HDR Vivid视频录制  | AVCodec Kit  |
| 解码  | HDR Vivid视频播放  | AVCodec Kit  |
| 转换  | 视频解码支持HDRVivid2SDR  | AVCodec Kit  |
| HDR Vivid视频动态元数据生成  | Media Kit  |
| HDR视频色彩空间转换  | Media Kit  |
类别
开发指导
提供能力的Kit
采集
HDR Vivid相机录像
Camera Kit
编码
HDR Vivid视频录制
AVCodec Kit
解码
HDR Vivid视频播放
AVCodec Kit
转换
视频解码支持HDRVivid2SDR
AVCodec Kit
HDR Vivid视频动态元数据生成
Media Kit
HDR视频色彩空间转换
Media Kit
HDR Vivid图片
应用只需调用媒体领域提供的API，即可接入HarmonyOS的HDR Vivid图片采集、转码和解码显示功能，基于HDR Vivid标准，制作出高质量的图片。
| 类别  | 开发指导  | 提供能力的Kit  |
| --- | --- | --- |
| 采集  | HDR Vivid相机拍照  | Camera Kit  |
| 编码  | HDR Vivid图片编码  | Image Kit  |
| 解码  | HDR Vivid图片解码  | Image Kit  |
| 转换  | HDR图片动态元数据生成  | Image Kit  |
| HDR图片色彩空间转换  | Image Kit  |
| 单层HDR图片转换双层  | Image Kit  |
| 双层HDR图片转换单层  | Image Kit  |
类别
开发指导
提供能力的Kit
采集
HDR Vivid相机拍照
Camera Kit
编码
HDR Vivid图片编码
Image Kit
解码
HDR Vivid图片解码
Image Kit
转换
HDR图片动态元数据生成
Image Kit
HDR图片色彩空间转换
Image Kit
单层HDR图片转换双层
Image Kit
双层HDR图片转换单层
Image Kit

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audio-kit-V14
爬取时间: 2025-04-28 19:46:07
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audio-kit-intro-V14
爬取时间: 2025-04-28 19:46:21
来源: Huawei Developer
Audio Kit（音频服务）旨在提供场景化的音频播放和录制接口，助力开发者迅速构建音频高清采集及沉浸式播放能力。
亮点/特征
-  低时延播放 提供统一音频低时延/非低时延播放能力接口，通过垂直打通硬件，达成最低的音频输出时延。在游戏、提示/告警音、K歌等场景下，可以通过低时延接口，实现音频快速流畅播放。
-  音效模式 提供系统音效模式设置，应用可以按需开/关系统音效，确保最佳音效输出体验。 系统默认为音乐、听书、影院等不同场景进行相应音效处理，但应用内部自身也存在一些定制化音效，为确保最终音效不产生冲突，系统提供音效模式配置开关，允许应用按需开/关系统音效。
-  音振协同 提供音振协同能力接口，实现音频及振动流的低时延同步控制。达成在输入法中开启音频和振动效果，打字输入时音振协同、节奏一致，来电铃声和振动同时响起，铃音和振动节奏同步一致的体验。
开发说明
在每个功能中，会介绍多种实现方式以应对不同的使用场景，以及该场景相关的子功能点。比如在音频播放功能内，会同时介绍音频的并发策略、音量管理和输出设备等在操作系统中的处理方式，帮助开发者能够开发出功能覆盖更全面的应用。
本开发指导仅针对音频播放或录制本身，Audio Kit提供相关能力，不涉及UI界面、图形处理、媒体存储或其他相关领域功能。
在开发音频功能之前，尤其是要实现处理音频数据的功能前，建议开发者先了解声学相关的知识，帮助理解操作系统提供的API是如何控制音频系统，从而开发出更易用、体验更好的音视频类应用。建议了解的相关概念包括但不限于：
-  音频量化的过程：采样 > 量化 > 编码
-  音频量化过程的相关概念：模拟信号和数字信号、采样率、声道、采样格式、位宽、码率、常见编码格式（如AAC、MP3、PCM、WMA等）、常见封装格式（如WAV、MPA、FLAC、AAC、OGG等）
音频流介绍
在开发音频应用之前，还需要了解什么是音频流，它是HarmonyOS音频系统中的关键概念，在之后的章节中会多次提及。
音频流，是指音频系统中一个具备音频格式和音频使用场景信息的独立音频数据处理单元。可以表示播放，也可以表示录制，并且具备独立音量调节和音频设备路由切换能力。
音频流基础信息通过AudioStreamInfo表示，包含采样、声道、位宽、编码信息，是创建音频播放或录制流的必要参数，描述了音频数据的基本属性。在配置时开发者需要保证基础信息与传输的音频数据相匹配，音频系统才能正确处理数据。
音频流使用场景信息
除了基本属性，音频流还需要具备使用场景信息。基础信息只能对音频数据进行描述，但在实际的使用过程中，不同的音频流，在音量大小、设备路由、并发策略上是有区别的。系统就是通过音频流所附带的使用场景信息，为不同的音频流制定合适的处理策略，以达到更好的音频用户体验。
-  播放场景 音频播放场景的信息，通过StreamUsage进行描述。 StreamUsage指音频流本身的用途类型，包括媒体、语音通信、语音播报、通知、铃声等。
-  录制场景 音频流录制场景的信息，通过SourceType进行描述。 SourceType指音频流中录音源的类型，包括麦克风音频源、语音识别音频源、语音通话音频源等。
可参考使用合适的音频流类型进行设置。
支持的音频格式
audio模块下的接口支持PCM编码，包括AudioRenderer、AudioCapturer、TonePlayer、OpenSL ES等。
音频格式说明：
-  支持的常用的音频采样率（Hz）：8000、11025、12000、16000、22050、24000、32000、44100、48000、64000、8820012+、96000，17640012+、19200012+具体参考枚举AudioSamplingRate。 不同设备支持的采样率规格会存在差异。
-  支持单声道、双声道，具体参考AudioChannel。
-  支持的采样格式：U8（无符号8位整数）、S16LE（带符号的16位整数，小尾数）、S24LE（带符号的24位整数，小尾数）、S32LE（带符号的32位整数，小尾数）、F32LE（带符号的32位浮点数，小尾数），具体参考AudioSampleFormat。 由于系统限制，S24LE、S32LE、F32LE仅部分设备支持，请根据实际情况使用。 小尾数指的是小端模式，即数据的高字节保存在内存的高地址中，而数据的低字节保存在内存的低地址中。这种存储模式将地址的高低和数据的位权有效结合起来，高地址部分权值高，低地址部分权值低。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/using-right-streamusage-and-sourcetype-V14
爬取时间: 2025-04-28 19:46:34
来源: Huawei Developer
音频流类型是定义音频数据播放和录制方式的关键属性。对于播放流，其类型由StreamUsage确定；对于录制流，则由SourceType决定。音频流类型对音量控制、音频焦点管理以及输入/输出设备的选择具有决定性影响。
为了确保音频行为符合预期并提供优质的用户体验，应用开发者应根据具体业务场景和实际需求，为音频选择恰当的流类型。
接下来，文档将介绍常用的音频流类型及其适用场景，同时说明不同流类型对音频业务的影响。最后，指导开发者在采用不同方法实现音频播放和音频录制时，应当如何设置音频流类型。
常用的音频流类型及其适用场景
播放音频流类型
下表中列举常用的播放音频流类型，由StreamUsage定义。
| 音频流使用类型（StreamUsage） | 适用场景 |
| --- | --- |
| STREAM_USAGE_MUSIC | 适用于播放音乐，同样适用于其他媒体场景，如使用SoundPool播放简短音效等。 |
| STREAM_USAGE_MOVIE | 适用于播放短视频、电影、电视剧等各类视频内容。 |
| STREAM_USAGE_AUDIOBOOK | 适用于播放有声读物、新闻、播客等。 |
| STREAM_USAGE_GAME | 适用于游戏内配乐、配音，后台音乐不会被打断；游戏内语音，建议使用STREAM_USAGE_VOICE_COMMUNICATION。 |
| STREAM_USAGE_NAVIGATION | 适用于导航场景的语音播报功能。 |
| STREAM_USAGE_VOICE_MESSAGE | 适用于播放语音短消息。 |
| STREAM_USAGE_VOICE_COMMUNICATION | 适用于VoIP语音通话。 |
| STREAM_USAGE_ALARM | 适用于播放闹铃。 |
| STREAM_USAGE_RINGTONE | 适用于VoIP来电响铃等。 |
| STREAM_USAGE_NOTIFICATION | 适用于播放通知音、提示音。 |
录制音频流类型
下表中列举常用的录制音频流类型，由SourceType定义。
| 音频流使用类型（StreamUsage） | 适用场景 |
| --- | --- |
| SOURCE_TYPE_MIC | 适用于普通录音。 |
| SOURCE_TYPE_VOICE_COMMUNICATION | 适用于VoIP语音通话。 |
| SOURCE_TYPE_VOICE_MESSAGE | 适用于录制语音短消息。 |
流类型对音频业务的影响
不同的流类型会影响用户在控制音量时的体验，以及系统在调整音频焦点和选择输入/输出设备时的表现。
音量控制
播放流类型（StreamUsage）决定了音频流所属的音量类型（AudioVolumeType），各类音量类型（如媒体、铃声、闹钟、通话等）拥有独立的音量值，在用户界面上可独立调节，相互之间不会影响。
常见的播放流类型与音量类型的对应关系为：
| 音频流使用类型（StreamUsage） | 音量类型（AudioVolumeType） |
| --- | --- |
| MUSIC、MOVIE、AUDIOBOOK、GAME | 媒体音量（MEDIA） |
| RINGTONE、NOTIFICATION | 铃声音量（RINGTONE） |
| VOICE_COMMUNICATION | 通话音量（VOICE_CALL） |
| ALARM | 闹钟音量（ALARM） |
音频焦点调整
音频流类型在音频焦点管理中扮演着关键角色，不同类型的音频流具有不同的默认优先级和处理方式。
当应用启动音频播放或录制时，系统会根据音频流类型自动申请焦点，这可能会中断其他音频或降低其音量。音频焦点的具体介绍可参考音频焦点和音频会话介绍。
此处仅说明常见的音频流类型影响音频焦点的表现。
-  启动导航（Navigation）时，正在播放的音乐（Music）音量会自动调低，待导航（Navigation）结束后，音乐（Music）音量将自动恢复。
-  开始播放视频（Movie）时，将会停止正在播放的音乐（Music）；当视频（Movie）播放结束后，音乐（Music）播放不会自动恢复，对应的应用也不会收到任何恢复通知。
-  开始语音通话（VoiceCommunication）时，将会暂停正在播放的音乐（Music）；当语音通话（VoiceCommunication）结束后，播放音乐（Music）的应用将收到恢复播放的通知。
-  音乐（Music）和游戏音频（Game）可以混音播放，两者互不影响。
-  开始录制语音短消息（VoiceMessage）时，会自动暂停正在播放的音乐（Music）；当语音短消息（VoiceMessage）录制结束后， 播放音乐（Music）的应用将收到恢复播放的通知。
输入/输出设备选择
对于不同类型的音频流，系统会为其选定相应的输入/输出设备。
此处仅说明常见的音频流类型对应的输入/输出设备。
-  音乐（Music）类型音频流的默认输出设备为扬声器。
-  语音通话（VoiceCommunication）类型音频流的默认输入设备为麦克风，默认输出设备为听筒。
-  闹铃（Alarm）类型音频流的默认输出设备为扬声器‌。若先连接蓝牙耳机，再开始播放Alarm音频，则扬声器和蓝牙耳机会同时播放。
若默认的输入/输出设备不符合使用诉求，应用也可以调用相关接口主动修改。应用使用AudioRenderer开发音频播放功能时，可以调用setDefaultOutputDevice接口，设置默认发声设备。
设置音频流类型
应用可采用多种方法实现音频播放或录音功能，因此，设置音频流类型的方式也各不相同。
常见的设置播放音频流类型的方法有：
-  使用AudioRenderer开发音频播放功能： 可以在调用createAudioRenderer以获取音频渲染器时，传入对应的StreamUsage。 createAudioRenderer的参数options类型为AudioRendererOptions，包含AudioRendererInfo渲染器信息，使用AudioRendererInfo.usage可指定StreamUsage音频流类型。
-  使用OHAudio开发音频播放功能： 可以在调用OH_AudioStreamBuilder_SetRendererInfo接口时，传入对应的OH_AudioStream_Usage指定音频流类型。
-  使用AVPlayer开发音频播放功能(ArkTS)： 可以通过设置AVPlayer的属性audioRendererInfo来实现。AVPlayer.audioRendererInfo的类型为audio.AudioRendererInfo。使用AudioRendererInfo.usage可指定StreamUsage音频流类型。 在设置AVPlayer的audioRendererInfo属性时，只允许在initialized状态下设置。 如果应用未主动设置该属性，AVPlayer将进行默认处理。当媒体源包含视频时，usage的默认值为STREAM_USAGE_MOVIE；否则，usage的默认值为STREAM_USAGE_MUSIC。
-  使用AVPlayer开发音频播放功能(C/C++)： 可以在调用OH_AVPlayer_SetAudioRendererInfo接口时，传入对应的OH_AudioStream_Usage指定音频流类型。
-  使用SoundPool开发音频播放功能： 可以在调用createSoundPool接口时，传入对应的StreamUsage指定音频流类型。
常见的设置录制音频流类型的方法有：
-  使用AudioCapturer开发音频录制功能： 可以在调用createAudioCapturer接口时，传入对应的SourceType。 createAudioCapturer的参数options类型为AudioCapturerOptions，包含AudioCapturerInfo采集器信息，使用AudioCapturerInfo.source可指定SourceType音源类型。
-  使用OHAudio开发音频录制功能： 可以在调用OH_AudioStreamBuilder_SetCapturerInfo接口时，传入对应的OH_AudioStream_SourceType指定音源类型。
-  使用AVRecorder开发音频录制功能： 可以在调用AVRecorder.prepare接口时，传入对应的AudioSourceType。 AVRecorder.prepare的参数config类型为AVRecorderConfig，使用AVRecorderConfig.audioSourceType可指定音源类型。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audio-playback-concurrency-V14
爬取时间: 2025-04-28 19:46:48
来源: Huawei Developer
在应用播放或录制声音时，常出现与其他音频流的并发或中断情况，这对用户体验构成显著影响。例如，当应用启动视频播放时，若后台正在播放音乐，用户会期望音乐能自动暂停，以确保视频音频优先播放，这正是音频焦点功能的体现。对于涉及音频服务的应用而言，妥善地管理音频焦点非常重要，它可以显著提升用户的音频体验。
本文档将介绍系统的音频焦点策略，以及应用如何申请、释放音频焦点，以及应对焦点变化的方法。
同时，系统提供了音频会话（AudioSession）机制，允许应用自定义其音频流的焦点策略。在系统进行焦点管理时，只要条件允许，本应用的所有音频流将优先遵循这一策略。
音频焦点
系统预设了默认的音频焦点策略，根据音频流的类型及启动的先后顺序，对所有播放和录制音频流进行统一管理。
在启动播放或录制功能前，应用需要先申请音频焦点；而在播放或录制结束后，应适时释放音频焦点。在播放或录制的过程中，可能会因其他音频流的介入而失去焦点，此时，应用需依据焦点变化采取相应措施。
对于应用而言，为了确保为用户提供优质的音频焦点体验，应当注意以下几点：
-  在启动播放或录制操作前，应根据音频的具体用途，选择并使用合适的音频流类型，即准确设置StreamUsage或SourceType。
-  在播放或录制的过程中，需监听音频焦点事件，并在接收到音频焦点中断事件（InterruptEvent）时，采取相应的处理措施。
-  若应用程序有意主动管理音频焦点，可使用音频会话（AudioSession）相关的接口进行操作。
申请音频焦点
当应用开始播放或录制音频时，系统将自动为相应的音频流申请音频焦点。
例如，应用使用AudioRenderer开发音频播放功能，当调用AudioRenderer的start时，系统会自动为应用请求音频焦点。
若音频焦点请求成功，音频流将正常启动；反之，若音频焦点请求被拒绝，音频流将无法开始播放或录制。
建议应用主动监听音频焦点事件，一旦音频焦点请求被拒绝，应用将接收到音频焦点事件（InterruptEvent）。
特殊场景：
1.  短音播放：若应用使用SoundPool开发音频播放功能，且StreamUsage指定为Music、Movie、AudioBook等类型，播放短音，则其申请焦点时默认为并发模式，不会影响其他音频。
2.  静音播放：若应用以静音状态开始播放音频（或视频），并且希望静音阶段不影响其他音频，当后续解除静音的时候，再以正常策略申请音频焦点，则可以调用静音并发播放模式的相关接口。具体可参考： 使用AVPlayer开发音频播放功能，可以调用setMediaMuted函数。 使用AudioRenderer开发音频播放功能，可调用setSilentModeAndMixWithOthers函数。 使用OHAudio开发音频播放功能，可调用OH_AudioRenderer_SetSilentModeAndMixWithOthers函数。
3.  使用AVPlayer开发音频播放功能，可以调用setMediaMuted函数。
4.  使用AudioRenderer开发音频播放功能，可调用setSilentModeAndMixWithOthers函数。
5.  使用OHAudio开发音频播放功能，可调用OH_AudioRenderer_SetSilentModeAndMixWithOthers函数。
-  使用AVPlayer开发音频播放功能，可以调用setMediaMuted函数。
-  使用AudioRenderer开发音频播放功能，可调用setSilentModeAndMixWithOthers函数。
-  使用OHAudio开发音频播放功能，可调用OH_AudioRenderer_SetSilentModeAndMixWithOthers函数。
释放音频焦点
当应用结束播放或录制音频时，系统会自动为相应的音频流释放音频焦点。
例如，应用使用AudioRenderer开发音频播放功能，当调用AudioRenderer的pause、stop、release等时，系统会为其释放音频焦点。
当音频流释放音频焦点时，若存在受其影响的其他音频流（如音量被调低或被暂停的流），将触发恢复操作。
若应用不希望在音频流停止时立即释放音频焦点，可使用音频会话（AudioSession）的相关接口，实现音频焦点释放的延迟效果。
音频焦点策略
当音频流申请或释放音频焦点时，系统依据音频焦点策略，对所有音频流（包括播放和录制）实施焦点管理，决定哪些音频流可正常运行，哪些需被打断或执行其他操作。
系统预设的默认音频焦点策略，主要依据音频流类型（即播放流的StreamUsage和录制流的SourceType）及音频流启动的顺序进行决策。
为防止焦点变化不符合预期，应用在启动播放或录制前，应根据音频流的用途，准确设置StreamUsage或SourceType。关于各类型的详细说明，请参考使用合适的音频流类型。
常见的音频焦点场景示例如下：
若默认的音频焦点策略无法满足特定场景的需求，应用程序可利用音频会话（AudioSession），调整本应用音频流所采用的音频焦点策略。
焦点模式
针对同一应用创建的多个音频流，应用可通过设置焦点模式（InterruptMode），选择由应用自主管控，或由系统统一管理。
系统预设了两种焦点模式：
-  共享焦点模式（SHARE_MODE）：同一应用创建的多个音频流共享一个音频焦点。这些音频流之间的并发规则由应用自行决定，音频焦点策略不会介入。仅当其他应用创建的音频流与该应用的音频流同时播放时，才会触发音频焦点策略的管理。
-  独立焦点模式（INDEPENDENT_MODE）：应用创建的每个音频流均独立拥有一个音频焦点，多个音频流同时播放时，将触发音频焦点策略的管理。
应用可根据需求选择合适的焦点模式。在创建音频流时，系统默认采用共享焦点模式（SHARE_MODE），应用可主动设置所需模式。
设置焦点模式的方法：
-  若使用AVPlayer开发音频播放功能(ArkTS)，则可以通过修改AVPlayer的audioInterruptMode属性进行设置。
-  若使用AVPlayer开发音频播放功能(C/C++)，则可以调用OH_AVPlayer_SetAudioInterruptMode函数进行设置。
-  若使用AudioRenderer开发音频播放功能，则可以调用setInterruptMode函数进行设置。
-  若使用OHAudio开发音频播放功能(C/C++)，则可以调用OH_AudioStreamBuilder_SetRendererInterruptMode函数进行设置。
处理音频焦点变化
在应用播放或录制音频的过程中，若有其他音频流申请焦点，系统会根据焦点策略进行焦点处理。若判定本音频流的焦点有变化，需要执行暂停、继续、降低音量、恢复音量等操作，则系统会自动执行一些必要的操作，并通过音频焦点事件（InterruptEvent）通知应用。
因此，为了维持应用和系统的状态一致性，保证良好的用户体验，推荐应用监听音频焦点事件，并在焦点发生变化时，根据InterruptEvent做出必要的响应。
使用不同方式开发时，如何监听音频焦点事件：
-  若使用AVPlayer开发音频播放功能(ArkTS)，可以调用on('audioInterrupt')接口，监听音频焦点事件InterruptEvent。
-  若使用AVPlayer开发音频播放功能(C/C++)，可以调用OH_AVPlayer_SetOnInfoCallback()接口，监听音频焦点事件OH_AVPlayerOnInfoCallback。
-  若使用AudioRenderer开发音频播放功能，可以调用on('audioInterrupt')接口，监听音频焦点事件InterruptEvent。
-  若使用OHAudio开发音频播放功能(C/C++)，可以调用OH_AudioStreamBuilder_SetRendererCallback接口，监听音频焦点事件OH_AudioRenderer_OnInterruptEvent。
-  若使用AudioCapturer开发音频录制功能，可以调用on('audioInterrupt')接口，监听音频焦点事件InterruptEvent。
-  若使用OHAudio开发音频录制功能(C/C++)，可以调用OH_AudioStreamBuilder_SetCapturerCallback接口，监听音频焦点事件OH_AudioCapturer_OnInterruptEvent。
应用在收到音频焦点事件（InterruptEvent）时，需要根据其中信息，做出相应的处理，以保持应用与系统状态一致，带给用户良好的音频体验。
在音频焦点事件中，应用应重点关注两个信息：打断类型（InterruptForceType）和打断提示（InterruptHint）。
-  打断类型（InterruptForceType）： InterruptForceType参数提示应用该焦点变化是否已由系统强制操作： 强制打断类型（INTERRUPT_FORCE）：由系统进行操作，强制执行。应用需要做一些必要的处理，例如更新状态、更新界面显示等。 共享打断类型（INTERRUPT_SHARE）：由应用进行操作，应用可以选择响应或忽略，系统不会干涉。 系统默认优先采用强制打断类型（INTERRUPT_FORCE），应用无法更改。 对于一些系统无法强制执行的操作（例如INTERRUPT_HINT_RESUME），会采用共享打断类型（INTERRUPT_SHARE）。
-  强制打断类型（INTERRUPT_FORCE）：由系统进行操作，强制执行。应用需要做一些必要的处理，例如更新状态、更新界面显示等。
-  共享打断类型（INTERRUPT_SHARE）：由应用进行操作，应用可以选择响应或忽略，系统不会干涉。
-  打断提示（InterruptHint）： InterruptHint参数用于提示应用音频流的状态： 继续（INTERRUPT_HINT_RESUME）：音频流可恢复播放或录制，仅会接收到PAUSE（暂停提示）之后收到。 此操作无法由系统强制执行，其对应的InterruptForceType一定为INTERRUPT_SHARE类型。 暂停（INTERRUPT_HINT_PAUSE）：音频暂停，暂时失去音频焦点。后续待焦点可用时，会再收到INTERRUPT_HINT_RESUME。 停止（INTERRUPT_HINT_STOP）：音频停止，彻底失去音频焦点。 降低音量（INTERRUPT_HINT_DUCK）：音频降低音量播放，而不会停止。默认降低至正常音量的20%。 恢复音量（INTERRUPT_HINT_UNDUCK）：音频恢复正常音量。
-  继续（INTERRUPT_HINT_RESUME）：音频流可恢复播放或录制，仅会接收到PAUSE（暂停提示）之后收到。 此操作无法由系统强制执行，其对应的InterruptForceType一定为INTERRUPT_SHARE类型。
-  暂停（INTERRUPT_HINT_PAUSE）：音频暂停，暂时失去音频焦点。后续待焦点可用时，会再收到INTERRUPT_HINT_RESUME。
-  停止（INTERRUPT_HINT_STOP）：音频停止，彻底失去音频焦点。
-  降低音量（INTERRUPT_HINT_DUCK）：音频降低音量播放，而不会停止。默认降低至正常音量的20%。
-  恢复音量（INTERRUPT_HINT_UNDUCK）：音频恢复正常音量。
-  强制打断类型（INTERRUPT_FORCE）：由系统进行操作，强制执行。应用需要做一些必要的处理，例如更新状态、更新界面显示等。
-  共享打断类型（INTERRUPT_SHARE）：由应用进行操作，应用可以选择响应或忽略，系统不会干涉。
-  继续（INTERRUPT_HINT_RESUME）：音频流可恢复播放或录制，仅会接收到PAUSE（暂停提示）之后收到。 此操作无法由系统强制执行，其对应的InterruptForceType一定为INTERRUPT_SHARE类型。
-  暂停（INTERRUPT_HINT_PAUSE）：音频暂停，暂时失去音频焦点。后续待焦点可用时，会再收到INTERRUPT_HINT_RESUME。
-  停止（INTERRUPT_HINT_STOP）：音频停止，彻底失去音频焦点。
-  降低音量（INTERRUPT_HINT_DUCK）：音频降低音量播放，而不会停止。默认降低至正常音量的20%。
-  恢复音量（INTERRUPT_HINT_UNDUCK）：音频恢复正常音量。
处理音频焦点示例:
为了带给用户更好的音频体验，针对不同的音频焦点事件内容，应用需要做出相应的处理操作。此处以使用AudioRenderer开发音频播放功能为例，展示推荐应用采取的处理方法，提供伪代码供开发者参考。
若使用其他接口开发音频播放或音频录制功能，处理方法类似，具体的代码实现，开发者可结合实际情况编写，处理方法也可自行调整。
```typescript
import { audio } from '@kit.AudioKit';  // 导入audio模块
import { BusinessError } from '@kit.BasicServicesKit'; // 导入BusinessError
let isPlay: boolean; // 是否正在播放，实际开发中，对应与音频播放状态相关的模块
let isDucked: boolean; //是否降低音量，实际开发中，对应与音频音量相关的模块
let started: boolean; // 标识符，记录“开始播放（start）”操作是否成功
async function onAudioInterrupt(): Promise<void> {
// 此处以使用AudioRenderer开发音频播放功能举例，变量audioRenderer即为播放时创建的AudioRenderer实例。
audioRenderer.on('audioInterrupt', async(interruptEvent: audio.InterruptEvent) => {
// 在发生音频焦点变化时，audioRenderer收到interruptEvent回调，此处根据其内容做相应处理
// 1. 可选：读取interruptEvent.forceType的类型，判断系统是否已强制执行相应操作。
// 注：默认焦点策略下，INTERRUPT_HINT_RESUME为INTERRUPT_SHARE类型，其余hintType均为INTERRUPT_FORCE类型。因此对forceType可不做判断。
// 2. 必选：读取interruptEvent.hintType的类型，做出相应的处理。
if (interruptEvent.forceType === audio.InterruptForceType.INTERRUPT_FORCE) {
// 强制打断类型（INTERRUPT_FORCE）：音频相关处理已由系统执行，应用需更新自身状态，做相应调整
switch (interruptEvent.hintType) {
case audio.InterruptHint.INTERRUPT_HINT_PAUSE:
// 此分支表示系统已将音频流暂停（临时失去焦点），为保持状态一致，应用需切换至音频暂停状态
// 临时失去焦点：待其他音频流释放音频焦点后，本音频流会收到resume对应的音频焦点事件，到时可自行继续播放
isPlay = false; // 此句为简化处理，代表应用切换至音频暂停状态的若干操作
break;
case audio.InterruptHint.INTERRUPT_HINT_STOP:
// 此分支表示系统已将音频流停止（永久失去焦点），为保持状态一致，应用需切换至音频暂停状态
// 永久失去焦点：后续不会再收到任何音频焦点事件，若想恢复播放，需要用户主动触发。
isPlay = false; // 此句为简化处理，代表应用切换至音频暂停状态的若干操作
break;
case audio.InterruptHint.INTERRUPT_HINT_DUCK:
// 此分支表示系统已将音频音量降低（默认降到正常音量的20%）
isDucked = true; // 此句为简化处理，代表应用切换至降低音量播放状态的若干操作
break;
case audio.InterruptHint.INTERRUPT_HINT_UNDUCK:
// 此分支表示系统已将音频音量恢复正常
isDucked = false; // 此句为简化处理，代表应用切换至正常音量播放状态的若干操作
break;
default:
break;
}
} else if (interruptEvent.forceType === audio.InterruptForceType.INTERRUPT_SHARE) {
// 共享打断类型（INTERRUPT_SHARE）：应用可自主选择执行相关操作或忽略音频焦点事件
switch (interruptEvent.hintType) {
case audio.InterruptHint.INTERRUPT_HINT_RESUME:
// 此分支表示临时失去焦点后被暂停的音频流此时可以继续播放，建议应用继续播放，切换至音频播放状态
// 若应用此时不想继续播放，可以忽略此音频焦点事件，不进行处理即可
// 继续播放，此处主动执行start()，以标识符变量started记录start()的执行结果
await audioRenderer.start().then(() => {
started = true; // start()执行成功
}).catch((err: BusinessError) => {
started = false; // start()执行失败
});
// 若start()执行成功，则切换至音频播放状态
if (started) {
isPlay = true; // 此句为简化处理，代表应用切换至音频播放状态的若干操作
} else {
// 音频继续播放的操作执行失败
}
break;
default:
break;
}
}
});
}
```
使用AudioSession管理音频焦点
应用可以使用音频会话（AudioSession）的相关接口，自定义本应用音频流的焦点策略。在系统进行焦点管理时，只要条件允许，本应用的所有音频流将优先遵循这一策略。
使用音频会话（AudioSession）相关接口，主要可以实现以下功能：
-  应用激活音频会话（AudioSession）并指定音频会话策略（AudioSessionStrategy）后，本应用的所有音频流在参与焦点管理时，会优先使用该策略。 典型场景：应用播放短视频时，会打断后台音乐，应用希望自身的音频流停止后，后台的音乐可以自动恢复。
-  音频会话（AudioSession）处于激活状态下，本应用的音频流全部停止时，不会立刻释放音频焦点，系统会保持音频焦点，直到音频会话停用时再释放音频焦点，或是直到该应用有新的音频流申请焦点。 典型场景：应用连续播放多个音频时，在多个音频衔接的间隙，不希望后台被影响的其他音频自动恢复，希望整个播放过程保持音频焦点的连贯性。
音频会话（AudioSession）使用流程
音频会话（AudioSession）使用流程示意图：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165926.56219404174063616502742410794413:50001231000000:2800:9D9071F020A91166E857FEBCEC896135E13E010E1CDFCAB35F46D1FDB8B4BA4A.png)
1.  音频业务开始之前，需要先获取AudioSessionManager实例。 具体方法可参考获取音频会话管理器(ArkTS)或获取音频会话管理器(C/C++)。
2.  在音频业务开始前，还需要激活当前应用的AudioSession，并根据实际需要指定AudioSessionStrategy。 具体方法可参考激活音频会话(ArkTS)或激活音频会话(C/C++)。 AudioSession处于激活状态时，有以下特性：
3.  应用正常开始播放、录制等音频业务。系统会在音频流开始时，申请音频焦点。本应用的所有音频流在参与焦点处理时，会优先使用AudioSession指定的策略。
4.  音频业务结束之后，停用AudioSession。系统会在音频流停止且AudioSession停用时，释放音频焦点。 应用需要在音频业务结束之后，主动停用AudioSession。 应用在停用AudioSession时，如果该应用的所有音频流已全部停止（即处于保持焦点的静默等待状态），则会立刻释放音频焦点；如果该应用仍有音频流在运行，则它的音频流仍然会持有焦点，直到音频流停止时才释放。 具体方法可参考停用音频会话(ArkTS)或停用音频会话(C/C++)。
在使用AudioSession的过程中，推荐应用监听AudioSession停用事件，当AudioSession被停用时，应用可以及时收到AudioSession停用事件。
音频会话策略（AudioSessionStrategy）
应用在激活AudioSession时，需指定音频会话策略（AudioSessionStrategy），其中包含音频并发模式（AudioConcurrencyMode）参数，用于声明不同的音频并发策略。
系统预设了以下四种音频并发模式：
-  默认模式（CONCURRENCY_DEFAULT）：即系统默认的音频焦点策略。
-  并发模式（CONCURRENCY_MIX_WITH_OTHERS）：和其它音频流并发。
-  降低音量模式（CONCURRENCY_DUCK_OTHERS）：和其他音频流并发，并且降低其他音频流的音量。
-  暂停模式（CONCURRENCY_PAUSE_OTHERS）：暂停其他音频流，待释放焦点后通知其他音频流恢复。
当应用通过AudioSession使用上述各种模式时，系统将尽量满足其焦点策略，但在所有场景下可能无法保证完全满足。
如使用CONCURRENCY_PAUSE_OTHERS模式时，Movie流申请音频焦点，如果Music流正在播放，则Music流会被暂停。但是如果VoiceCommunication流正在播放，则VoiceCommunication流不会被暂停。
监听AudioSession停用事件
应用在使用AudioSession的过程中，推荐应用监听音频会话停用事件（AudioSessionDeactivatedEvent）。当AudioSession被停用（非主动停用）时，应用会收到此事件通知。应用可根据自身业务需求，做相应的处理，例如释放相应资源、重新激活AudioSession等。
具体方法可参考音频会话管理(ArkTS)或音频会话管理(C/C++)。
音频会话停用事件（AudioSessionDeactivatedEvent）包含参数音频会话停用原因（AudioSessionDeactivatedReason），该参数表示AudioSession被停用的原因，主要有两种：
-  应用焦点被抢占（DEACTIVATED_LOWER_PRIORITY）：该应用所有的音频流全部被其他音频流打断，丢失焦点，AudioSession被同时停用。
-  超时（DEACTIVATED_TIMEOUT）：若AudioSession处于激活状态，但该应用没有音频流在运行状态，则AudioSession会在1分钟之后被超时停用。 当AudioSession因超时而停用时，被其压低音量（Duck）的音频会触发恢复音量（Unduck）操作，被其暂停（Pause）的音频流会触发停止（Stop）操作。
典型场景
以下列举一些典型的焦点适配场景。
| 先播放的音频类型 | 推荐流类型 | 后播放的音频类型 | 推荐流类型 | 推荐体验 | 适配方案 |
| --- | --- | --- | --- | --- | --- |
| 音乐 | STREAM_USAGE_MUSIC | 音乐 | STREAM_USAGE_MUSIC | 后播音乐正常播放，先播音乐停止播放，UI变成停止播放状态。 | 先播音乐应用注册焦点事件监听，接收到INTERRUPT_HINT_STOP事件时，停止音乐播放，并更新UI界面。 |
| 音乐 | STREAM_USAGE_MUSIC | 导航 | STREAM_USAGE_NAVIGATION | 导航正常播放，音乐降低音量播放。当导航结束后，音乐恢复正常音量。 | 音乐应用注册焦点事件监听，接收到INTERRUPT_HINT_DUCK和INTERRUPT_HINT_UNDUCK事件时，可以选择更新UI界面。 |
| 视频 | STREAM_USAGE_MOVIE | 闹铃 | STREAM_USAGE_ALARM | 闹铃响起后，视频暂停播放；闹钟结束后，视频继续播放。 | 视频应用注册焦点事件监听，接收到INTERRUPT_HINT_PAUSE事件时，直接暂停视频播放，并更新UI界面。当闹铃结束后，视频应用接收到INTERRUPT_HINT_RESUME事件，重新启动播放。 |
| 音乐 | STREAM_USAGE_MUSIC | 来电铃声 | STREAM_USAGE_RINGTONE | 开始响铃，音乐暂停播放；不接通或者接通再挂断后，音乐恢复播放。 | 音乐应用注册焦点事件监听，接收到INTERRUPT_HINT_PAUSE事件时，直接暂停音乐播放，并更新UI界面。当电话结束后，音频应用接收到INTERRUPT_HINT_RESUME事件，重新启动播放。 |
| 音乐 | STREAM_USAGE_MUSIC | VoIP通话 | STREAM_USAGE_VOICE_COMMUNICATION | 通话接通时，音乐暂停播放；通话挂断后，音乐恢复播放。 | 音乐应用注册焦点事件监听，接收到INTERRUPT_HINT_PAUSE事件时，直接暂停音乐播放，并更新UI界面。当通话结束后，音乐应用接收到INTERRUPT_HINT_RESUME事件，重新启动播放。 |

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audio-session-V14
爬取时间: 2025-04-28 19:47:02
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audio-session-management-V14
爬取时间: 2025-04-28 19:47:55
来源: Huawei Developer
对于涉及多个音频流并发播放的场景，系统已预设了默认的音频焦点策略，该策略将对所有音频流（包括播放和录制）实施统一的焦点管理。
应用可利用音频会话管理（AudioSessionManager）提供的接口，通过AudioSession主动管理应用内音频流的焦点，自定义本应用音频流的焦点策略，调整本应用音频流释放音频焦点的时机，从而贴合应用特定的使用需求。
本文档主要介绍AudioSession相关ArkTS API的使用方法和注意事项，更多音频焦点及音频会话的信息，可参考：音频焦点和音频会话介绍。
获取音频会话管理器
创建AudioSessionManager实例。在使用AudioSessionManager的API前，需要先通过getSessionManager创建实例。
```typescript
import { audio } from '@kit.AudioKit';
let audioSessionManager: audio.AudioSessionManager = audioManager.getSessionManager();
```
激活音频会话
应用可以通过AudioSessionManager.activateAudioSession接口激活当前应用的音频会话。
应用在激活AudioSession时，需指定音频会话策略（AudioSessionStrategy）。策略中包含参数concurrencyMode，其类型为AudioConcurrencyMode，用于声明音频并发策略。
```typescript
import { audio } from '@kit.AudioKit';
import { BusinessError } from '@kit.BasicServicesKit';
let strategy: audio.AudioSessionStrategy = {
concurrencyMode: audio.AudioConcurrencyMode.CONCURRENCY_MIX_WITH_OTHERS
};
audioSessionManager.activateAudioSession(strategy).then(() => {
console.info('activateAudioSession SUCCESS');
}).catch((err: BusinessError) => {
console.error(`ERROR: ${err}`);
});
```
查询音频会话是否已激活
应用可以通过isAudioSessionActivated接口检查当前应用的音频会话是否已激活。
```typescript
let isActivated = audioSessionManager.isAudioSessionActivated();
```
停用音频会话
应用可以通过deactivateAudioSession接口停用当前应用的音频会话。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
audioSessionManager.deactivateAudioSession().then(() => {
console.info('deactivateAudioSession SUCCESS');
}).catch((err: BusinessError) => {
console.error(`ERROR: ${err}`);
});
```
监听音频会话停用事件
应用可以通过on('audioSessionDeactivated')接口监听音频会话停用事件（AudioSessionDeactivatedEvent）。
当AudioSession被停用（非主动停用）时，应用会收到音频会话停用事件（AudioSessionDeactivatedEvent），其中包含音频会话停用原因（AudioSessionDeactivatedReason）。
在收到AudioSessionDeactivatedEvent时，应用可根据自身业务需求，做相应的处理，例如释放相应资源、重新激活AudioSession等。
```typescript
import { audio } from '@kit.AudioKit';
audioSessionManager.on('audioSessionDeactivated', (audioSessionDeactivatedEvent: audio.AudioSessionDeactivatedEvent) => {
console.info(`reason of audioSessionDeactivated: ${audioSessionDeactivatedEvent.reason} `);
});
```
取消监听音频会话停用事件
应用可以通过off('audioSessionDeactivated')接口取消监听音频会话停用事件。
```typescript
audioSessionManager.off('audioSessionDeactivated');
```
完整示例
参考以下示例，完成音频会话从创建到激活并监听的过程。
```typescript
import { audio } from '@kit.AudioKit';
import { BusinessError } from '@kit.BasicServicesKit';
let audioManager = audio.getAudioManager();
// 创建音频会话管理器
let audioSessionManager: audio.AudioSessionManager = audioManager.getSessionManager();
// 设置音频并发模式
let strategy: audio.AudioSessionStrategy = {
concurrencyMode: audio.AudioConcurrencyMode.CONCURRENCY_MIX_WITH_OTHERS
};
// 激活音频会话
audioSessionManager.activateAudioSession(strategy).then(() => {
console.info('activateAudioSession SUCCESS');
}).catch((err: BusinessError) => {
console.error(`ERROR: ${err}`);
});
// 查询音频会话是否已激活。
let isActivated = audioSessionManager.isAudioSessionActivated();
// 监听音频会话停用事件
audioSessionManager.on('audioSessionDeactivated', (audioSessionDeactivatedEvent: audio.AudioSessionDeactivatedEvent) => {
console.info(`reason of audioSessionDeactivated: ${audioSessionDeactivatedEvent.reason} `);
});
// 音频会话激活后，应用在此处正常执行音频播放、暂停、停止、释放等操作即可。
// 停用音频会话
audioSessionManager.deactivateAudioSession().then(() => {
console.info('deactivateAudioSession SUCCESS');
}).catch((err: BusinessError) => {
console.error(`ERROR: ${err}`);
});
// 取消监听音频会话停用事件
audioSessionManager.off('audioSessionDeactivated');
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/using-ohaudio-for-session-V14
爬取时间: 2025-04-28 19:48:10
来源: Huawei Developer
对于涉及多个音频流并发播放的场景，系统已预设了默认的音频焦点策略，该策略将对所有音频流（包括播放和录制）实施统一的焦点管理。
应用可利用音频会话管理（AudioSessionManager）提供的接口，通过AudioSession主动管理应用内音频流的焦点，自定义本应用音频流的焦点策略，调整本应用音频流释放音频焦点的时机，从而贴合应用特定的使用需求。
本文档主要介绍AudioSession相关C API的使用方法和注意事项，更多音频焦点及音频会话的信息，可参考：音频焦点和音频会话介绍。
使用入门
应用要使用OHAudio提供的音频会话管理（AudioSessionManager）能力，需要添加对应的头文件。
在 CMake 脚本中链接动态库
添加头文件
应用通过引入native_audio_session_manager.h头文件，使用音频播放相关API。
获取音频会话管理器
创建OH_AudioSessionManager实例。在使用音频会话管理功能前，需要先通过OH_AudioManager_GetAudioSessionManager创建音频会话管理实例。
激活音频会话
应用可以通过OH_AudioSessionManager_ActivateAudioSession接口激活当前应用的音频会话。
应用在激活音频会话时，需指定音频会话策略（OH_AudioSession_Strategy），其中包含音频并发模式（OH_AudioSession_ConcurrencyMode）参数，用于声明不同的音频并发策略。
查询音频会话是否已激活
应用可以通过OH_AudioSessionManager_IsAudioSessionActivated接口检查当前应用的音频会话是否已激活。
停用音频会话
应用可以通过OH_AudioSessionManager_DeactivateAudioSession接口停用当前应用的音频会话。
监听音频会话停用事件
在使用AudioSession功能的过程中，推荐应用监听音频会话停用事件（OH_AudioSession_DeactivatedEvent）。
当AudioSession被停用（非主动停用）时，应用会收到音频会话停用事件（OH_AudioSession_DeactivatedEvent），其中包含音频会话停用原因（OH_AudioSession_DeactivatedReason）。
在收到AudioSessionDeactivatedEvent时，应用可根据自身业务需求，做相应的处理，例如释放相应资源、重新激活AudioSession等。
定义回调函数
注册音频会话停用事件回调
应用可以通过OH_AudioSessionManager_RegisterSessionDeactivatedCallback接口监听音频会话停用事件。
取消注册音频会话停用事件回调
应用可以通过OH_AudioSessionManager_UnregisterSessionDeactivatedCallback接口取消监听音频会话停用事件。
完整示例
参考以下示例，完成音频会话从创建到激活并监听的过程。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audio-playback-V14
爬取时间: 2025-04-28 19:48:24
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audio-playback-overview-V14
爬取时间: 2025-04-28 19:48:37
来源: Huawei Developer
如何选择音频播放开发方式
系统提供了多样化的API，来帮助开发者完成音频播放的开发，不同的API适用于不同音频数据格式、音频资源来源、音频使用场景，甚至是不同开发语言。因此，选择合适的音频播放API，有助于降低开发工作量，实现更佳的音频播放效果。
-  AudioRenderer：用于音频输出的ArkTS/JS API，仅支持PCM格式，需要应用持续写入音频数据进行工作。应用可以在输入前添加数据预处理，如设定音频文件的采样率、位宽等，要求开发者具备音频处理的基础知识，适用于更专业、更多样化的媒体播放应用开发。
-  AudioHaptic：用于音振协同播放的ArkTS/JS API，适用于需要在播放音频时同步发起振动的场景，如来电铃声随振、键盘按键反馈、消息通知反馈等。
-  OpenSL ES：一套跨平台标准化的音频Native API，同样提供音频输出能力，仅支持PCM格式，适用于从其他嵌入式平台移植，或依赖在Native层实现音频输出功能的播放应用使用。
-  OHAudio：用于音频输出的Native API，此API在设计上实现归一，同时支持普通音频通路和低时延通路。仅支持PCM格式，适用于依赖Native层实现音频输出功能的场景。
除上述方式外，也可以通过Media Kit中的AVPlayer和SoundPool实现音频播放。
-  AVPlayer：用于音频播放的ArkTS/JS API，集成了流媒体和本地资源解析、媒体资源解封装、音频解码和音频输出功能。可用于直接播放mp3、m4a等格式的音频文件，不支持直接播放PCM格式文件。
-  SoundPool：低时延的短音播放ArkTS/JS API，适用于播放急促简短的音效，如相机快门音效、按键音效、游戏射击音效等。
后台播放或熄屏播放开发须知
应用如果要实现后台播放或熄屏播放，需要同时满足：
1.  使用媒体会话（AVSession）功能注册到系统内统一管理。具体参考AVSession Kit开发指导。 注意：若应用没有注册AVSession，且应用进入后台，则系统会对其音频行为做强制管控，主要包括：
2.  申请长时任务避免进入挂起（Suspend）状态。具体参考长时任务开发指导。
当应用进入后台，播放被中断，如果被媒体会话管控，将打印日志“pause id”；如果没有该日志，则说明被长时任务管控。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/using-audiorenderer-for-playback-V14
爬取时间: 2025-04-28 19:48:51
来源: Huawei Developer
AudioRenderer是音频渲染器，用于播放PCM（Pulse Code Modulation）音频数据，相比AVPlayer而言，可以在输入前添加数据预处理，更适合有音频开发经验的开发者，以实现更灵活的播放功能。
开发指导
使用AudioRenderer播放音频涉及到AudioRenderer实例的创建、音频渲染参数的配置、渲染的开始与停止、资源的释放等。本开发指导将以一次渲染音频数据的过程为例，向开发者讲解如何使用AudioRenderer进行音频渲染，建议搭配AudioRenderer的API说明阅读。
下图展示了AudioRenderer的状态变化，在创建实例后，调用对应的方法可以进入指定的状态实现对应的行为。需要注意的是在确定的状态执行不合适的方法可能导致AudioRenderer发生错误，建议开发者在调用状态转换的方法前进行状态检查，避免程序运行产生预期以外的结果。
为保证UI线程不被阻塞，大部分AudioRenderer调用都是异步的。对于每个API均提供了callback函数和Promise函数，以下示例均采用callback函数。
图1AudioRenderer状态变化示意图
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165927.61300207246223193701201481694059:50001231000000:2800:5BF016CA1BE6372D2D919CE974FB3B6E9B100C902A3A0DE2470B7DE280019CE9.png)
在进行应用开发的过程中，建议开发者通过on('stateChange')方法订阅AudioRenderer的状态变更。因为针对AudioRenderer的某些操作，仅在音频播放器在固定状态时才能执行。如果应用在音频播放器处于错误状态时执行操作，系统可能会抛出异常或生成其他未定义的行为。
-  prepared状态： 通过调用createAudioRenderer()方法进入到该状态。
-  running状态： 正在进行音频数据播放，可以在prepared状态通过调用start()方法进入此状态，也可以在paused状态和stopped状态通过调用start()方法进入此状态。
-  paused状态： 在running状态可以通过调用pause()方法暂停音频数据的播放并进入paused状态，暂停播放之后可以通过调用start()方法继续音频数据播放。
-  stopped状态： 在paused/running状态可以通过stop()方法停止音频数据的播放。
-  released状态： 在prepared、paused、stopped等状态，用户均可通过release()方法释放掉所有占用的硬件和软件资源，并且不会再进入到其他的任何一种状态了。
开发步骤及注意事项
1.  配置音频渲染参数并创建AudioRenderer实例，音频渲染参数的详细信息可以查看AudioRendererOptions。
```typescript
import { audio } from '@kit.AudioKit';
let audioStreamInfo: audio.AudioStreamInfo = {
samplingRate: audio.AudioSamplingRate.SAMPLE_RATE_48000, // 采样率
channels: audio.AudioChannel.CHANNEL_2, // 通道
sampleFormat: audio.AudioSampleFormat.SAMPLE_FORMAT_S16LE, // 采样格式
encodingType: audio.AudioEncodingType.ENCODING_TYPE_RAW // 编码格式
};
let audioRendererInfo: audio.AudioRendererInfo = {
usage: audio.StreamUsage.STREAM_USAGE_MUSIC,
rendererFlags: 0
};
let audioRendererOptions: audio.AudioRendererOptions = {
streamInfo: audioStreamInfo,
rendererInfo: audioRendererInfo
};
audio.createAudioRenderer(audioRendererOptions, (err, data) => {
if (err) {
console.error(`Invoke createAudioRenderer failed, code is ${err.code}, message is ${err.message}`);
return;
} else {
console.info('Invoke createAudioRenderer succeeded.');
let audioRenderer = data;
}
});
```
2.  调用on('writeData')方法，订阅监听音频数据写入回调，推荐使用API version 12支持返回回调结果的方式。 API version 12开始该方法支持返回回调结果，系统可以根据开发者返回的值来决定此次回调中的数据是否播放。 能填满回调所需长度数据的情况下，返回audio.AudioDataCallbackResult.VALID，系统会取用完整长度的数据缓冲进行播放。请不要在未填满数据的情况下返回audio.AudioDataCallbackResult.VALID，否则会导致杂音、卡顿等现象。 在无法填满回调所需长度数据的情况下，建议开发者返回audio.AudioDataCallbackResult.INVALID，系统不会处理该段音频数据，然后会再次向应用请求数据，确认数据填满后返回audio.AudioDataCallbackResult.VALID。 回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。 API version 11该方法不支持返回回调结果，系统默认回调中的数据均为有效数据。 请确保填满回调所需长度数据，否则会导致杂音、卡顿等现象。 在无法填满回调所需长度数据的情况下，建议开发者选择暂时停止写入数据（不暂停音频流），阻塞回调函数，等待数据充足时，再继续写入数据，确保数据填满。在阻塞回调函数后，如需调用AudioRenderer相关接口，需先解阻塞。 开发者如果不希望播放本次回调中的音频数据，可以主动将回调中的数据块置空（置空后，也会被系统统计到已写入的数据，播放静音帧）。 回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。
```typescript
import { audio } from '@kit.AudioKit';
import { BusinessError } from '@kit.BasicServicesKit';
import { fileIo as fs } from '@kit.CoreFileKit';
class Options {
offset?: number;
length?: number;
}
let bufferSize: number = 0;
let path = getContext().cacheDir;
// 确保该沙箱路径下存在该资源
let filePath = path + '/StarWars10s-2C-48000-4SW.wav';
let file: fs.File = fs.openSync(filePath, fs.OpenMode.READ_ONLY);
let writeDataCallback = (buffer: ArrayBuffer) => {
let options: Options = {
offset: bufferSize,
length: buffer.byteLength
};
try {
fs.readSync(file.fd, buffer, options);
bufferSize += buffer.byteLength;
// 系统会判定buffer有效，正常播放。
return audio.AudioDataCallbackResult.VALID;
} catch (error) {
console.error('Error reading file:', error);
// 系统会判定buffer无效，不播放。
return audio.AudioDataCallbackResult.INVALID;
}
};
audioRenderer.on('writeData', writeDataCallback);
```
3.  API version 12开始该方法支持返回回调结果，系统可以根据开发者返回的值来决定此次回调中的数据是否播放。 能填满回调所需长度数据的情况下，返回audio.AudioDataCallbackResult.VALID，系统会取用完整长度的数据缓冲进行播放。请不要在未填满数据的情况下返回audio.AudioDataCallbackResult.VALID，否则会导致杂音、卡顿等现象。 在无法填满回调所需长度数据的情况下，建议开发者返回audio.AudioDataCallbackResult.INVALID，系统不会处理该段音频数据，然后会再次向应用请求数据，确认数据填满后返回audio.AudioDataCallbackResult.VALID。 回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。
```typescript
import { audio } from '@kit.AudioKit';
import { BusinessError } from '@kit.BasicServicesKit';
import { fileIo as fs } from '@kit.CoreFileKit';
class Options {
offset?: number;
length?: number;
}
let bufferSize: number = 0;
let path = getContext().cacheDir;
// 确保该沙箱路径下存在该资源
let filePath = path + '/StarWars10s-2C-48000-4SW.wav';
let file: fs.File = fs.openSync(filePath, fs.OpenMode.READ_ONLY);
let writeDataCallback = (buffer: ArrayBuffer) => {
let options: Options = {
offset: bufferSize,
length: buffer.byteLength
};
try {
fs.readSync(file.fd, buffer, options);
bufferSize += buffer.byteLength;
// 系统会判定buffer有效，正常播放。
return audio.AudioDataCallbackResult.VALID;
} catch (error) {
console.error('Error reading file:', error);
// 系统会判定buffer无效，不播放。
return audio.AudioDataCallbackResult.INVALID;
}
};
audioRenderer.on('writeData', writeDataCallback);
```
4.  能填满回调所需长度数据的情况下，返回audio.AudioDataCallbackResult.VALID，系统会取用完整长度的数据缓冲进行播放。请不要在未填满数据的情况下返回audio.AudioDataCallbackResult.VALID，否则会导致杂音、卡顿等现象。
5.  在无法填满回调所需长度数据的情况下，建议开发者返回audio.AudioDataCallbackResult.INVALID，系统不会处理该段音频数据，然后会再次向应用请求数据，确认数据填满后返回audio.AudioDataCallbackResult.VALID。
6.  回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。
7.  API version 11该方法不支持返回回调结果，系统默认回调中的数据均为有效数据。 请确保填满回调所需长度数据，否则会导致杂音、卡顿等现象。 在无法填满回调所需长度数据的情况下，建议开发者选择暂时停止写入数据（不暂停音频流），阻塞回调函数，等待数据充足时，再继续写入数据，确保数据填满。在阻塞回调函数后，如需调用AudioRenderer相关接口，需先解阻塞。 开发者如果不希望播放本次回调中的音频数据，可以主动将回调中的数据块置空（置空后，也会被系统统计到已写入的数据，播放静音帧）。 回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
import { fileIo as fs } from '@kit.CoreFileKit';
class Options {
offset?: number;
length?: number;
}
let bufferSize: number = 0;
let path = getContext().cacheDir;
// 确保该沙箱路径下存在该资源
let filePath = path + '/StarWars10s-2C-48000-4SW.wav';
let file: fs.File = fs.openSync(filePath, fs.OpenMode.READ_ONLY);
let writeDataCallback = (buffer: ArrayBuffer) => {
// 如果开发者不希望播放某段buffer，可在此处添加判断并对buffer进行置空处理。
let options: Options = {
offset: bufferSize,
length: buffer.byteLength
};
fs.readSync(file.fd, buffer, options);
bufferSize += buffer.byteLength;
};
audioRenderer.on('writeData', writeDataCallback);
```
8.  请确保填满回调所需长度数据，否则会导致杂音、卡顿等现象。
9.  在无法填满回调所需长度数据的情况下，建议开发者选择暂时停止写入数据（不暂停音频流），阻塞回调函数，等待数据充足时，再继续写入数据，确保数据填满。在阻塞回调函数后，如需调用AudioRenderer相关接口，需先解阻塞。
10.  开发者如果不希望播放本次回调中的音频数据，可以主动将回调中的数据块置空（置空后，也会被系统统计到已写入的数据，播放静音帧）。
11.  回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。
12.  调用start()方法进入running状态，开始渲染音频。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
audioRenderer.start((err: BusinessError) => {
if (err) {
console.error(`Renderer start failed, code is ${err.code}, message is ${err.message}`);
} else {
console.info('Renderer start success.');
}
});
```
13.  调用stop()方法停止渲染。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
audioRenderer.stop((err: BusinessError) => {
if (err) {
console.error(`Renderer stop failed, code is ${err.code}, message is ${err.message}`);
} else {
console.info('Renderer stopped.');
}
});
```
14.  调用release()方法销毁实例，释放资源。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
audioRenderer.release((err: BusinessError) => {
if (err) {
console.error(`Renderer release failed, code is ${err.code}, message is ${err.message}`);
} else {
console.info('Renderer released.');
}
});
```
-  API version 12开始该方法支持返回回调结果，系统可以根据开发者返回的值来决定此次回调中的数据是否播放。 能填满回调所需长度数据的情况下，返回audio.AudioDataCallbackResult.VALID，系统会取用完整长度的数据缓冲进行播放。请不要在未填满数据的情况下返回audio.AudioDataCallbackResult.VALID，否则会导致杂音、卡顿等现象。 在无法填满回调所需长度数据的情况下，建议开发者返回audio.AudioDataCallbackResult.INVALID，系统不会处理该段音频数据，然后会再次向应用请求数据，确认数据填满后返回audio.AudioDataCallbackResult.VALID。 回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。
```typescript
import { audio } from '@kit.AudioKit';
import { BusinessError } from '@kit.BasicServicesKit';
import { fileIo as fs } from '@kit.CoreFileKit';
class Options {
offset?: number;
length?: number;
}
let bufferSize: number = 0;
let path = getContext().cacheDir;
// 确保该沙箱路径下存在该资源
let filePath = path + '/StarWars10s-2C-48000-4SW.wav';
let file: fs.File = fs.openSync(filePath, fs.OpenMode.READ_ONLY);
let writeDataCallback = (buffer: ArrayBuffer) => {
let options: Options = {
offset: bufferSize,
length: buffer.byteLength
};
try {
fs.readSync(file.fd, buffer, options);
bufferSize += buffer.byteLength;
// 系统会判定buffer有效，正常播放。
return audio.AudioDataCallbackResult.VALID;
} catch (error) {
console.error('Error reading file:', error);
// 系统会判定buffer无效，不播放。
return audio.AudioDataCallbackResult.INVALID;
}
};
audioRenderer.on('writeData', writeDataCallback);
```
-  能填满回调所需长度数据的情况下，返回audio.AudioDataCallbackResult.VALID，系统会取用完整长度的数据缓冲进行播放。请不要在未填满数据的情况下返回audio.AudioDataCallbackResult.VALID，否则会导致杂音、卡顿等现象。
-  在无法填满回调所需长度数据的情况下，建议开发者返回audio.AudioDataCallbackResult.INVALID，系统不会处理该段音频数据，然后会再次向应用请求数据，确认数据填满后返回audio.AudioDataCallbackResult.VALID。
-  回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。
-  API version 11该方法不支持返回回调结果，系统默认回调中的数据均为有效数据。 请确保填满回调所需长度数据，否则会导致杂音、卡顿等现象。 在无法填满回调所需长度数据的情况下，建议开发者选择暂时停止写入数据（不暂停音频流），阻塞回调函数，等待数据充足时，再继续写入数据，确保数据填满。在阻塞回调函数后，如需调用AudioRenderer相关接口，需先解阻塞。 开发者如果不希望播放本次回调中的音频数据，可以主动将回调中的数据块置空（置空后，也会被系统统计到已写入的数据，播放静音帧）。 回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
import { fileIo as fs } from '@kit.CoreFileKit';
class Options {
offset?: number;
length?: number;
}
let bufferSize: number = 0;
let path = getContext().cacheDir;
// 确保该沙箱路径下存在该资源
let filePath = path + '/StarWars10s-2C-48000-4SW.wav';
let file: fs.File = fs.openSync(filePath, fs.OpenMode.READ_ONLY);
let writeDataCallback = (buffer: ArrayBuffer) => {
// 如果开发者不希望播放某段buffer，可在此处添加判断并对buffer进行置空处理。
let options: Options = {
offset: bufferSize,
length: buffer.byteLength
};
fs.readSync(file.fd, buffer, options);
bufferSize += buffer.byteLength;
};
audioRenderer.on('writeData', writeDataCallback);
```
-  请确保填满回调所需长度数据，否则会导致杂音、卡顿等现象。
-  在无法填满回调所需长度数据的情况下，建议开发者选择暂时停止写入数据（不暂停音频流），阻塞回调函数，等待数据充足时，再继续写入数据，确保数据填满。在阻塞回调函数后，如需调用AudioRenderer相关接口，需先解阻塞。
-  开发者如果不希望播放本次回调中的音频数据，可以主动将回调中的数据块置空（置空后，也会被系统统计到已写入的数据，播放静音帧）。
-  回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。
-  能填满回调所需长度数据的情况下，返回audio.AudioDataCallbackResult.VALID，系统会取用完整长度的数据缓冲进行播放。请不要在未填满数据的情况下返回audio.AudioDataCallbackResult.VALID，否则会导致杂音、卡顿等现象。
-  在无法填满回调所需长度数据的情况下，建议开发者返回audio.AudioDataCallbackResult.INVALID，系统不会处理该段音频数据，然后会再次向应用请求数据，确认数据填满后返回audio.AudioDataCallbackResult.VALID。
-  回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。
-  请确保填满回调所需长度数据，否则会导致杂音、卡顿等现象。
-  在无法填满回调所需长度数据的情况下，建议开发者选择暂时停止写入数据（不暂停音频流），阻塞回调函数，等待数据充足时，再继续写入数据，确保数据填满。在阻塞回调函数后，如需调用AudioRenderer相关接口，需先解阻塞。
-  开发者如果不希望播放本次回调中的音频数据，可以主动将回调中的数据块置空（置空后，也会被系统统计到已写入的数据，播放静音帧）。
-  回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。
选择正确的StreamUsage
创建播放器时候，开发者需要根据应用场景指定播放器的StreamUsage，选择正确的StreamUsage可以避免用户遇到不符合预期的行为。
在音频API文档StreamUsage介绍中，列举了每一种类型推荐的应用场景。例如音乐场景推荐使用STREAM_USAGE_MUSIC，电影或者视频场景推荐使用STREAM_USAGE_MOVIE，游戏场景推荐使用STREAM_USAGE_GAME，等等。
如果开发者配置了不正确的StreamUsage，可能带来一些不符合预期的行为。例如以下场景。
完整示例
下面展示了使用AudioRenderer渲染音频文件的示例代码。
```typescript
import { audio } from '@kit.AudioKit';
import { BusinessError } from '@kit.BasicServicesKit';
import { fileIo as fs } from '@kit.CoreFileKit';
const TAG = 'AudioRendererDemo';
class Options {
offset?: number;
length?: number;
}
let bufferSize: number = 0;
let renderModel: audio.AudioRenderer | undefined = undefined;
let audioStreamInfo: audio.AudioStreamInfo = {
samplingRate: audio.AudioSamplingRate.SAMPLE_RATE_48000, // 采样率
channels: audio.AudioChannel.CHANNEL_2, // 通道
sampleFormat: audio.AudioSampleFormat.SAMPLE_FORMAT_S16LE, // 采样格式
encodingType: audio.AudioEncodingType.ENCODING_TYPE_RAW // 编码格式
};
let audioRendererInfo: audio.AudioRendererInfo = {
usage: audio.StreamUsage.STREAM_USAGE_MUSIC, // 音频流使用类型
rendererFlags: 0 // 音频渲染器标志
};
let audioRendererOptions: audio.AudioRendererOptions = {
streamInfo: audioStreamInfo,
rendererInfo: audioRendererInfo
};
let path = getContext().cacheDir;
// 确保该沙箱路径下存在该资源
let filePath = path + '/StarWars10s-2C-48000-4SW.wav';
let file: fs.File = fs.openSync(filePath, fs.OpenMode.READ_ONLY);
let writeDataCallback = (buffer: ArrayBuffer) => {
let options: Options = {
offset: bufferSize,
length: buffer.byteLength
};
try {
fs.readSync(file.fd, buffer, options);
bufferSize += buffer.byteLength;
// API version 11 不支持返回回调结果，从 API version 12 开始支持返回回调结果
return audio.AudioDataCallbackResult.VALID;
} catch (error) {
console.error('Error reading file:', error);
// API version 11 不支持返回回调结果，从 API version 12 开始支持返回回调结果
return audio.AudioDataCallbackResult.INVALID;
}
};
// 初始化，创建实例，设置监听事件
function init() {
audio.createAudioRenderer(audioRendererOptions, (err, renderer) => { // 创建AudioRenderer实例
if (!err) {
console.info(`${TAG}: creating AudioRenderer success`);
renderModel = renderer;
if (renderModel !== undefined) {
(renderModel as audio.AudioRenderer).on('writeData', writeDataCallback);
}
} else {
console.info(`${TAG}: creating AudioRenderer failed, error: ${err.message}`);
}
});
}
// 开始一次音频渲染
function start() {
if (renderModel !== undefined) {
let stateGroup = [audio.AudioState.STATE_PREPARED, audio.AudioState.STATE_PAUSED, audio.AudioState.STATE_STOPPED];
if (stateGroup.indexOf((renderModel as audio.AudioRenderer).state.valueOf()) === -1) { // 当且仅当状态为prepared、paused和stopped之一时才能启动渲染
console.error(TAG + 'start failed');
return;
}
// 启动渲染
(renderModel as audio.AudioRenderer).start((err: BusinessError) => {
if (err) {
console.error('Renderer start failed.');
} else {
console.info('Renderer start success.');
}
});
}
}
// 暂停渲染
function pause() {
if (renderModel !== undefined) {
// 只有渲染器状态为running的时候才能暂停
if ((renderModel as audio.AudioRenderer).state.valueOf() !== audio.AudioState.STATE_RUNNING) {
console.info('Renderer is not running');
return;
}
// 暂停渲染
(renderModel as audio.AudioRenderer).pause((err: BusinessError) => {
if (err) {
console.error('Renderer pause failed.');
} else {
console.info('Renderer pause success.');
}
});
}
}
// 停止渲染
async function stop() {
if (renderModel !== undefined) {
// 只有渲染器状态为running或paused的时候才可以停止
if ((renderModel as audio.AudioRenderer).state.valueOf() !== audio.AudioState.STATE_RUNNING && (renderModel as audio.AudioRenderer).state.valueOf() !== audio.AudioState.STATE_PAUSED) {
console.info('Renderer is not running or paused.');
return;
}
// 停止渲染
(renderModel as audio.AudioRenderer).stop((err: BusinessError) => {
if (err) {
console.error('Renderer stop failed.');
} else {
fs.close(file);
console.info('Renderer stop success.');
}
});
}
}
// 销毁实例，释放资源
async function release() {
if (renderModel !== undefined) {
// 渲染器状态不是released状态，才能release
if (renderModel.state.valueOf() === audio.AudioState.STATE_RELEASED) {
console.info('Renderer already released');
return;
}
// 释放资源
(renderModel as audio.AudioRenderer).release((err: BusinessError) => {
if (err) {
console.error('Renderer release failed.');
} else {
console.info('Renderer release success.');
}
});
}
}
```
当同优先级或高优先级音频流要使用输出设备时，当前音频流会被中断，应用可以自行响应中断事件并做出处理。具体的音频并发处理方式可参考处理音频焦点事件。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audio-output-device-change-V14
爬取时间: 2025-04-28 19:49:05
来源: Huawei Developer
开发者可以了解音频流输出设备变更信息，并完成相应适配，确保应用在设备发生变更时的用户体验。
开发者可使用AudioRenderer的outputDeviceChangeWithInfo，用于监听音频流输出设备变化及原因。当系统出现音频输出设备的上下线、用户强选、设备抢占或设备选择策略变更等情况，导致音频流输出设备变更时，系统将通过该接口通知应用当前音频流设备变更信息，包含当前音频流输出设备信息和设备变更原因。
音频流输出设备信息
在outputDeviceChangeWithInfo返回的音频流设备变更信息中，包含当前音频流输出设备信息，以数组形式发送，一般该列表仅包含一个设备信息，具体可参考AudioDeviceDescriptors（设备信息列表）。
音频流输出设备变更原因
当发生下述四种情况（AudioStreamDeviceChangeReason）时，系统将向应用发送设备变更回调。
-  REASON_NEW_DEVICE_AVAILABLE：新设备可用。 触发场景： 普通蓝牙设备（耳机、眼镜、音箱、车机等）连接、支持佩戴检测的蓝牙设备（耳机、眼镜等）佩戴、有线设备（3.5mm耳机、Type-C耳机、USB耳机、USB音箱等）插入、分布式设备上线等。
-  REASON_OLD_DEVICE_UNAVAILABLE：旧设备不可用。 当报告此原因时，应用程序应考虑暂停音频播放。 触发场景： 普通蓝牙设备（耳机、眼镜、音箱、车机等）断开、支持佩戴检测的蓝牙耳机双耳摘下、支持佩戴检测的蓝牙眼镜摘下、有线设备（3.5mm耳机、Type-C耳机、USB耳机、音箱等）拔出、分布式设备下线等。 针对此场景，常用业务场景的处理建议如下：
-  REASON_OVERRODE：用户强制选择设备。 触发场景： 用户从界面选择切换音频流输出设备、从外设选择接听蜂窝或VoIP来电。
-  REASON_UNKNOWN：未知原因。
参考示例
```typescript
import { audio } from '@kit.AudioKit';
import { BusinessError } from '@kit.BasicServicesKit';
let audioRenderer: audio.AudioRenderer | undefined = undefined;
let audioStreamInfo: audio.AudioStreamInfo = {
samplingRate: audio.AudioSamplingRate.SAMPLE_RATE_48000, // 采样率
channels: audio.AudioChannel.CHANNEL_2, // 通道
sampleFormat: audio.AudioSampleFormat.SAMPLE_FORMAT_S16LE, // 采样格式
encodingType: audio.AudioEncodingType.ENCODING_TYPE_RAW // 编码格式
};
let audioRendererInfo: audio.AudioRendererInfo = {
usage: audio.StreamUsage.STREAM_USAGE_MUSIC, // 音频流使用类型
rendererFlags: 0 // 音频渲染器标志
};
let audioRendererOptions: audio.AudioRendererOptions = {
streamInfo: audioStreamInfo,
rendererInfo: audioRendererInfo
};
// 创建AudioRenderer实例
audio.createAudioRenderer(audioRendererOptions).then((data) => {
audioRenderer = data;
console.info('AudioFrameworkRenderLog: AudioRenderer Created : Success : Stream Type: SUCCESS');
}).catch((err: BusinessError) => {
console.error(`AudioFrameworkRenderLog: AudioRenderer Created : ERROR : ${err}`);
});
if (audioRenderer) {
// 订阅监听音频流输出设备变化及原因
(audioRenderer as audio.AudioRenderer).on('outputDeviceChangeWithInfo', async (deviceChangeInfo: audio.AudioStreamDeviceChangeInfo) => {
switch (deviceChangeInfo.changeReason) {
case audio.AudioStreamDeviceChangeReason.REASON_OLD_DEVICE_UNAVAILABLE:
// 响应设备不可用事件，如果应用处于播放状态，应暂停播放，更新UX界面。
// await audioRenderer.pause();
break;
case audio.AudioStreamDeviceChangeReason.REASON_NEW_DEVICE_AVAILABLE:
// 应用根据业务情况响应设备可用事件。
break;
case audio.AudioStreamDeviceChangeReason.REASON_OVERRODE:
// 应用根据业务情况响应设备强选事件。
break;
case audio.AudioStreamDeviceChangeReason.REASON_UNKNOWN:
// 应用根据业务情况响应未知原因事件。
break;
}
});
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/using-ohaudio-for-playback-V14
爬取时间: 2025-04-28 19:49:18
来源: Huawei Developer
OHAudio是系统在API version 10中引入的一套C API，此API在设计上实现归一，同时支持普通音频通路和低时延通路。仅支持PCM格式，适用于依赖Native层实现音频输出功能的场景。
OHAudio音频播放状态变化示意图：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165927.46267796675649531173431580513793:50001231000000:2800:36AD5405FB4A4C2D5B5F5C73AD2B8F225FE2347E45A8714789F99FDE3EB3FA76.png)
使用入门
开发者要使用OHAudio提供的播放能力，需要添加对应的头文件。
在 CMake 脚本中链接动态库
添加头文件
开发者通过引入<native_audiostreambuilder.h>和<native_audiorenderer.h>头文件，使用音频播放相关API。
音频流构造器
OHAudio提供OH_AudioStreamBuilder接口，遵循构造器设计模式，用于构建音频流。开发者需要根据业务场景，指定对应的OH_AudioStream_Type。
OH_AudioStream_Type包含两种类型：
使用OH_AudioStreamBuilder_Create创建构造器示例：
在音频业务结束之后，开发者应该执行OH_AudioStreamBuilder_Destroy接口来销毁构造器。
开发步骤及注意事项
详细的API说明请参考OHAudio API参考。
开发者可以通过以下几个步骤来实现一个简单的播放功能。
1.  创建构造器
2.  配置音频流参数 创建音频播放构造器后，可以设置音频流所需要的参数，可以参考下面的案例。 注意，播放的音频数据要通过回调接口写入，开发者要实现回调接口，使用OH_AudioStreamBuilder_SetRendererCallback设置回调函数。回调函数的声明请查看OH_AudioRenderer_Callbacks。
3.  设置音频回调函数 多音频并发处理可参考文档处理音频焦点事件，仅接口语言差异。 在设置音频回调函数时API version 12新增回调函数OH_AudioRenderer_OnWriteDataCallback用于写入音频数据。 API version 12开始推荐使用OH_AudioRenderer_OnWriteDataCallback代替OH_AudioRenderer_Callbacks_Struct.OH_AudioRenderer_OnWriteData用于写入音频数据。 能填满回调所需长度数据的情况下，返回AUDIO_DATA_CALLBACK_RESULT_VALID，系统会取用完整长度的数据缓冲进行播放。请不要在未填满数据的情况下返回AUDIO_DATA_CALLBACK_RESULT_VALID，否则会导致杂音、卡顿等现象。 在无法填满回调所需长度数据的情况下，建议开发者返回AUDIO_DATA_CALLBACK_RESULT_INVALID，系统不会处理该段音频数据，然后会再次向应用请求数据，确认数据填满后返回AUDIO_DATA_CALLBACK_RESULT_VALID。 回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。 从API version 12开始可通过OH_AudioStreamBuilder_SetFrameSizeInCallback设置audioDataSize的大小。 API version 11使用回调函数OH_AudioRenderer_Callbacks_Struct.OH_AudioRenderer_OnWriteData用于写入音频数据。 该函数不支持返回回调结果，系统默认回调中的数据均为有效数据。请确保填满回调所需长度数据，否则会导致杂音、卡顿等现象。 在无法填满回调所需长度数据的情况下，建议开发者选择暂时停止写入数据（不暂停音频流），阻塞回调函数，等待数据充足时，再继续写入数据，确保数据填满。在阻塞回调函数后，如需调用AudioRenderer相关接口，需先解阻塞。 开发者如果不希望播放本次回调中的音频数据，可以主动将回调中的数据块置空（置空后，也会被系统统计到已写入的数据，播放静音帧）。 回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。 为了避免不可预期的行为，在设置音频回调函数时，可以通过下面两种方式中的任意一种来设置音频回调函数： 请确保OH_AudioRenderer_Callbacks的每一个回调都被自定义的回调方法或空指针初始化。 使用前，初始化并清零结构体。
4.  API version 12开始推荐使用OH_AudioRenderer_OnWriteDataCallback代替OH_AudioRenderer_Callbacks_Struct.OH_AudioRenderer_OnWriteData用于写入音频数据。 能填满回调所需长度数据的情况下，返回AUDIO_DATA_CALLBACK_RESULT_VALID，系统会取用完整长度的数据缓冲进行播放。请不要在未填满数据的情况下返回AUDIO_DATA_CALLBACK_RESULT_VALID，否则会导致杂音、卡顿等现象。 在无法填满回调所需长度数据的情况下，建议开发者返回AUDIO_DATA_CALLBACK_RESULT_INVALID，系统不会处理该段音频数据，然后会再次向应用请求数据，确认数据填满后返回AUDIO_DATA_CALLBACK_RESULT_VALID。 回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。 从API version 12开始可通过OH_AudioStreamBuilder_SetFrameSizeInCallback设置audioDataSize的大小。
5.  能填满回调所需长度数据的情况下，返回AUDIO_DATA_CALLBACK_RESULT_VALID，系统会取用完整长度的数据缓冲进行播放。请不要在未填满数据的情况下返回AUDIO_DATA_CALLBACK_RESULT_VALID，否则会导致杂音、卡顿等现象。
6.  在无法填满回调所需长度数据的情况下，建议开发者返回AUDIO_DATA_CALLBACK_RESULT_INVALID，系统不会处理该段音频数据，然后会再次向应用请求数据，确认数据填满后返回AUDIO_DATA_CALLBACK_RESULT_VALID。
7.  回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。
8.  API version 11使用回调函数OH_AudioRenderer_Callbacks_Struct.OH_AudioRenderer_OnWriteData用于写入音频数据。 该函数不支持返回回调结果，系统默认回调中的数据均为有效数据。请确保填满回调所需长度数据，否则会导致杂音、卡顿等现象。 在无法填满回调所需长度数据的情况下，建议开发者选择暂时停止写入数据（不暂停音频流），阻塞回调函数，等待数据充足时，再继续写入数据，确保数据填满。在阻塞回调函数后，如需调用AudioRenderer相关接口，需先解阻塞。 开发者如果不希望播放本次回调中的音频数据，可以主动将回调中的数据块置空（置空后，也会被系统统计到已写入的数据，播放静音帧）。 回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。
9.  该函数不支持返回回调结果，系统默认回调中的数据均为有效数据。请确保填满回调所需长度数据，否则会导致杂音、卡顿等现象。
10.  在无法填满回调所需长度数据的情况下，建议开发者选择暂时停止写入数据（不暂停音频流），阻塞回调函数，等待数据充足时，再继续写入数据，确保数据填满。在阻塞回调函数后，如需调用AudioRenderer相关接口，需先解阻塞。
11.  开发者如果不希望播放本次回调中的音频数据，可以主动将回调中的数据块置空（置空后，也会被系统统计到已写入的数据，播放静音帧）。
12.  回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。
13.  请确保OH_AudioRenderer_Callbacks的每一个回调都被自定义的回调方法或空指针初始化。
14.  使用前，初始化并清零结构体。
15.  构造播放音频流
16.  使用音频流 音频流包含下面接口，用来实现对音频流的控制。
17.  释放构造器 构造器不再使用时，需要释放相关资源。
-  API version 12开始推荐使用OH_AudioRenderer_OnWriteDataCallback代替OH_AudioRenderer_Callbacks_Struct.OH_AudioRenderer_OnWriteData用于写入音频数据。 能填满回调所需长度数据的情况下，返回AUDIO_DATA_CALLBACK_RESULT_VALID，系统会取用完整长度的数据缓冲进行播放。请不要在未填满数据的情况下返回AUDIO_DATA_CALLBACK_RESULT_VALID，否则会导致杂音、卡顿等现象。 在无法填满回调所需长度数据的情况下，建议开发者返回AUDIO_DATA_CALLBACK_RESULT_INVALID，系统不会处理该段音频数据，然后会再次向应用请求数据，确认数据填满后返回AUDIO_DATA_CALLBACK_RESULT_VALID。 回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。 从API version 12开始可通过OH_AudioStreamBuilder_SetFrameSizeInCallback设置audioDataSize的大小。
-  能填满回调所需长度数据的情况下，返回AUDIO_DATA_CALLBACK_RESULT_VALID，系统会取用完整长度的数据缓冲进行播放。请不要在未填满数据的情况下返回AUDIO_DATA_CALLBACK_RESULT_VALID，否则会导致杂音、卡顿等现象。
-  在无法填满回调所需长度数据的情况下，建议开发者返回AUDIO_DATA_CALLBACK_RESULT_INVALID，系统不会处理该段音频数据，然后会再次向应用请求数据，确认数据填满后返回AUDIO_DATA_CALLBACK_RESULT_VALID。
-  回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。
-  API version 11使用回调函数OH_AudioRenderer_Callbacks_Struct.OH_AudioRenderer_OnWriteData用于写入音频数据。 该函数不支持返回回调结果，系统默认回调中的数据均为有效数据。请确保填满回调所需长度数据，否则会导致杂音、卡顿等现象。 在无法填满回调所需长度数据的情况下，建议开发者选择暂时停止写入数据（不暂停音频流），阻塞回调函数，等待数据充足时，再继续写入数据，确保数据填满。在阻塞回调函数后，如需调用AudioRenderer相关接口，需先解阻塞。 开发者如果不希望播放本次回调中的音频数据，可以主动将回调中的数据块置空（置空后，也会被系统统计到已写入的数据，播放静音帧）。 回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。
-  该函数不支持返回回调结果，系统默认回调中的数据均为有效数据。请确保填满回调所需长度数据，否则会导致杂音、卡顿等现象。
-  在无法填满回调所需长度数据的情况下，建议开发者选择暂时停止写入数据（不暂停音频流），阻塞回调函数，等待数据充足时，再继续写入数据，确保数据填满。在阻塞回调函数后，如需调用AudioRenderer相关接口，需先解阻塞。
-  开发者如果不希望播放本次回调中的音频数据，可以主动将回调中的数据块置空（置空后，也会被系统统计到已写入的数据，播放静音帧）。
-  回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。
-  能填满回调所需长度数据的情况下，返回AUDIO_DATA_CALLBACK_RESULT_VALID，系统会取用完整长度的数据缓冲进行播放。请不要在未填满数据的情况下返回AUDIO_DATA_CALLBACK_RESULT_VALID，否则会导致杂音、卡顿等现象。
-  在无法填满回调所需长度数据的情况下，建议开发者返回AUDIO_DATA_CALLBACK_RESULT_INVALID，系统不会处理该段音频数据，然后会再次向应用请求数据，确认数据填满后返回AUDIO_DATA_CALLBACK_RESULT_VALID。
-  回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。
-  该函数不支持返回回调结果，系统默认回调中的数据均为有效数据。请确保填满回调所需长度数据，否则会导致杂音、卡顿等现象。
-  在无法填满回调所需长度数据的情况下，建议开发者选择暂时停止写入数据（不暂停音频流），阻塞回调函数，等待数据充足时，再继续写入数据，确保数据填满。在阻塞回调函数后，如需调用AudioRenderer相关接口，需先解阻塞。
-  开发者如果不希望播放本次回调中的音频数据，可以主动将回调中的数据块置空（置空后，也会被系统统计到已写入的数据，播放静音帧）。
-  回调函数结束后，音频服务会把缓冲中数据放入队列里等待播放，因此请勿在回调外再次更改缓冲中的数据。对于最后一帧，如果数据不够填满缓冲长度，开发者需要使用剩余数据拼接空数据的方式，将缓冲填满，避免缓冲内的历史脏数据对播放效果产生不良的影响。
-  请确保OH_AudioRenderer_Callbacks的每一个回调都被自定义的回调方法或空指针初始化。
-  使用前，初始化并清零结构体。
| 接口 | 说明 |
| --- | --- |
| OH_AudioStream_Result OH_AudioRenderer_Start(OH_AudioRenderer* renderer) | 开始播放 |
| OH_AudioStream_Result OH_AudioRenderer_Pause(OH_AudioRenderer* renderer) | 暂停播放 |
| OH_AudioStream_Result OH_AudioRenderer_Stop(OH_AudioRenderer* renderer) | 停止播放 |
| OH_AudioStream_Result OH_AudioRenderer_Flush(OH_AudioRenderer* renderer) | 释放缓存数据 |
| OH_AudioStream_Result OH_AudioRenderer_Release(OH_AudioRenderer* renderer) | 释放播放实例 |
设置音频流音量
开发者可使用OH_AudioRenderer_SetVolume接口设置当前音频流音量值。
开发示例
设置低时延模式
当设备支持低时延通路且采样率设置为48000时，开发者可以使用低时延模式创建播放器，获得更高质量的音频体验。
开发流程与普通播放场景一致，仅需要在创建音频流构造器时，调用OH_AudioStreamBuilder_SetLatencyMode()设置低时延模式。
当音频录制场景OH_AudioStream_Usage为AUDIOSTREAM_USAGE_VOICE_COMMUNICATION和AUDIOSTREAM_USAGE_VIDEO_COMMUNICATION时，不支持主动设置低时延模式，系统会根据设备的能力，决策输出的音频通路。
开发示例
设置音频声道布局
播放音频文件时，可以通过设置音频的声道布局信息，指定渲染或播放时的扬声器摆位，使得渲染和播放效果更佳，获得更高质量的音频体验。
开发流程与普通播放场景一致，仅需要在创建音频流构造器时，调用OH_AudioStreamBuilder_SetChannelLayout()设置声道布局信息。
当声道布局与声道数不匹配时，创建音频流会失败。建议在设置声道布局时，确认下发的声道布局信息是正确的。
如果不知道准确的声道布局信息，或者开发者需要使用默认声道布局，可以不调用设置声道布局接口，或者下发CH_LAYOUT_UNKNOWN，以使用基于声道数的默认声道布局。
对于HOA格式的音频，想要获得正确的渲染和播放效果，必须指定声道布局信息。
开发示例
播放AudioVivid格式音源
播放AudioVivid格式音频文件时，需要使用与普通播放不同的数据写入回调函数，该回调可以同时写入PCM数据与元数据。
开发流程与普通播放场景一致，仅需要在创建音频流构造器时，调用OH_AudioStreamBuilder_SetWriteDataWithMetadataCallback()设置PCM数据与元数据同时写入的回调函数，同时调用OH_AudioStreamBuilder_SetEncodingType()设置编码类型为AUDIOSTREAM_ENCODING_TYPE_AUDIOVIVID。
在播放AudioVivid时，帧长是固定的，不可通过OH_AudioStreamBuilder_SetFrameSizeInCallback()设置回调帧长。同时，在设置播放声道数和声道布局时，需要将写入音源的声床数和对象数相加后进行设置。
开发示例
示例代码

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/using-audiohaptic-for-playback-V14
爬取时间: 2025-04-28 19:49:32
来源: Huawei Developer
AudioHaptic11+提供音频与振动协同播放及管理的方法，适用于需要在播放音频时同步发起振动的场景，如来电铃声随振、键盘按键反馈、消息通知反馈等。
开发指导
使用AudioHaptic播放音频并同步开启振动，涉及到音频及振动资源的管理、音频时延模式及音频流使用类型的配置、音振播放器的创建及管理等。本开发指导将以一次音振协同播放的过程为例，向开发者讲解如何使用AudioHaptic进行音振协同播放，建议配合AudioHaptic的API说明阅读。
权限申请
如果应用创建的AudioHapticPlayer需要触发振动，则需要校验应用是否拥有该权限：ohos.permission.VIBRATE。
开发步骤及注意事项
1.  获取音振管理器实例，并注册音频及振动资源，资源支持情况可以查看AudioHapticManager。
```typescript
import { audio, audioHaptic } from '@kit.AudioKit';
import { BusinessError } from '@kit.BasicServicesKit';
let audioHapticManagerInstance: audioHaptic.AudioHapticManager = audioHaptic.getAudioHapticManager();
let audioUri = 'data/audioTest.wav'; // 需更改为目标音频资源的Uri
let hapticUri = 'data/hapticTest.json'; // 需更改为目标振动资源的Uri
let id = 0;
audioHapticManagerInstance.registerSource(audioUri, hapticUri).then((value: number) => {
console.info(`Promise returned to indicate that the source id of the registerd source ${value}.`);
id = value;
}).catch ((err: BusinessError) => {
console.error(`Failed to register source ${err}`);
});
```
2.  设置音振播放器参数，各参数作用可以查看AudioHapticManager。
```typescript
let latencyMode: audioHaptic.AudioLatencyMode = audioHaptic.AudioLatencyMode.AUDIO_LATENCY_MODE_FAST;
audioHapticManagerInstance.setAudioLatencyMode(id, latencyMode);
let usage: audio.StreamUsage = audio.StreamUsage.STREAM_USAGE_NOTIFICATION;
audioHapticManagerInstance.setStreamUsage(id, usage);
```
3.  创建AudioHapticPlayer实例。
```typescript
let options: audioHaptic.AudioHapticPlayerOptions = {muteAudio: false, muteHaptics: false};
let audioHapticPlayer: audioHaptic.AudioHapticPlayer | undefined = undefined;
audioHapticManagerInstance.createPlayer(id, options).then((value: audioHaptic.AudioHapticPlayer) => {
console.info(`Promise returned to indicate that the audio haptic player instance.`);
audioHapticPlayer = value;
}).catch ((err: BusinessError) => {
console.error(`Failed to create player ${err}`);
});
console.info(`Create the audio haptic player successfully.`);
```
4.  调用start()方法，开启音频播放并同步开启振动。
```typescript
audioHapticPlayer.start().then(() => {
console.info(`Promise returned to indicate that start playing successfully.`);
}).catch ((err: BusinessError) => {
console.error(`Failed to start playing. ${err}`);
});
```
5.  调用stop()方法，停止音频播放并同步停止振动。
```typescript
audioHapticPlayer.stop().then(() => {
console.info(`Promise returned to indicate that stop playing successfully.`);
}).catch ((err: BusinessError) => {
console.error(`Failed to stop playing. ${err}`);
});
```
6.  释放AudioHapticPlayer实例。
```typescript
audioHapticPlayer.release().then(() => {
console.info(`Promise returned to indicate that release the audio haptic player successfully.`);
}).catch ((err: BusinessError) => {
console.error(`Failed to release the audio haptic player. ${err}`);
});
```
7.  将已注册的音频及振动资源移除注册
```typescript
audioHapticManagerInstance.unregisterSource(id).then(() => {
console.info(`Promise returned to indicate that unregister source successfully`);
}).catch ((err: BusinessError) => {
console.error(`Failed to unregistere source ${err}`);
});
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/volume-management-V14
爬取时间: 2025-04-28 19:49:46
来源: Huawei Developer
播放音量的管理主要包括对系统音量的管理和对音频流音量的管理。系统音量与音频流音量分别是指HarmonyOS系统的总音量和指定音频流的音量，其中音频流音量的大小受制于系统音量，管理两者的接口不同。
系统音量
管理系统音量的接口是AudioVolumeManager，在使用之前，需要使用getVolumeManager()获取AudioVolumeManager实例。
通过AudioVolumeManager只能获取音量信息及监听音量变化，不能主动调节系统音量。如果应用需要调节系统音量，可以使用音量面板调节系统音量。
```typescript
import { audio } from '@kit.AudioKit';
let audioManager = audio.getAudioManager();
let audioVolumeManager = audioManager.getVolumeManager();
```
监听系统音量变化
通过设置监听事件，可以监听系统音量的变化：
```typescript
import { audio } from '@kit.AudioKit';
audioVolumeManager.on('volumeChange', (volumeEvent: audio.VolumeEvent) => {
console.info(`VolumeType of stream: ${volumeEvent.volumeType} `);
console.info(`Volume level: ${volumeEvent.volume} `);
console.info(`Whether to updateUI: ${volumeEvent.updateUi} `);
});
```
使用音量面板调节系统音量
应用无法直接调节系统音量，可以通过系统音量面板，让用户通过界面操作来调节音量。当用户通过应用内音量面板调节音量时，系统会展示音量提示界面，显性地提示用户系统音量发生改变。
系统提供了ArkTS组件AVVolumePanel（音量面板），应用可以创建该组件，具体样例和介绍请查看AVVolumePanel参考文档。
音频流音量
管理音频流音量的接口是AVPlayer或AudioRenderer的setVolume()方法，使用AVPlayer设置音频流音量的示例代码如下：
```typescript
let volume = 1.0;  // 指定的音量大小，取值范围为[0.00-1.00]，1表示最大音量
avPlayer.setVolume(volume);
```
使用AudioRenderer设置音频流音量的示例代码如下：
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
audioRenderer.setVolume(0.5).then(() => {  // 音量范围为[0.0-1.0]
console.info('Invoke setVolume succeeded.');
}).catch((err: BusinessError) => {
console.error(`Invoke setVolume failed, code is ${err.code}, message is ${err.message}`);
});
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audio-effect-management-V14
爬取时间: 2025-04-28 19:50:00
来源: Huawei Developer
音效管理主要包括播放实例音效管理和全局音效查询两部分，播放实例音效管理主要包括查询和设置当前音频播放流的音效模式，全局音效查询支持查询StreamUsage对应场景支持的音效模式。
播放实例音效管理
主要包括查询和设置当前音频播放流的音效模式，音效模式包括EFFECT_NONE关闭音效模式和EFFECT_DEFAULT默认音效模式。默认音效模式会根据创建音频流的StreamUsage自动加载对应场景的音效。
获取播放实例
管理播放实例音效的接口是getAudioEffectMode()查询当前音频播放流的音效模式和setAudioEffectMode(mode: AudioEffectMode)设置当前音频播放流的音效模式，在使用之前，需要使用createAudioRenderer(options: AudioRendererOptions)先创建音频播放流AudioRenderer实例。
1.  导入音频接口。
```typescript
import { audio } from '@kit.AudioKit';
```
2.  配置音频渲染参数并创建AudioRenderer实例，音频渲染参数的详细信息可以查看AudioRendererOptions，创建AudioRenderer实例时会默认挂载EFFECT_DEFAULT模式音效。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
let audioStreamInfo: audio.AudioStreamInfo = {
samplingRate: audio.AudioSamplingRate.SAMPLE_RATE_48000,
channels: audio.AudioChannel.CHANNEL_2,
sampleFormat: audio.AudioSampleFormat.SAMPLE_FORMAT_S16LE,
encodingType: audio.AudioEncodingType.ENCODING_TYPE_RAW
};
let audioRendererInfo: audio.AudioRendererInfo = {
usage: audio.StreamUsage.STREAM_USAGE_MUSIC,
rendererFlags: 0
};
let audioRendererOptions: audio.AudioRendererOptions = {
streamInfo: audioStreamInfo,
rendererInfo: audioRendererInfo
};
let audioRenderer: audio.AudioRenderer | undefined = undefined;
audio.createAudioRenderer(audioRendererOptions, (err: BusinessError, data: audio.AudioRenderer) => {
if (err) {
console.error(`Invoke createAudioRenderer failed, code is ${err.code}, message is ${err.message}`);
return;
} else {
console.info('Invoke createAudioRenderer succeeded.');
audioRenderer = data;
}
});
```
查询当前播放实例的音效模式
```typescript
import { audio } from '@kit.AudioKit';
import { BusinessError } from '@kit.BasicServicesKit';
audioRenderer.getAudioEffectMode((err: BusinessError, effectMode: audio.AudioEffectMode) => {
if (err) {
console.error(`Failed to get params, code is ${err.code}, message is ${err.message}`);
return;
} else {
console.info(`getAudioEffectMode: ${effectMode}`);
}
});
```
设置当前播放实例的音效模式
关闭系统音效：
```typescript
import { audio } from '@kit.AudioKit';
import { BusinessError } from '@kit.BasicServicesKit';
audioRenderer.setAudioEffectMode(audio.AudioEffectMode.EFFECT_NONE, (err: BusinessError) => {
if (err) {
console.error(`Failed to set params, code is ${err.code}, message is ${err.message}`);
return;
} else {
console.info('Callback invoked to indicate a successful audio effect mode setting.');
}
});
```
开启系统音效默认模式：
```typescript
import { audio } from '@kit.AudioKit';
import { BusinessError } from '@kit.BasicServicesKit';
audioRenderer.setAudioEffectMode(audio.AudioEffectMode.EFFECT_DEFAULT, (err: BusinessError) => {
if (err) {
console.error(`Failed to set params, code is ${err.code}, message is ${err.message}`);
return;
} else {
console.info('Callback invoked to indicate a successful audio effect mode setting.');
}
});
```
全局查询音效模式
主要包括全局音效查询相应StreamUsage对应场景的音效模式。
对于播放音频类的应用，开发者需要关注该应用的音频流使用什么音效模式并做出相应的操作，比如音乐App播放时，应选择音乐场景下的模式。在使用查询接口前，开发者需要使用getStreamManager()创建一个AudioStreamManager音频流管理实例。
获取音频流管理接口
创建AudioStreamManager实例。在使用AudioStreamManager的API前，需要使用getStreamManager()创建一个AudioStreamManager实例。
```typescript
import { audio } from '@kit.AudioKit';
let audioManager = audio.getAudioManager();
let audioStreamManager = audioManager.getStreamManager();
```
查询对应场景的音效模式
```typescript
import { audio } from '@kit.AudioKit';
import { BusinessError } from '@kit.BasicServicesKit';
audioStreamManager.getAudioEffectInfoArray(audio.StreamUsage.STREAM_USAGE_MUSIC, async (err: BusinessError, audioEffectInfoArray: audio.AudioEffectInfoArray) => {
if (err) {
console.error('Failed to get effect info array');
return;
} else {
console.info(`getAudioEffectInfoArray: ${audioEffectInfoArray}`);
}
});
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audio-playback-stream-management-V14
爬取时间: 2025-04-28 19:50:13
来源: Huawei Developer
对于播放音频类的应用，开发者需要关注该应用的音频流的状态以做出相应的操作，比如监听到状态为播放中/暂停时，及时改变播放按钮的UI显示。
读取或监听应用内音频流状态变化
参考使用AudioRenderer开发音频播放功能或audio.createAudioRenderer，完成AudioRenderer的创建，然后可以通过以下两种方式查看音频流状态的变化：
-  方法1：直接查看AudioRenderer的state：
```typescript
import { audio } from '@kit.AudioKit';
let audioRendererState: audio.AudioState = audioRenderer.state;
console.info(`Current state is: ${audioRendererState }`)
```
-  方法2：注册stateChange监听AudioRenderer的状态变化：
```typescript
import { audio } from '@kit.AudioKit';
audioRenderer.on('stateChange', (rendererState: audio.AudioState) => {
console.info(`State change to: ${rendererState}`)
});
```
获取state后可对照AudioState来进行相应的操作，比如更改暂停播放按钮的显示等。
读取或监听所有音频流的变化
如果部分应用需要查询获取所有音频流的变化信息，可以通过AudioStreamManager读取或监听所有音频流的变化。
如下为音频流管理调用关系图：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165928.14665432245071078770784812623264:50001231000000:2800:DB63995A31D9DC058EB9B4CA9CBAD6270A7021057ECE0D77F80D737930D74AEF.png)
在进行应用开发的过程中，开发者需要使用getStreamManager()创建一个AudioStreamManager实例，进而通过该实例管理音频流。开发者可通过调用on('audioRendererChange')监听音频流的变化，在音频流状态变化、设备变化时获得通知。同时可通过off('audioRendererChange')取消相关事件的监听。另外，开发者可以主动调用getCurrentAudioRendererInfoArray()来查询播放流的唯一ID、播放流客户端的UID、音频流状态等信息。
详细API含义可参考AudioStreamManager。
开发步骤及注意事项
1.  创建AudioStreamManager实例。 在使用AudioStreamManager的API前，需要使用getStreamManager()创建一个AudioStreamManager实例。
```typescript
import { audio } from '@kit.AudioKit';
let audioManager = audio.getAudioManager();
let audioStreamManager = audioManager.getStreamManager();
```
2.  使用on('audioRendererChange')监听音频播放流的变化。 如果音频流监听应用需要在音频播放流状态变化、设备变化时获取通知，可以订阅该事件。
```typescript
import { audio } from '@kit.AudioKit';
audioStreamManager.on('audioRendererChange',  (AudioRendererChangeInfoArray: audio.AudioRendererChangeInfoArray) => {
for (let i = 0; i < AudioRendererChangeInfoArray.length; i++) {
let AudioRendererChangeInfo = AudioRendererChangeInfoArray[i];
console.info(`## RendererChange on is called for ${i} ##`);
console.info(`StreamId for ${i} is: ${AudioRendererChangeInfo.streamId}`);
console.info(`Content ${i} is: ${AudioRendererChangeInfo.rendererInfo.content}`);
console.info(`Stream ${i} is: ${AudioRendererChangeInfo.rendererInfo.usage}`);
console.info(`Flag ${i} is: ${AudioRendererChangeInfo.rendererInfo.rendererFlags}`);
for (let j = 0;j < AudioRendererChangeInfo.deviceDescriptors.length; j++) {
console.info(`Id: ${i} : ${AudioRendererChangeInfo.deviceDescriptors[j].id}`);
console.info(`Type: ${i} : ${AudioRendererChangeInfo.deviceDescriptors[j].deviceType}`);
console.info(`Role: ${i} : ${AudioRendererChangeInfo.deviceDescriptors[j].deviceRole}`);
console.info(`Name: ${i} : ${AudioRendererChangeInfo.deviceDescriptors[j].name}`);
console.info(`Address: ${i} : ${AudioRendererChangeInfo.deviceDescriptors[j].address}`);
console.info(`SampleRates: ${i} : ${AudioRendererChangeInfo.deviceDescriptors[j].sampleRates[0]}`);
console.info(`ChannelCount ${i} : ${AudioRendererChangeInfo.deviceDescriptors[j].channelCounts[0]}`);
console.info(`ChannelMask: ${i} : ${AudioRendererChangeInfo.deviceDescriptors[j].channelMasks}`);
}
}
});
```
3.  （可选）使用off('audioRendererChange')取消监听音频播放流变化。
```typescript
audioStreamManager.off('audioRendererChange');
console.info('RendererChange Off is called ');
```
4.  （可选）使用getCurrentAudioRendererInfoArray()获取所有音频播放流的信息。 该接口可获取音频播放流唯一ID、音频播放客户端的UID、音频状态以及音频播放器的其他信息。 对所有音频流状态进行监听的应用需要声明权限ohos.permission.USE_BLUETOOTH，否则无法获得实际的设备名称和设备地址信息，查询到的设备名称和设备地址（蓝牙设备的相关属性）将为空字符串。
```typescript
import { audio } from '@kit.AudioKit';
import { BusinessError } from '@kit.BasicServicesKit';
async function getCurrentAudioRendererInfoArray(): Promise<void> {
await audioStreamManager.getCurrentAudioRendererInfoArray().then((AudioRendererChangeInfoArray: audio.AudioRendererChangeInfoArray) => {
console.info(`getCurrentAudioRendererInfoArray  Get Promise is called `);
if (AudioRendererChangeInfoArray != null) {
for (let i = 0; i < AudioRendererChangeInfoArray.length; i++) {
let AudioRendererChangeInfo = AudioRendererChangeInfoArray[i];
console.info(`StreamId for ${i} is: ${AudioRendererChangeInfo.streamId}`);
console.info(`Content ${i} is: ${AudioRendererChangeInfo.rendererInfo.content}`);
console.info(`Stream ${i} is: ${AudioRendererChangeInfo.rendererInfo.usage}`);
console.info(`Flag ${i} is: ${AudioRendererChangeInfo.rendererInfo.rendererFlags}`);
for (let j = 0;j < AudioRendererChangeInfo.deviceDescriptors.length; j++) {
console.info(`Id: ${i} : ${AudioRendererChangeInfo.deviceDescriptors[j].id}`);
console.info(`Type: ${i} : ${AudioRendererChangeInfo.deviceDescriptors[j].deviceType}`);
console.info(`Role: ${i} : ${AudioRendererChangeInfo.deviceDescriptors[j].deviceRole}`);
console.info(`Name: ${i} : ${AudioRendererChangeInfo.deviceDescriptors[j].name}`);
console.info(`Address: ${i} : ${AudioRendererChangeInfo.deviceDescriptors[j].address}`);
console.info(`SampleRates: ${i} : ${AudioRendererChangeInfo.deviceDescriptors[j].sampleRates[0]}`);
console.info(`ChannelCount ${i} : ${AudioRendererChangeInfo.deviceDescriptors[j].channelCounts[0]}`);
console.info(`ChannelMask: ${i} : ${AudioRendererChangeInfo.deviceDescriptors[j].channelMasks}`);
}
}
}
}).catch((err: BusinessError ) => {
console.error(`Invoke getCurrentAudioRendererInfoArray failed, code is ${err.code}, message is ${err.message}`);
});
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audio-output-device-management-V14
爬取时间: 2025-04-28 19:50:27
来源: Huawei Developer
有时设备同时连接多个音频输出设备，需要指定音频输出设备进行音频播放，此时需要使用AudioRoutingManager接口进行输出设备的管理，API说明可以参考AudioRoutingManager API文档。
创建AudioRoutingManager实例
在使用AudioRoutingManager管理音频设备前，需要先导入模块并创建实例。
```typescript
import { audio } from '@kit.AudioKit';  // 导入audio模块
let audioManager = audio.getAudioManager();  // 需要先创建AudioManager实例
let audioRoutingManager = audioManager.getRoutingManager();  // 再调用AudioManager的方法创建AudioRoutingManager实例
```
支持的音频输出设备类型
目前支持的音频输出设备见下表：
| 名称 | 值 | 说明 |
| --- | --- | --- |
| EARPIECE | 1 | 听筒。 |
| SPEAKER | 2 | 扬声器。 |
| WIRED_HEADSET | 3 | 有线耳机，带麦克风。 |
| WIRED_HEADPHONES | 4 | 有线耳机，无麦克风。 |
| BLUETOOTH_SCO | 7 | 蓝牙设备SCO（Synchronous Connection Oriented）连接。 |
| BLUETOOTH_A2DP | 8 | 蓝牙设备A2DP（Advanced Audio Distribution Profile）连接。 |
| USB_HEADSET | 22 | USB耳机，带麦克风。 |
获取输出设备信息
使用getDevices()方法可以获取当前所有输出设备的信息。
```typescript
import { audio } from '@kit.AudioKit';
audioRoutingManager.getDevices(audio.DeviceFlag.OUTPUT_DEVICES_FLAG).then((data: audio.AudioDeviceDescriptors) => {
console.info('Promise returned to indicate that the device list is obtained.');
});
```
监听设备连接状态变化
可以设置监听事件来监听设备连接状态的变化，当有设备连接或断开时触发回调：
监听设备连接状态变化可以监听到全部的设备连接状态变化，不建议作为应用处理自动暂停的依据。应用如需处理自动暂停相关业务，可参考音频流输出设备变更原因。
```typescript
import { audio } from '@kit.AudioKit';
// 监听音频设备状态变化
audioRoutingManager.on('deviceChange', audio.DeviceFlag.OUTPUT_DEVICES_FLAG, (deviceChanged: audio.DeviceChangeAction) => {
console.info(`device change type : ${deviceChanged.type}`);  // 设备连接状态变化，0为连接，1为断开连接
console.info(`device descriptor size : ${deviceChanged.deviceDescriptors.length}`);
console.info(`device change descriptor : ${deviceChanged.deviceDescriptors[0].deviceRole}`);  // 设备角色
console.info(`device change descriptor : ${deviceChanged.deviceDescriptors[0].deviceType}`);  // 设备类型
});
// 取消监听音频设备状态变化
audioRoutingManager.off('deviceChange');
```
获取最高优先级输出设备信息
使用getPreferOutputDeviceForRendererInfo()方法, 可以获取当前最高优先级的输出设备。
最高优先级输出设备表示声音将在此设备输出的设备。
```typescript
import { audio } from '@kit.AudioKit';
import { BusinessError } from '@kit.BasicServicesKit';
let rendererInfo: audio.AudioRendererInfo = {
usage : audio.StreamUsage.STREAM_USAGE_MUSIC,
rendererFlags : 0
};
async function getPreferOutputDeviceForRendererInfo() {
audioRoutingManager.getPreferOutputDeviceForRendererInfo(rendererInfo).then((desc: audio.AudioDeviceDescriptors) => {
console.info(`device descriptor: ${desc}`);
}).catch((err: BusinessError) => {
console.error(`Result ERROR: ${err}`);
})
}
```
监听最高优先级输出设备变化
```typescript
import { audio } from '@kit.AudioKit';
let rendererInfo: audio.AudioRendererInfo = {
usage : audio.StreamUsage.STREAM_USAGE_MUSIC,
rendererFlags : 0
};
// 监听最高优先级输出设备变化
audioRoutingManager.on('preferOutputDeviceChangeForRendererInfo', rendererInfo, (desc: audio.AudioDeviceDescriptors) => {
console.info(`device change descriptor : ${desc[0].deviceRole}`);  // 设备角色
console.info(`device change descriptor : ${desc[0].deviceType}`);  // 设备类型
});
// 取消监听最高优先级输出设备变化
audioRoutingManager.off('preferOutputDeviceChangeForRendererInfo');
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audio-recording-V14
爬取时间: 2025-04-28 19:51:21
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audio-recording-overview-V14
爬取时间: 2025-04-28 19:51:34
来源: Huawei Developer
如何选择音频录制开发方式
系统提供了多样化的API，来帮助开发者完成音频录制的开发，不同的API适用于不同录音输出格式、音频使用场景或不同开发语言。因此，选择合适的音频录制API，有助于降低开发工作量，实现更佳的音频录制效果。
-  AudioCapturer：用于音频输入的的ArkTS/JS API，仅支持PCM格式，需要应用持续读取音频数据进行工作。应用可以在音频输出后添加数据处理，要求开发者具备音频处理的基础知识，适用于更专业、更多样化的媒体录制应用开发。
-  OpenSL ES：一套跨平台标准化的音频Native API，同样提供音频输入原子能力，仅支持PCM格式，适用于从其他嵌入式平台移植，或依赖在Native层实现音频输入功能的录音应用使用。
-  OHAudio：用于音频输入的Native API，此API在设计上实现归一，同时支持普通音频通路和低时延通路。仅支持PCM格式，适用于依赖Native层实现音频输入功能的场景。
除上述方式外，也可以通过Media Kit中的AVRecorder实现音频录制。
开发音频录制应用须知
-  应用可以调用麦克风录制音频，但该行为属于隐私敏感行为，在调用麦克风前，需要先向用户申请权限：ohos.permission.MICROPHONE。 如何使用和管理麦克风请参考管理麦克风。
-  如果需要持续录制或后台录制，请申请长时任务避免进入挂起（Suspend）状态。具体参考长时任务开发指导。
-  录制需要在前台启动，启动后可以退后台。在后台启动录制将会失败。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/using-audiocapturer-for-recording-V14
爬取时间: 2025-04-28 19:51:48
来源: Huawei Developer
AudioCapturer是音频采集器，用于录制PCM（Pulse Code Modulation）音频数据，适合有音频开发经验的开发者实现更灵活的录制功能。
开发指导
使用AudioCapturer录制音频涉及到AudioCapturer实例的创建、音频采集参数的配置、采集的开始与停止、资源的释放等。本开发指导将以一次录制音频数据的过程为例，向开发者讲解如何使用AudioCapturer进行音频录制，建议搭配AudioCapturer的API说明阅读。
下图展示了AudioCapturer的状态变化，在创建实例后，调用对应的方法可以进入指定的状态实现对应的行为。需要注意的是在确定的状态执行不合适的方法可能导致AudioCapturer发生错误，建议开发者在调用状态转换的方法前进行状态检查，避免程序运行产生预期以外的结果。
图1AudioCapturer状态变化示意图
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165928.99892211599750446892071756622913:50001231000000:2800:D1EAEAC60199FDC5CEF3ED969F15A9E2A4B38AD1B9F942F3B54C1F77DB4E594D.png)
使用on('stateChange')方法可以监听AudioCapturer的状态变化，每个状态对应值与说明见AudioState。
开发步骤及注意事项
1.  配置音频采集参数并创建AudioCapturer实例，音频采集参数的详细信息可以查看AudioCapturerOptions。 当设置Mic音频源（即SourceType为SOURCE_TYPE_MIC、SOURCE_TYPE_VOICE_RECOGNITION、SOURCE_TYPE_VOICE_COMMUNICATION、SOURCE_TYPE_VOICE_MESSAGE）时，需要申请麦克风权限ohos.permission.MICROPHONE，申请方式参考：向用户申请授权。
```typescript
import { audio } from '@kit.AudioKit';
let audioStreamInfo: audio.AudioStreamInfo = {
samplingRate: audio.AudioSamplingRate.SAMPLE_RATE_48000, // 采样率
channels: audio.AudioChannel.CHANNEL_2, // 通道
sampleFormat: audio.AudioSampleFormat.SAMPLE_FORMAT_S16LE, // 采样格式
encodingType: audio.AudioEncodingType.ENCODING_TYPE_RAW // 编码格式
};
let audioCapturerInfo: audio.AudioCapturerInfo = {
source: audio.SourceType.SOURCE_TYPE_MIC,
capturerFlags: 0
};
let audioCapturerOptions: audio.AudioCapturerOptions = {
streamInfo: audioStreamInfo,
capturerInfo: audioCapturerInfo
};
audio.createAudioCapturer(audioCapturerOptions, (err, data) => {
if (err) {
console.error(`Invoke createAudioCapturer failed, code is ${err.code}, message is ${err.message}`);
} else {
console.info('Invoke createAudioCapturer succeeded.');
let audioCapturer = data;
}
});
```
2.  调用on('readData')方法，订阅监听音频数据读入回调。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
import { fileIo as fs } from '@kit.CoreFileKit';
class Options {
offset?: number;
length?: number;
}
let bufferSize: number = 0;
let path = getContext().cacheDir;
let filePath = path + '/StarWars10s-2C-48000-4SW.pcm';
let file: fs.File = fs.openSync(filePath, fs.OpenMode.READ_WRITE | fs.OpenMode.CREATE);
let readDataCallback = (buffer: ArrayBuffer) => {
let options: Options = {
offset: bufferSize,
length: buffer.byteLength
}
fs.writeSync(file.fd, buffer, options);
bufferSize += buffer.byteLength;
};
audioCapturer.on('readData', readDataCallback);
```
3.  调用start()方法进入running状态，开始录制音频。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
audioCapturer.start((err: BusinessError) => {
if (err) {
console.error(`Capturer start failed, code is ${err.code}, message is ${err.message}`);
} else {
console.info('Capturer start success.');
}
});
```
4.  调用stop()方法停止录制。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
audioCapturer.stop((err: BusinessError) => {
if (err) {
console.error(`Capturer stop failed, code is ${err.code}, message is ${err.message}`);
} else {
console.info('Capturer stopped.');
}
});
```
5.  调用release()方法销毁实例，释放资源。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
audioCapturer.release((err: BusinessError) => {
if (err) {
console.error(`capturer release failed, code is ${err.code}, message is ${err.message}`);
} else {
console.info('capturer released.');
}
});
```
完整示例
下面展示了使用AudioCapturer录制音频的完整示例代码。
```typescript
import { audio } from '@kit.AudioKit';
import { BusinessError } from '@kit.BasicServicesKit';
import { fileIo as fs } from '@kit.CoreFileKit';
const TAG = 'AudioCapturerDemo';
class Options {
offset?: number;
length?: number;
}
let bufferSize: number = 0;
let audioCapturer: audio.AudioCapturer | undefined = undefined;
let audioStreamInfo: audio.AudioStreamInfo = {
samplingRate: audio.AudioSamplingRate.SAMPLE_RATE_48000, // 采样率
channels: audio.AudioChannel.CHANNEL_2, // 通道
sampleFormat: audio.AudioSampleFormat.SAMPLE_FORMAT_S16LE, // 采样格式
encodingType: audio.AudioEncodingType.ENCODING_TYPE_RAW // 编码格式
};
let audioCapturerInfo: audio.AudioCapturerInfo = {
source: audio.SourceType.SOURCE_TYPE_MIC, // 音源类型
capturerFlags: 0 // 音频采集器标志
};
let audioCapturerOptions: audio.AudioCapturerOptions = {
streamInfo: audioStreamInfo,
capturerInfo: audioCapturerInfo
};
let path = getContext().cacheDir;
let filePath = path + '/StarWars10s-2C-48000-4SW.pcm';
let file: fs.File = fs.openSync(filePath, fs.OpenMode.READ_WRITE | fs.OpenMode.CREATE);
let readDataCallback = (buffer: ArrayBuffer) => {
let options: Options = {
offset: bufferSize,
length: buffer.byteLength
}
fs.writeSync(file.fd, buffer, options);
bufferSize += buffer.byteLength;
};
// 初始化，创建实例，设置监听事件
function init() {
audio.createAudioCapturer(audioCapturerOptions, (err, capturer) => { // 创建AudioCapturer实例
if (err) {
console.error(`Invoke createAudioCapturer failed, code is ${err.code}, message is ${err.message}`);
return;
}
console.info(`${TAG}: create AudioCapturer success`);
audioCapturer = capturer;
if (audioCapturer !== undefined) {
(audioCapturer as audio.AudioCapturer).on('readData', readDataCallback);
}
});
}
// 开始一次音频采集
function start() {
if (audioCapturer !== undefined) {
let stateGroup = [audio.AudioState.STATE_PREPARED, audio.AudioState.STATE_PAUSED, audio.AudioState.STATE_STOPPED];
if (stateGroup.indexOf((audioCapturer as audio.AudioCapturer).state.valueOf()) === -1) { // 当且仅当状态为STATE_PREPARED、STATE_PAUSED和STATE_STOPPED之一时才能启动采集
console.error(`${TAG}: start failed`);
return;
}
// 启动采集
(audioCapturer as audio.AudioCapturer).start((err: BusinessError) => {
if (err) {
console.error('Capturer start failed.');
} else {
console.info('Capturer start success.');
}
});
}
}
// 停止采集
function stop() {
if (audioCapturer !== undefined) {
// 只有采集器状态为STATE_RUNNING或STATE_PAUSED的时候才可以停止
if ((audioCapturer as audio.AudioCapturer).state.valueOf() !== audio.AudioState.STATE_RUNNING && (audioCapturer as audio.AudioCapturer).state.valueOf() !== audio.AudioState.STATE_PAUSED) {
console.info('Capturer is not running or paused');
return;
}
//停止采集
(audioCapturer as audio.AudioCapturer).stop((err: BusinessError) => {
if (err) {
console.error('Capturer stop failed.');
} else {
fs.close(file);
console.info('Capturer stop success.');
}
});
}
}
// 销毁实例，释放资源
function release() {
if (audioCapturer !== undefined) {
// 采集器状态不是STATE_RELEASED或STATE_NEW状态，才能release
if ((audioCapturer as audio.AudioCapturer).state.valueOf() === audio.AudioState.STATE_RELEASED || (audioCapturer as audio.AudioCapturer).state.valueOf() === audio.AudioState.STATE_NEW) {
console.info('Capturer already released');
return;
}
//释放资源
(audioCapturer as audio.AudioCapturer).release((err: BusinessError) => {
if (err) {
console.error('Capturer release failed.');
} else {
console.info('Capturer release success.');
}
});
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/using-ohaudio-for-recording-V14
爬取时间: 2025-04-28 19:52:02
来源: Huawei Developer
OHAudio是系统在API version 10中引入的一套C API，此API在设计上实现归一，同时支持普通音频通路和低时延通路。仅支持PCM格式，适用于依赖Native层实现音频输入功能的场景。
OHAudio音频频录状态变化示意图：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165928.03499277577952407135070757474427:50001231000000:2800:739ADF20A8A8FD5D1D983C3D1ADA8C93DF46D3B540ADA9DAEC3980CD4143D8D3.png)
使用入门
开发者要使用OHAudio提供的录制能力，需要添加对应的头文件。
在 CMake 脚本中链接动态库
添加头文件
开发者通过引入<native_audiostreambuilder.h>和<native_audiocapturer.h>头文件，使用音频录制相关API。
音频流构造器
OHAudio提供OH_AudioStreamBuilder接口，遵循构造器设计模式，用于构建音频流。开发者需要根据业务场景，指定对应的OH_AudioStream_Type。
OH_AudioStream_Type包含两种类型：
使用OH_AudioStreamBuilder_Create创建构造器示例：
在音频业务结束之后，开发者应该执行OH_AudioStreamBuilder_Destroy接口来销毁构造器。
开发步骤及注意事项
详细的API说明请参考OHAudio API参考。
开发者可以通过以下几个步骤来实现一个简单的录制功能。
1.  创建构造器
2.  配置音频流参数 创建音频录制构造器后，可以设置音频流所需要的参数，可以参考下面的案例。 同样，音频录制的音频数据要通过回调接口读入，开发者要实现回调接口，使用OH_AudioStreamBuilder_SetCapturerCallback设置回调函数。回调函数的声明请查看OH_AudioCapturer_Callbacks。
3.  设置音频回调函数 多音频并发处理可参考文档处理音频焦点事件，仅接口语言差异。 为了避免不可预期的行为，在设置音频回调函数时，可以通过下面两种方式中的任意一种来设置音频回调函数： 请确保OH_AudioCapturer_Callbacks的每一个回调都被自定义的回调方法或空指针初始化。 使用前，初始化并清零结构体。
4.  请确保OH_AudioCapturer_Callbacks的每一个回调都被自定义的回调方法或空指针初始化。
5.  使用前，初始化并清零结构体。
6.  构造录制音频流
7.  使用音频流 录制音频流包含下面接口，用来实现对音频流的控制。
8.  释放构造器 构造器不再使用时，需要释放相关资源。
-  请确保OH_AudioCapturer_Callbacks的每一个回调都被自定义的回调方法或空指针初始化。
-  使用前，初始化并清零结构体。
| 接口 | 说明 |
| --- | --- |
| OH_AudioStream_Result OH_AudioCapturer_Start(OH_AudioCapturer* capturer) | 开始录制 |
| OH_AudioStream_Result OH_AudioCapturer_Pause(OH_AudioCapturer* capturer) | 暂停录制 |
| OH_AudioStream_Result OH_AudioCapturer_Stop(OH_AudioCapturer* capturer) | 停止录制 |
| OH_AudioStream_Result OH_AudioCapturer_Flush(OH_AudioCapturer* capturer) | 释放缓存数据 |
| OH_AudioStream_Result OH_AudioCapturer_Release(OH_AudioCapturer* capturer) | 释放录制实例 |
设置低时延模式
当设备支持低时延通路时，开发者可以使用低时延模式创建音频录制构造器，获得更高质量的音频体验。
开发流程与普通录制场景一致，仅需要在创建音频录制构造器时，调用OH_AudioStreamBuilder_SetLatencyMode()设置低时延模式。
当音频录制场景OH_AudioStream_SourceType为AUDIOSTREAM_SOURCE_TYPE_VOICE_COMMUNICATION时，不支持主动设置低时延模式，系统会根据设备的能力，决策输出的音频通路。
开发示例
示例代码

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/mic-management-V14
爬取时间: 2025-04-28 19:52:16
来源: Huawei Developer
因为在录制过程中需要使用麦克风录制相关音频数据，所以建议开发者在调用录制接口前查询麦克风状态，并在录制过程中监听麦克风的状态变化，避免影响录制效果。
在音频录制过程中，用户可以将麦克风静音，此时录音过程正常进行，录制生成的数据文件的大小随录制时长递增，但写入文件的数据均为0，即无声数据（空白数据）。
开发步骤及注意事项
在AudioVolumeGroupManager中提供了管理麦克风状态的方法，接口的详细说明请参考API文档。
1.  创建audioVolumeGroupManager对象。
```typescript
import { audio } from '@kit.AudioKit';
let audioVolumeGroupManager: audio.AudioVolumeGroupManager;
// 创建audioVolumeGroupManager对象
async function loadVolumeGroupManager() {
const groupid = audio.DEFAULT_VOLUME_GROUP_ID;
audioVolumeGroupManager = await audio.getAudioManager().getVolumeManager().getVolumeGroupManager(groupid);
console.info('audioVolumeGroupManager create success.');
}
```
2.  调用isMicrophoneMute查询麦克风当前静音状态，返回true为静音，false为非静音。
```typescript
// 查询麦克风是否静音
async function isMicrophoneMute() {
await audioVolumeGroupManager.isMicrophoneMute().then((value: boolean) => {
console.info(`isMicrophoneMute is: ${value}.`);
});
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audio-recording-stream-management-V14
爬取时间: 2025-04-28 19:52:30
来源: Huawei Developer
对于录制音频类的应用，开发者需要关注该应用的音频流的状态以做出相应的操作，比如监听到状态为结束时，及时提示用户录制已结束。
读取或监听应用内音频流状态变化
参考使用AudioCapturer开发音频录制功能或audio.createAudioCapturer，完成AudioCapturer的创建，然后可以通过以下两种方式查看音频流状态的变化：
-  方法1：直接查看AudioCapturer的state：
```typescript
let audioCapturerState: audio.AudioState = audioCapturer.state;
console.info(`Current state is: ${audioCapturerState }`)
```
-  方法2：注册stateChange监听AudioCapturer的状态变化：
```typescript
audioCapturer.on('stateChange', (capturerState: audio.AudioState) => {
console.info(`State change to: ${capturerState}`)
});
```
获取state后可对照AudioState来进行相应的操作，比如显示录制结束的提示等。
读取或监听所有录制流的变化
如果部分应用需要查询获取所有音频流的变化信息，可以通过AudioStreamManager读取或监听所有音频流的变化。
如下为音频流管理调用关系图：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165929.58778419722421444690858389452593:50001231000000:2800:EA09C339C5F87D63865C0011264E12F480226352C28FD5030E231464E78CEE10.png)
在进行应用开发的过程中，开发者需要使用getStreamManager()创建一个AudioStreamManager实例，进而通过该实例管理音频流。开发者可通过调用on('audioCapturerChange')监听音频流的变化，在音频流状态变化、设备变化时获得通知，同时可通过off('audioCapturerChange')取消相关事件的监听。另外，开发者可以通过主动调用getCurrentAudioCapturerInfoArray()查询录制流的唯一ID、录制流客户端的UID、以及流状态等信息。
详细API含义可参考AudioStreamManager。
开发步骤及注意事项
1.  创建AudioStreamManager实例。 在使用AudioStreamManager的API前，需要使用getStreamManager()创建一个AudioStreamManager实例。
```typescript
import { audio } from '@kit.AudioKit';
import { BusinessError } from '@kit.BasicServicesKit';
let audioManager = audio.getAudioManager();
let audioStreamManager = audioManager.getStreamManager();
```
2.  使用on('audioCapturerChange')监听音频录制流更改事件。 如果音频流监听应用需要在音频录制流状态变化、设备变化时获取通知，可以订阅该事件。
```typescript
audioStreamManager.on('audioCapturerChange', (AudioCapturerChangeInfoArray: audio.AudioCapturerChangeInfoArray) =>  {
for (let i = 0; i < AudioCapturerChangeInfoArray.length; i++) {
console.info(`## CapChange on is called for element ${i} ##`);
console.info(`StreamId for ${i} is: ${AudioCapturerChangeInfoArray[i].streamId}`);
console.info(`Source for ${i} is: ${AudioCapturerChangeInfoArray[i].capturerInfo.source}`);
console.info(`Flag  ${i} is: ${AudioCapturerChangeInfoArray[i].capturerInfo.capturerFlags}`);
let devDescriptor: audio.AudioDeviceDescriptors = AudioCapturerChangeInfoArray[i].deviceDescriptors;
for (let j = 0; j < AudioCapturerChangeInfoArray[i].deviceDescriptors.length; j++) {
console.info(`Id: ${i} : ${AudioCapturerChangeInfoArray[i].deviceDescriptors[j].id}`);
console.info(`Type: ${i} : ${AudioCapturerChangeInfoArray[i].deviceDescriptors[j].deviceType}`);
console.info(`Role: ${i} : ${AudioCapturerChangeInfoArray[i].deviceDescriptors[j].deviceRole}`);
console.info(`Name: ${i} : ${AudioCapturerChangeInfoArray[i].deviceDescriptors[j].name}`);
console.info(`Address: ${i} : ${AudioCapturerChangeInfoArray[i].deviceDescriptors[j].address}`);
console.info(`SampleRates: ${i} : ${AudioCapturerChangeInfoArray[i].deviceDescriptors[j].sampleRates[0]}`);
console.info(`ChannelCounts ${i} : ${AudioCapturerChangeInfoArray[i].deviceDescriptors[j].channelCounts[0]}`);
console.info(`ChannelMask: ${i} : ${AudioCapturerChangeInfoArray[i].deviceDescriptors[j].channelMasks}`);
}
}
});
```
3.  （可选）使用off('audioCapturerChange')取消监听音频录制流变化。
```typescript
audioStreamManager.off('audioCapturerChange');
console.info('CapturerChange Off is called');
```
4.  （可选）使用getCurrentAudioCapturerInfoArray()获取当前音频录制流的信息。 该接口可获取音频录制流唯一ID、音频录制客户端的UID、音频状态以及音频捕获器的其他信息。 对所有音频流状态进行监听的应用需要声明权限ohos.permission.USE_BLUETOOTH，否则无法获得实际的设备名称和设备地址信息，查询到的设备名称和设备地址（蓝牙设备的相关属性）将为空字符串。
```typescript
async function getCurrentAudioCapturerInfoArray(){
await audioStreamManager.getCurrentAudioCapturerInfoArray().then((AudioCapturerChangeInfoArray: audio.AudioCapturerChangeInfoArray) => {
console.info('getCurrentAudioCapturerInfoArray  Get Promise Called ');
if (AudioCapturerChangeInfoArray != null) {
for (let i = 0; i < AudioCapturerChangeInfoArray.length; i++) {
console.info(`StreamId for ${i} is: ${AudioCapturerChangeInfoArray[i].streamId}`);
console.info(`Source for ${i} is: ${AudioCapturerChangeInfoArray[i].capturerInfo.source}`);
console.info(`Flag  ${i} is: ${AudioCapturerChangeInfoArray[i].capturerInfo.capturerFlags}`);
for (let j = 0; j < AudioCapturerChangeInfoArray[i].deviceDescriptors.length; j++) {
console.info(`Id: ${i} : ${AudioCapturerChangeInfoArray[i].deviceDescriptors[j].id}`);
console.info(`Type: ${i} : ${AudioCapturerChangeInfoArray[i].deviceDescriptors[j].deviceType}`);
console.info(`Role: ${i} : ${AudioCapturerChangeInfoArray[i].deviceDescriptors[j].deviceRole}`);
console.info(`Name: ${i} : ${AudioCapturerChangeInfoArray[i].deviceDescriptors[j].name}`);
console.info(`Address: ${i} : ${AudioCapturerChangeInfoArray[i].deviceDescriptors[j].address}`);
console.info(`SampleRates: ${i} : ${AudioCapturerChangeInfoArray[i].deviceDescriptors[j].sampleRates[0]}`);
console.info(`ChannelCounts ${i} : ${AudioCapturerChangeInfoArray[i].deviceDescriptors[j].channelCounts[0]}`);
console.info(`ChannelMask: ${i} : ${AudioCapturerChangeInfoArray[i].deviceDescriptors[j].channelMasks}`);
}
}
}
}).catch((err: BusinessError) => {
console.error(`Invoke getCurrentAudioCapturerInfoArray failed, code is ${err.code}, message is ${err.message}`);
});
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audio-input-device-management-V14
爬取时间: 2025-04-28 19:52:43
来源: Huawei Developer
有时设备同时连接多个音频输入设备，需要指定音频输入设备进行音频录制，此时需要使用AudioRoutingManager接口进行输入设备的管理，API说明可以参考AudioRoutingManager API文档。
创建AudioRoutingManager实例
在使用AudioRoutingManager管理音频设备前，需要先导入模块并创建实例。
```typescript
import { audio } from '@kit.AudioKit';  // 导入audio模块
let audioManager = audio.getAudioManager();  // 需要先创建AudioManager实例
let audioRoutingManager = audioManager.getRoutingManager();  // 再调用AudioManager的方法创建AudioRoutingManager实例
```
支持的音频输入设备类型
目前支持的音频输入设备见下表：
| 名称 | 值 | 说明 |
| --- | --- | --- |
| WIRED_HEADSET | 3 | 有线耳机，带麦克风。 |
| BLUETOOTH_SCO | 7 | 蓝牙设备SCO（Synchronous Connection Oriented）连接。 |
| MIC | 15 | 麦克风。 |
| USB_HEADSET | 22 | USB耳机，带麦克风。 |
获取输入设备信息
使用getDevices()方法可以获取当前所有输入设备的信息。
```typescript
import { audio } from '@kit.AudioKit';
audioRoutingManager.getDevices(audio.DeviceFlag.INPUT_DEVICES_FLAG).then((data: audio.AudioDeviceDescriptors) => {
console.info('Promise returned to indicate that the device list is obtained.');
});
```
监听设备连接状态变化
可以设置监听事件来监听设备连接状态的变化，当有设备连接或断开时触发回调：
```typescript
import { audio } from '@kit.AudioKit';
// 监听音频设备状态变化
audioRoutingManager.on('deviceChange', audio.DeviceFlag.INPUT_DEVICES_FLAG, (deviceChanged: audio.DeviceChangeAction) => {
console.info('device change type : ' + deviceChanged.type);  // 设备连接状态变化，0为连接，1为断开连接
console.info('device descriptor size : ' + deviceChanged.deviceDescriptors.length);
console.info('device change descriptor : ' + deviceChanged.deviceDescriptors[0].deviceRole);  // 设备角色
console.info('device change descriptor : ' + deviceChanged.deviceDescriptors[0].deviceType);  // 设备类型
});
// 取消监听音频设备状态变化
audioRoutingManager.off('deviceChange', (deviceChanged: audio.DeviceChangeAction) => {
console.info('Should be no callback.');
});
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audio-call-V14
爬取时间: 2025-04-28 19:52:57
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audio-call-overview-V14
爬取时间: 2025-04-28 19:53:10
来源: Huawei Developer
常用的音频通话模式包括VoIP通话和蜂窝通话。
-  VoIP通话： VoIP（Voice over Internet Protocol）通话是指基于互联网协议（IP）进行通讯的一种语音通话技术。VoIP通话会将通话信息打包成数据包，通过网络进行传输，因此VoIP通话对网络要求较高，通话质量与网络连接速度紧密相关。
-  蜂窝通话（仅对系统应用开放）： 蜂窝通话是指传统的电话功能，由运营商提供服务，目前仅对系统应用开放，未向第三方应用提供开发接口。
在开发音频通话相关功能时，开发者可以根据实际情况，检查当前的音频场景模式和铃声模式，以使用相应的音频处理策略。
音频场景模式
应用使用音频通话相关功能时，系统会切换至与通话相关的音频场景模式（AudioScene），当前预置了多种音频场景，包括响铃、通话、语音聊天等，在不同的场景下，系统会采用不同的策略来处理音频。
当前预置的音频场景：
-  AUDIO_SCENE_DEFAULT：默认音频场景，音频通话之外的场景均可使用。
-  AUDIO_SCENE_VOICE_CHAT：语音聊天音频场景，VoIP通话时使用。
应用可通过AudioManager的getAudioScene来获取当前的音频场景模式。当应用开始或结束使用音频通话相关功能时，可通过此方法检查系统是否已切换为合适的音频场景模式。
铃声模式
在用户进入到音频通话时，应用可以使用铃声或振动来提示用户。系统通过调整铃声模式（AudioRingMode），实现便捷地管理铃声音量，并调整设备的振动模式。
当前预置的三种铃声模式：
-  RINGER_MODE_SILENT：静音模式，此模式下铃声音量为零（即静音）。
-  RINGER_MODE_VIBRATE：振动模式，此模式下铃声音量为零，设备振动开启（即响铃时静音，触发振动）。
-  RINGER_MODE_NORMAL：响铃模式，此模式下铃声音量正常。
应用可以调用AudioVolumeGroupManager中的getRingerMode获取当前的铃声模式，以便采取合适的提示策略。
如果应用希望及时获取铃声模式的变化情况，可以通过AudioVolumeGroupManager中的on('ringerModeChange')监听铃声模式变化事件，使应用在铃声模式发生变化时及时收到通知，方便应用做出相应的调整。
通话场景音频设备切换
在通话场景下，系统会根据默认优先级选择合适的音频设备。应用可以根据需要，切换音频设备。
切换方式可参考AVSession Kit使用通话设备切换组件。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audio-call-development-V14
爬取时间: 2025-04-28 19:53:24
来源: Huawei Developer
在音频通话场景下，音频输出（播放对端声音）和音频输入（录制本端声音）会同时进行，应用可以通过使用AudioRenderer来实现音频输出，通过使用AudioCapturer来实现音频输入，同时使用AudioRenderer和AudioCapturer即可实现音频通话功能。
在音频通话开始和结束时，应用可以自行检查当前的音频场景模式和铃声模式，以便采取合适的音频管理及提示策略。
以下代码示范了同时使用AudioRenderer和AudioCapturer实现音频通话功能的基本过程，其中未包含音频通话数据的传输过程，实际开发中，需要将网络传输来的对端通话数据解码播放，此处仅以读取音频文件的数据代替；同时需要将本端录制的通话数据编码打包，通过网络发送给对端，此处仅以将数据写入音频文件代替。
使用AudioRenderer播放对端的通话声音
该过程与使用AudioRenderer开发音频播放功能过程相似，关键区别在于audioRendererInfo参数和音频数据来源。audioRendererInfo参数中，音频内容类型需设置为语音：CONTENT_TYPE_SPEECH，音频流使用类型需设置为VOIP通话：STREAM_USAGE_VOICE_COMMUNICATION。
```typescript
import { audio } from '@kit.AudioKit';
import { fileIo as fs } from '@kit.CoreFileKit';
import { BusinessError } from '@kit.BasicServicesKit';
const TAG = 'VoiceCallDemoForAudioRenderer';
// 与使用AudioRenderer开发音频播放功能过程相似，关键区别在于audioRendererInfo参数和音频数据来源
class Options {
offset?: number;
length?: number;
}
let bufferSize: number = 0;
let renderModel: audio.AudioRenderer | undefined = undefined;
let audioStreamInfo: audio.AudioStreamInfo = {
samplingRate: audio.AudioSamplingRate.SAMPLE_RATE_48000, // 采样率
channels: audio.AudioChannel.CHANNEL_2, // 通道
sampleFormat: audio.AudioSampleFormat.SAMPLE_FORMAT_S16LE, // 采样格式
encodingType: audio.AudioEncodingType.ENCODING_TYPE_RAW // 编码格式
};
let audioRendererInfo: audio.AudioRendererInfo = {
// 需使用通话场景相应的参数
usage: audio.StreamUsage.STREAM_USAGE_VOICE_COMMUNICATION, // 音频流使用类型：VOIP通话
rendererFlags: 0 // 音频渲染器标志：默认为0即可
};
let audioRendererOptions: audio.AudioRendererOptions = {
streamInfo: audioStreamInfo,
rendererInfo: audioRendererInfo
};
let path = getContext().cacheDir;
// 确保该沙箱路径下存在该资源
let filePath = path + '/StarWars10s-2C-48000-4SW.wav';
let file: fs.File = fs.openSync(filePath, fs.OpenMode.READ_ONLY);
let writeDataCallback = (buffer: ArrayBuffer) => {
let options: Options = {
offset: bufferSize,
length: buffer.byteLength
};
fs.readSync(file.fd, buffer, options);
bufferSize += buffer.byteLength;
};
// 初始化，创建实例，设置监听事件
audio.createAudioRenderer(audioRendererOptions, (err: BusinessError, renderer: audio.AudioRenderer) => { // 创建AudioRenderer实例
if (!err) {
console.info(`${TAG}: creating AudioRenderer success`);
renderModel = renderer;
if (renderModel !== undefined) {
renderModel.on('stateChange', (state: audio.AudioState) => { // 设置监听事件，当转换到指定的状态时触发回调
if (state == 1) {
console.info('audio renderer state is: STATE_PREPARED');
}
if (state == 2) {
console.info('audio renderer state is: STATE_RUNNING');
}
});
renderModel.on('markReach', 1000, (position: number) => { // 订阅markReach事件，当渲染的帧数达到1000帧时触发回调
if (position == 1000) {
console.info('ON Triggered successfully');
}
});
renderModel.on('writeData', writeDataCallback);
}
} else {
console.info(`${TAG}: creating AudioRenderer failed, error: ${err.message}`);
}
});
// 开始一次音频渲染
async function start() {
if (renderModel !== undefined) {
let stateGroup: number[] = [audio.AudioState.STATE_PREPARED, audio.AudioState.STATE_PAUSED, audio.AudioState.STATE_STOPPED];
if (stateGroup.indexOf(renderModel.state.valueOf()) === -1) { // 当且仅当状态为STATE_PREPARED、STATE_PAUSED和STATE_STOPPED之一时才能启动渲染
console.error(TAG + 'start failed');
return;
}
renderModel.start((err: BusinessError) => {
if (err) {
console.error('Renderer start failed.');
} else {
console.info('Renderer start success.');
}
});
}
}
// 暂停渲染
async function pause() {
if (renderModel !== undefined) {
// 只有渲染器状态为STATE_RUNNING的时候才能暂停
if (renderModel.state.valueOf() !== audio.AudioState.STATE_RUNNING) {
console.info('Renderer is not running');
return;
}
await renderModel.pause(); // 暂停渲染
if (renderModel.state.valueOf() === audio.AudioState.STATE_PAUSED) {
console.info('Renderer is paused.');
} else {
console.error('Pausing renderer failed.');
}
}
}
// 停止渲染
async function stop() {
if (renderModel !== undefined) {
// 只有渲染器状态为STATE_RUNNING或STATE_PAUSED的时候才可以停止
if (renderModel.state.valueOf() !== audio.AudioState.STATE_RUNNING && renderModel.state.valueOf() !== audio.AudioState.STATE_PAUSED) {
console.info('Renderer is not running or paused.');
return;
}
await renderModel.stop(); // 停止渲染
if (renderModel.state.valueOf() === audio.AudioState.STATE_STOPPED) {
console.info('Renderer stopped.');
} else {
console.error('Stopping renderer failed.');
}
}
}
// 销毁实例，释放资源
async function release() {
if (renderModel !== undefined) {
// 渲染器状态不是STATE_RELEASED状态，才能release
if (renderModel.state.valueOf() === audio.AudioState.STATE_RELEASED) {
console.info('Renderer already released');
return;
}
await renderModel.release(); // 释放资源
if (renderModel.state.valueOf() === audio.AudioState.STATE_RELEASED) {
console.info('Renderer released');
} else {
console.error('Renderer release failed.');
}
}
}
```
使用AudioCapturer录制本端的通话声音
该过程与使用AudioCapturer开发音频录制功能过程相似，关键区别在于audioCapturerInfo参数和音频数据流向。audioCapturerInfo参数中音源类型需设置为语音通话：SOURCE_TYPE_VOICE_COMMUNICATION。
所有录制均需要申请麦克风权限：ohos.permission.MICROPHONE，申请方式请参考向用户申请授权。
```typescript
import { audio } from '@kit.AudioKit';
import { fileIo as fs } from '@kit.CoreFileKit';
import { BusinessError } from '@kit.BasicServicesKit';
const TAG = 'VoiceCallDemoForAudioCapturer';
class Options {
offset?: number;
length?: number;
}
// 与使用AudioCapturer开发音频录制功能过程相似，关键区别在于audioCapturerInfo参数和音频数据流向
let bufferSize: number = 0;
let audioCapturer: audio.AudioCapturer | undefined = undefined;
let audioStreamInfo: audio.AudioStreamInfo = {
samplingRate: audio.AudioSamplingRate.SAMPLE_RATE_48000, // 采样率
channels: audio.AudioChannel.CHANNEL_2, // 通道
sampleFormat: audio.AudioSampleFormat.SAMPLE_FORMAT_S16LE, // 采样格式
encodingType: audio.AudioEncodingType.ENCODING_TYPE_RAW // 编码格式
};
let audioCapturerInfo: audio.AudioCapturerInfo = {
// 需使用通话场景相应的参数
source: audio.SourceType.SOURCE_TYPE_VOICE_COMMUNICATION, // 音源类型：语音通话
capturerFlags: 0 // 音频采集器标志：默认为0即可
};
let audioCapturerOptions: audio.AudioCapturerOptions = {
streamInfo: audioStreamInfo,
capturerInfo: audioCapturerInfo
};
let path = getContext().cacheDir;
let filePath = path + '/StarWars10s-2C-48000-4SW.wav';
let file: fs.File = fs.openSync(filePath, fs.OpenMode.READ_WRITE | fs.OpenMode.CREATE);
let readDataCallback = (buffer: ArrayBuffer) => {
let options: Options = {
offset: bufferSize,
length: buffer.byteLength
};
fs.writeSync(file.fd, buffer, options);
bufferSize += buffer.byteLength;
};
// 初始化，创建实例，设置监听事件
async function init() {
audio.createAudioCapturer(audioCapturerOptions, (err: BusinessError, capturer: audio.AudioCapturer) => { // 创建AudioCapturer实例
if (err) {
console.error(`Invoke createAudioCapturer failed, code is ${err.code}, message is ${err.message}`);
return;
}
console.info(`${TAG}: create AudioCapturer success`);
audioCapturer = capturer;
if (audioCapturer !== undefined) {
audioCapturer.on('markReach', 1000, (position: number) => { // 订阅markReach事件，当采集的帧数达到1000帧时触发回调
if (position === 1000) {
console.info('ON Triggered successfully');
}
});
audioCapturer.on('periodReach', 2000, (position: number) => { // 订阅periodReach事件，当采集的帧数每达到2000时触发回调
if (position === 2000) {
console.info('ON Triggered successfully');
}
});
audioCapturer.on('readData', readDataCallback);
}
});
}
// 开始一次音频采集
async function start() {
if (audioCapturer !== undefined) {
let stateGroup: number[] = [audio.AudioState.STATE_PREPARED, audio.AudioState.STATE_PAUSED, audio.AudioState.STATE_STOPPED];
if (stateGroup.indexOf(audioCapturer.state.valueOf()) === -1) { // 当且仅当状态为STATE_PREPARED、STATE_PAUSED和STATE_STOPPED之一时才能启动采集
console.error(`${TAG}: start failed`);
return;
}
audioCapturer.start((err: BusinessError) => {
if (err) {
console.error('Capturer start failed.');
} else {
console.info('Capturer start success.');
}
});
}
}
// 停止采集
async function stop() {
if (audioCapturer !== undefined) {
// 只有采集器状态为STATE_RUNNING或STATE_PAUSED的时候才可以停止
if (audioCapturer.state.valueOf() !== audio.AudioState.STATE_RUNNING && audioCapturer.state.valueOf() !== audio.AudioState.STATE_PAUSED) {
console.info('Capturer is not running or paused');
return;
}
await audioCapturer.stop(); // 停止采集
if (audioCapturer.state.valueOf() === audio.AudioState.STATE_STOPPED) {
console.info('Capturer stopped');
} else {
console.error('Capturer stop failed');
}
}
}
// 销毁实例，释放资源
async function release() {
if (audioCapturer !== undefined) {
// 采集器状态不是STATE_RELEASED或STATE_NEW状态，才能release
if (audioCapturer.state.valueOf() === audio.AudioState.STATE_RELEASED || audioCapturer.state.valueOf() === audio.AudioState.STATE_NEW) {
console.info('Capturer already released');
return;
}
await audioCapturer.release(); // 释放资源
if (audioCapturer.state.valueOf() === audio.AudioState.STATE_RELEASED) {
console.info('Capturer released');
} else {
console.error('Capturer release failed');
}
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/not-recommended-V14
爬取时间: 2025-04-28 19:54:18
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/replace-opensles-by-ohaudio-V14
爬取时间: 2025-04-28 19:54:31
来源: Huawei Developer
由于OpenSL ES无法满足音频系统的能力拓展，建议开发者使用OHAudio替代OpenSL ES开发音频业务。本文将介绍如何从使用OpenSL ES接口开发音频业务，切换为使用OHAudio接口。
支持的功能差异
两者支持的功能范围略有差异，OHAudio增加支持低时延播放/录制、监听业务变化等功能。
具体差异如下表所示。
|   | OpenSL ES | OHAudio |
| --- | --- | --- |
| 音频流式播放 | √ | √ |
| 音频流式录制 | √ | √ |
| 音频低时延播放 | × | √ |
| 音频低时延录制 | × | √ |
| 播放对象状态切换 | √ | √ |
| 录制对象状态切换 | √ | √ |
| 获取音频流对象状态 | √ | √ |
| 清理播放缓存 | × | √ |
| 监听音频打断事件 | × | √ |
| 监听音频流事件 | × | √ |
| 监听流异常事件 | × | √ |
| 监听播放设备变化事件 | × | √ |
开发模式差异
此小节将结合开发步骤，对比介绍OHAudio和OpenSL ES在开发模式上的差异。
音频播放和录制的实现类似，此处以音频播放为例说明。
构造实例
OpenSL ES:
通过全局接口获取到Engine对象，基于Engine结合不同输入输出配置参数，构造出不同音频播放对象。
OHAudio:
采用建造器模式，通过建造器，配合自定义参数设置，生成音频播放对象。
状态切换
OpenSL ES:
基于Object获取状态切换Interface，使用Interface接口切换状态，只有SL_PLAYSTATE_STOPPED、SL_PLAYSTATE_PAUSED、SL_PLAYSTATE_PLAYING三种状态。
OHAudio:
有独立的状态切换接口，基于状态机进行状态切换，共6个OH_AudioStream_State状态，主要在AUDIOSTREAM_STATE_PREPARED、AUDIOSTREAM_STATE_RUNNING、AUDIOSTREAM_STATE_STOPPED、AUDIOSTREAM_STATE_PAUSED、AUDIOSTREAM_STATE_RELEASED状态间切换。
数据处理
OpenSL ES:
基于扩展的OHBufferQueue接口，通过注册自定义的Callback函数，根据数据请求时机，将待播放数据填入系统内提供的缓冲区中。
OHAudio:
统一使用回调模式，在构造时注册数据输入回调，实现自定义的数据填充函数，在播放过程中会跟随系统调度和时延配置情况，自动在合适时机触发数据请求回调。
资源释放
OpenSL ES:
使用SLObjectItf接口实现对象资源释放。
OHAudio:
使用对应模块的释放接口实现对象资源释放。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/using-opensl-es-for-playback-V14
爬取时间: 2025-04-28 19:54:44
来源: Huawei Developer
OpenSL ES全称为Open Sound Library for Embedded Systems，是一个嵌入式、跨平台、免费的音频处理库。为嵌入式移动多媒体设备上的应用开发者提供标准化、高性能、低延迟的API。HarmonyOS的Native API基于Khronos Group开发的OpenSL ES1.0.1 API 规范实现，开发者可以通过<OpenSLES.h>和<OpenSLES_OpenHarmony.h>在HarmonyOS上使用相关API。
使用OHAudio替代OpenSL ES
HarmonyOS上的OpenSL ES接口，是早期SDK8版本开始提供，用于支持应用Native层音频开发的接口。但随着版本演进，接口定义的可扩展性不足，不再能满足音频系统的能力拓展，因此当前已不再推荐应用开发者继续使用此接口进行音频功能开发，可能存在一些接口能力不足的缺陷。
在SDK10版本，HarmonyOS推出了OHAudio接口，并将系统具备的所有音频功能都通过此接口开放。OHAudio接口已能够覆盖OpenSL ES在HarmonyOS中已提供的所有能力，并拓展支持音频焦点事件，低时延等新版本特性。
OHAudio的开发指南见使用OHAudio开发音频播放功能(C/C++)
考虑到一些接入HarmonyOS较早的应用开发者，这里提供了一份OpenSL ES接口切换到OHAudio的对照参考OpenSL ES接口切换OHAudio参考，便于开发者能够更快的在新版本切换到使用新接口。
HarmonyOS上的OpenSL ES
OpenSL ES中提供了以下的接口，HarmonyOS当前仅实现了部分接口，可以实现音频播放的基础功能。
调用未实现接口后会返回SL_RESULT_FEATURE_UNSUPPORTED, 当前没有相关扩展可以使用。
以下列表列举了HarmonyOS上已实现的OpenSL ES的接口，具体说明请参考OpenSL ES规范：
-  HarmonyOS上支持的SLInterfaceID：
-  HarmonyOS上支持的Engine接口：
-  HarmonyOS上支持的Object接口：
-  HarmonyOS上支持的Playback接口：
-  HarmonyOS上支持的Volume控制接口：
-  HarmonyOS上支持的BufferQueue接口： 以下接口需引入<OpenSLES_OpenHarmony.h>使用。 根据情况将buffer加到相应队列中。 如果是播放操作，则将带有音频数据的buffer插入到filledBufferQ_队列中；如果是录音操作，则将录音使用后的空闲buffer插入到freeBufferQ_队列中。 self：表示调用该函数的BufferQueue接口对象。 buffer：播放时表示带有音频数据的buffer，录音时表示已存储完录音数据后的空闲buffer。 size：表示buffer的大小。 释放BufferQueue接口对象。 self：表示调用该函数的BufferQueue接口对象将被释放。 获取BufferQueue接口对象状态。 self：表示调用该函数的BufferQueue接口对象。 state：BufferQueue的当前状态。 注册回调函数。 self：表示调用该函数的BufferQueue接口对象。 callback：播放/录音时注册的回调函数。 pContext：播放时传入待播放音频文件，录音时传入将要录制的音频文件。 根据情况获取相应的buffer。 如果是播放操作，则从freeBufferQ_队列中获取空闲buffer；如果是录音操作，则从filledBufferQ_队列中获取携带录音数据的buffer。 self：表示调用该函数的BufferQueue接口对象。 buffer：播放时表示空闲的buffer，录音时表示携带录音数据的buffer。 size：表示buffer的大小。
| SLInterfaceID | 说明 |
| --- | --- |
| SL_IID_ENGINE | 通用引擎，提供创建播放对象接口 |
| SL_IID_PLAY | 提供播放状态接口 |
| SL_IID_VOLUME | 提供音频播放流音量调节和读取接口 |
| SL_IID_OH_BUFFERQUEUE | 提供音频播放流数据回调注册接口 |
| 接口 | 说明 |
| --- | --- |
| SLresult (*Enqueue) (SLOHBufferQueueItf self, const void *buffer, SLuint32 size) | 根据情况将buffer加到相应队列中。 如果是播放操作，则将带有音频数据的buffer插入到filledBufferQ_队列中；如果是录音操作，则将录音使用后的空闲buffer插入到freeBufferQ_队列中。 self：表示调用该函数的BufferQueue接口对象。 buffer：播放时表示带有音频数据的buffer，录音时表示已存储完录音数据后的空闲buffer。 size：表示buffer的大小。  |
| SLresult (*Clear) (SLOHBufferQueueItf self) | 释放BufferQueue接口对象。 self：表示调用该函数的BufferQueue接口对象将被释放。  |
| SLresult (*GetState) (SLOHBufferQueueItf self, SLOHBufferQueueState *state) | 获取BufferQueue接口对象状态。 self：表示调用该函数的BufferQueue接口对象。 state：BufferQueue的当前状态。  |
| SLresult (*RegisterCallback) (SLOHBufferQueueItf self, SlOHBufferQueueCallback callback, void* pContext) | 注册回调函数。 self：表示调用该函数的BufferQueue接口对象。 callback：播放/录音时注册的回调函数。 pContext：播放时传入待播放音频文件，录音时传入将要录制的音频文件。  |
| SLresult (*GetBuffer) (SLOHBufferQueueItf self, SLuint8** buffer, SLuint32* size) | 根据情况获取相应的buffer。 如果是播放操作，则从freeBufferQ_队列中获取空闲buffer；如果是录音操作，则从filledBufferQ_队列中获取携带录音数据的buffer。 self：表示调用该函数的BufferQueue接口对象。 buffer：播放时表示空闲的buffer，录音时表示携带录音数据的buffer。 size：表示buffer的大小。  |
完整示例
在 CMake 脚本中链接动态库
参考以下示例代码，播放一个音频文件。
1.  添加头文件。
2.  使用slCreateEngine接口和获取engine实例。
3.  获取接口SL_IID_ENGINE的engineEngine实例。
4.  配置播放器信息，创建AudioPlayer。
5.  获取接口SL_IID_OH_BUFFERQUEUE的bufferQueueItf实例。
6.  打开音频文件，注册BufferQueueCallback回调。
7.  获取接口SL_PLAYSTATE_PLAYING的playItf实例，开始播放。
8.  结束音频播放。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/using-opensl-es-for-recording-V14
爬取时间: 2025-04-28 19:54:58
来源: Huawei Developer
OpenSL ES全称为Open Sound Library for Embedded Systems，是一个嵌入式、跨平台、免费的音频处理库。为嵌入式移动多媒体设备上的应用开发者提供标准化、高性能、低延迟的API。HarmonyOS的Native API基于Khronos Group开发的OpenSL ES1.0.1 API 规范实现，开发者可以通过<OpenSLES.h>和<OpenSLES_OpenHarmony.h>在HarmonyOS上使用相关API。
使用OHAudio替代OpenSL ES
HarmonyOS上的OpenSL ES接口，是早期SDK8版本开始提供，用于支持应用Native层音频开发的接口。但随着版本演进，接口定义的可扩展性不足，不再能满足音频系统的能力拓展，因此当前已不再推荐应用开发者继续使用此接口进行音频功能开发，可能存在一些接口能力不足的缺陷。
在SDK10版本，HarmonyOS推出了OHAudio接口，并将系统具备的所有音频功能都通过此接口开放。OHAudio接口已能够覆盖OpenSL ES在HarmonyOS中已提供的所有能力，并拓展支持音频焦点事件，低时延等新版本特性。
OHAudio的开发指南见使用OHAudio开发音频录制功能(C/C++)
考虑到一些接入HarmonyOS较早的应用开发者，这里提供了一份OpenSL ES接口切换到OHAudio的对照参考OpenSL ES接口切换OHAudio参考，便于开发者能够更快的在新版本切换到使用新接口。
HarmonyOS上的OpenSL ES
OpenSL ES中提供了以下的接口，HarmonyOS当前仅实现了部分接口，可以实现音频录制的基础功能。
调用未实现接口后会返回**SL_RESULT_FEATURE_UNSUPPORTED，**当前没有相关扩展可以使用。
以下列表列举了HarmonyOS上已实现的OpenSL ES的接口，具体说明请参考OpenSL ES规范：
-  HarmonyOS上支持的SLInterfaceID：
-  HarmonyOS上支持的Engine接口：
-  HarmonyOS上支持的Object接口：
-  HarmonyOS上支持的Recorder接口：
-  HarmonyOS上支持的BufferQueue接口： 以下接口需引入<OpenSLES_OpenHarmony.h>使用。 根据情况将buffer加到相应队列中。 如果是播放操作，则将带有音频数据的buffer插入到filledBufferQ_队列中；如果是录音操作，则将录音使用后的空闲buffer插入到freeBufferQ_队列中。 self：表示调用该函数的BufferQueue接口对象。 buffer：播放时表示带有音频数据的buffer，录音时表示已存储完录音数据后的空闲buffer。 size：表示buffer的大小。 释放BufferQueue接口对象。 self：表示调用该函数的BufferQueue接口对象将被释放。 获取BufferQueue接口对象状态。 self：表示调用该函数的BufferQueue接口对象。 state：BufferQueue的当前状态。 注册回调函数。 self：表示调用该函数的BufferQueue接口对象。 callback：播放/录音时注册的回调函数。 pContext：播放时传入待播放音频文件，录音时传入将要录制的音频文件。 根据情况获取相应的buffer。 如果是播放操作，则从freeBufferQ_队列中获取空闲buffer；如果是录音操作，则从filledBufferQ_队列中获取携带录音数据的buffer。 self：表示调用该函数的BufferQueue接口对象。 buffer：播放时表示空闲的buffer，录音时表示携带录音数据的buffer。 size：表示buffer的大小。
| SLInterfaceID | 说明 |
| --- | --- |
| SL_IID_ENGINE | 通用引擎，提供创建录音对象接口 |
| SL_IID_RECORD | 提供录音状态接口 |
| SL_IID_OH_BUFFERQUEUE | 提供音频录制流数据回调注册接口 |
| 接口 | 说明 |
| --- | --- |
| SLresult (*Enqueue) (SLOHBufferQueueItf self, const void *buffer, SLuint32 size) | 根据情况将buffer加到相应队列中。 如果是播放操作，则将带有音频数据的buffer插入到filledBufferQ_队列中；如果是录音操作，则将录音使用后的空闲buffer插入到freeBufferQ_队列中。 self：表示调用该函数的BufferQueue接口对象。 buffer：播放时表示带有音频数据的buffer，录音时表示已存储完录音数据后的空闲buffer。 size：表示buffer的大小。  |
| SLresult (*Clear) (SLOHBufferQueueItf self) | 释放BufferQueue接口对象。 self：表示调用该函数的BufferQueue接口对象将被释放。  |
| SLresult (*GetState) (SLOHBufferQueueItf self, SLOHBufferQueueState *state) | 获取BufferQueue接口对象状态。 self：表示调用该函数的BufferQueue接口对象。 state：BufferQueue的当前状态。  |
| SLresult (*RegisterCallback) (SLOHBufferQueueItf self, SlOHBufferQueueCallback callback, void* pContext) | 注册回调函数。 self：表示调用该函数的BufferQueue接口对象。 callback：播放/录音时注册的回调函数。 pContext：播放时传入待播放音频文件，录音时传入将要录制的音频文件。  |
| SLresult (*GetBuffer) (SLOHBufferQueueItf self, SLuint8** buffer, SLuint32* size) | 根据情况获取相应的buffer。 如果是播放操作，则从freeBufferQ_队列中获取空闲buffer；如果是录音操作，则从filledBufferQ_队列中获取携带录音数据的buffer。 self：表示调用该函数的BufferQueue接口对象。 buffer：播放时表示空闲的buffer，录音时表示携带录音数据的buffer。 size：表示buffer的大小。  |
完整示例
在 CMake 脚本中链接动态库
参考下列示例代码，完成音频录制。
1.  添加头文件。
2.  使用slCreateEngine接口创建引擎对象和实例化引擎对象engine。
3.  获取接口SL_IID_ENGINE的引擎接口engineEngine实例。
4.  配置录音器信息（配置输入源audiosource、输出源audiosink），创建录音对象pcmCapturerObject。
5.  获取录音接口SL_IID_RECORD的recordItf接口实例。
6.  获取接口SL_IID_OH_BUFFERQUEUE的bufferQueueItf实例。
7.  注册BufferQueueCallback回调。
8.  开始录音。
9.  结束音频录制。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/avcodec-kit-V14
爬取时间: 2025-04-28 19:55:12
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/avcodec-kit-intro-V14
爬取时间: 2025-04-28 19:55:25
来源: Huawei Developer
AVCodec Kit（Audio & Video Codec Kit，音视频编解码，封装解析）是媒体系统中的音视频的编解码、媒体文件的解析、封装、媒体数据输入等原子能力。
基于性能考虑，AVCodec Kit仅提供C接口。
能力范围
亮点/特征
-  系统内部数据零拷贝：在视频解码过程，AVCodec通过回调函数提供AVBuffer给应用，由应用将要解码的sample数据写入AVBuffer，在AVCodec中数据不再需要从内存拷入硬件解码器，而是直接送入解码器解码，实现系统内数据零拷贝。
-  视频编码、解码支持硬件加速：支持H.264、H.265、H.265 10bit的硬件编解码。
基础概念
-  媒体文件：携带有音视频、字幕等媒体数据的文件，如.mp4、.m4a。
-  流媒体：可以边下载，边播放的媒体传输形式，下载协议如HTTP/HTTPS、HLS协议。
-  音视频编码：将未压缩原序列音视频数据转换为另一种格式数据，如H.264、AAC。
-  音视频解码：将一种数据格式转换为未压缩状态的原序列音视频数据，如YUV、PCM。
-  媒体文件封装：将音频、视频、字幕等数据以及描述信息，按照某种格式要求，写入到同一个文件中，如.mp4。
-  媒体文件解封装：将文件中的音频、视频、字幕等媒体数据读出，解析出媒体的描述信息。
-  sample：有相同时间属性的一组数据。 对于音视频，通常是有相同解码时间戳的压缩数据。 对于字幕，通常包含对应时间点的字幕内容。 所有的轨道结尾数据都是为空。
使用方式
-  视频编解码 视频编码的输入和视频解码的输出支持Surface模式。 在编码和解码过程中，通过回调函数通知应用数据处理的情况；如编码过程通过回调通知应用，完成一帧编码，输出编码结果AVBuffer；在解码过程通过回调通知应用输入一帧码流到解码器解码，当解码完成也会通过回调通知应用解码完成，应用可以对数据做后续处理。 视频编解码的逻辑如图所示。 具体开发指导请参考视频解码Surface模式、视频编码Surface模式。
-  音频编解码 音频编码的输入和音频解码的输出为PCM格式。 在编码和解码过程中，通过回调函数通知应用数据处理的情况；如编码过程通过回调通知应用，完成一帧编码，输出编码结果AVBuffer；在解码过程通过回调通知应用输入一帧码流到解码器解码，当解码完成也会通过回调通知应用解码完成，应用可以对数据做后续处理。 音频编解码逻辑如图所示。 具体开发指导请参考音频解码、音频编码。
-  文件解析封装 在文件封装环节，应用将AVBuffer送入Codec对应的接口，执行数据封装，AVBuffer可以是由上述编码输出的AVBuffer，也可以是应用创建的AVBuffer，AVBuffer中要携带有效的码流数据和相关的时间描述等信息； 在文件解析环节，应用从Codec对应的接口获得携带有码流数据的AVBuffer，该AVBuffer可以送入上述视频和音频编解码对应接口。 文件封装解封装逻辑如图所示。 具体开发指导请参考媒体数据解析、媒体数据封装。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165929.77302689370317310747161464121793:50001231000000:2800:EB8406B95C5666B877C98249049C6DDCCE0F24D7637978C84D35ED9CD70EEAC4.png)
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165929.13249192476448632731288667028628:50001231000000:2800:5C33FE65A14A98409B2B87D1EFB20957BA341442615FC89F128B9741F1F28444.png)
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165929.04015519506561286032015256947398:50001231000000:2800:00EBB50AE50347170BDB6DBC9E4D912C465A695E82DD93B98A267CB7F5DB5722.png)

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/avcodec-support-formats-V14
爬取时间: 2025-04-28 19:55:39
来源: Huawei Developer
媒体编解码
视频解码
当前支持的解码能力如下：
| 视频硬解类型 | 视频软解类型 |
| --- | --- |
| AVC(H.264)、HEVC(H.265)、VVC(H.266) | AVC(H.264)、HEVC(H.265) |
视频解码软/硬件解码存在差异，基于MimeType创建解码器时，软解支持H264(OH_AVCODEC_MIMETYPE_VIDEO_AVC)、H265(OH_AVCODEC_MIMETYPE_VIDEO_HEVC)，
如果硬件平台支持，则可以使用H.264(OH_AVCODEC_MIMETYPE_VIDEO_AVC)、H.265(OH_AVCODEC_MIMETYPE_VIDEO_HEVC)、H.266(OH_AVCODEC_MIMETYPE_VIDEO_VVC)硬件解码能力。
每一种解码的能力范围，可以通过获取支持的编解码能力获取。
具体开发指导请参考视频解码。
视频编码
当前支持的编码能力如下：
| 视频编码类型 |
| --- |
| HEVC(H.265)、 AVC(H.264) |
目前仅支持硬件编码，基于MimeType创建编码器时，支持配置为H.264(OH_AVCODEC_MIMETYPE_VIDEO_AVC)和H.265(OH_AVCODEC_MIMETYPE_VIDEO_HEVC)。
每一种编码的能力范围，可以通过获取支持的编解码能力获取。
具体开发指导请参考视频编码。
音频解码
当前支持的解码能力如下:
| 容器规格 | 音频解码类型 |
| --- | --- |
| mp4 | AAC、MPEG(MP3)、Flac、Vorbis、AudioViVid11+ |
| m4a | AAC |
| flac | Flac |
| ogg | Vorbis、opus |
| aac | AAC |
| mp3 | MPEG(MP3) |
| amr | AMR(amrnb、amrwb) |
| raw | G711mu |
| ape | APE |
具体开发指导请参考音频解码。
音频编码
当前支持的编码能力如下：
| 容器规格 | 音频编码类型 |
| --- | --- |
| mp4 | AAC、Flac |
| m4a | AAC |
| flac | Flac |
| aac | AAC |
| mp3 | MP3 |
| raw | G711mu |
| amr | AMR |
| ogg | opus |
具体开发指导请参考音频编码。
媒体数据封装与解析
媒体数据解析
支持的解封装格式如下：
| 媒体格式 | 封装格式 | 码流格式 |
| --- | --- | --- |
| 音视频 | mp4 | 视频码流：AVC(H.264)、HEVC(H.265) 音频码流：AAC、MPEG(MP3)、AudioVivid 字幕流：WEBVTT  |
| 音视频 | fmp4 | 视频码流：AVC(H.264)、HEVC(H.265) 音频码流：AAC、MPEG(MP3)、AudioVivid  |
| 音视频 | mkv | 视频码流：AVC(H.264)、HEVC(H.265) 音频码流：AAC、MPEG(MP3)、OPUS  |
| 音视频 | mpeg-ts | 视频码流：AVC(H.264)、HEVC(H.265) 音频码流：AAC、MPEG(MP3)、Audio Vivid  |
| 音视频 | flv | 视频码流：AVC(H.264)、HEVC(H.265) 音频码流：AAC  |
| 音频 | m4a | 音频码流：AAC、AudioVivid |
| 音频 | aac | 音频码流：AAC |
| 音频 | mp3 | 音频码流：MPEG(MP3) |
| 音频 | ogg | 音频码流：OGG |
| 音频 | flac | 音频码流：FLAC |
| 音频 | wav | 音频码流：PCM、PCM-MULAW |
| 音频 | amr | 音频码流：AMR(AMR-NB、AMR-WB) |
| 音频 | ape | 音频码流：APE |
| 外挂字幕 | srt | 字幕流：SRT |
| 外挂字幕 | webvtt | 字幕流：WEBVTT |
视频码流：AVC(H.264)、HEVC(H.265)
音频码流：AAC、MPEG(MP3)、AudioVivid
字幕流：WEBVTT
视频码流：AVC(H.264)、HEVC(H.265)
音频码流：AAC、MPEG(MP3)、AudioVivid
视频码流：AVC(H.264)、HEVC(H.265)
音频码流：AAC、MPEG(MP3)、OPUS
视频码流：AVC(H.264)、HEVC(H.265)
音频码流：AAC、MPEG(MP3)、Audio Vivid
视频码流：AVC(H.264)、HEVC(H.265)
音频码流：AAC
DRM解密能力支持的解封装格式：mp4(H.264，H.265，AAC)、mpeg-ts(H.264，H.265，AAC)。
具体开发指导请参考媒体数据解析。
媒体数据封装
当前支持的封装能力如下：
| 封装格式 | 视频编解码类型 | 音频编解码类型 | 封面类型 |
| --- | --- | --- | --- |
| mp4 | AVC（H.264）、HEVC（H.265） | AAC、MPEG（MP3） | jpeg、png、bmp |
| m4a | - | AAC | jpeg、png、bmp |
| mp3 | - | MPEG（MP3） | - |
| amr | - | AMR(amrnb、amrwb) | - |
| wav | - | G711mu(pcm-mulaw) | - |
配置选项key值说明：
mp4封装格式：
| key | 描述 | aac | mp3 | H.264 | H.265 | jpg | png | bmp |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| OH_MD_KEY_AUD_SAMPLE_RATE | 采样率 | 必须 | 必须 | - | - | - | - | - |
| OH_MD_KEY_AUD_CHANNEL_COUNT | 声道数 | 必须 | 必须 | - | - | - | - | - |
| OH_MD_KEY_AUDIO_SAMPLE_FORMAT | 输出音频流格式 | 可选 | 可选 | - | - | - | - | - |
| OH_MD_KEY_CHANNEL_LAYOUT | 通道布局 | 可选 | 可选 | - | - | - | - | - |
| OH_MD_KEY_PROFILE | 编码档次 | 可选 | - | - | - | - | - | - |
| OH_MD_KEY_BITRATE | 码率 | 可选 | 可选 | 可选 | 可选 | - | - | - |
| OH_MD_KEY_CODEC_CONFIG | 编解码器特定数据 | 可选 | - | 可选 | 可选 | - | - | - |
| OH_MD_KEY_WIDTH | 宽度 | - | - | 必须 | 必须 | 必须 | 必须 | 必须 |
| OH_MD_KEY_HEIGHT | 高度 | - | - | 必须 | 必须 | 必须 | 必须 | 必须 |
| OH_MD_KEY_FRAME_RATE | 视频流帧率 | - | - | 可选 | 可选 | - | - | - |
| OH_MD_KEY_COLOR_PRIMARIES | 视频色域 | - | - | 可选 | 可选 | - | - | - |
| OH_MD_KEY_TRANSFER_CHARACTERISTICS | 视频传递函数 | - | - | 可选 | 可选 | - | - | - |
| OH_MD_KEY_MATRIX_COEFFICIENTS | 视频矩阵系数 | - | - | 可选 | 可选 | - | - | - |
| OH_MD_KEY_RANGE_FLAG | 值域标志 | - | - | 可选 | 可选 | - | - | - |
| OH_MD_KEY_VIDEO_IS_HDR_VIVID | 视频轨是否为HDR VIVID | - | - | - | 可选 | - | - | - |
m4a封装格式：
| key | 描述 | aac | jpg | png | bmp |
| --- | --- | --- | --- | --- | --- |
| OH_MD_KEY_AUD_SAMPLE_RATE | 采样率 | 必须 | - | - | - |
| OH_MD_KEY_AUD_CHANNEL_COUNT | 声道数 | 必须 | - | - | - |
| OH_MD_KEY_AUDIO_SAMPLE_FORMAT | 输出音频流格式 | 可选 | - | - | - |
| OH_MD_KEY_CHANNEL_LAYOUT | 通道布局 | 可选 | - | - | - |
| OH_MD_KEY_PROFILE | 编码档次 | 可选 | - | - | - |
| OH_MD_KEY_BITRATE | 码率 | 可选 | - | - | - |
| OH_MD_KEY_CODEC_CONFIG | 编解码器特定数据 | 可选 | - | - | - |
| OH_MD_KEY_WIDTH | 宽度 | - | 必须 | 必须 | 必须 |
| OH_MD_KEY_HEIGHT | 高度 | - | 必须 | 必须 | 必须 |
amr封装格式：
| key | 描述 | amr_nb | amr_wb |
| --- | --- | --- | --- |
| OH_MD_KEY_AUD_SAMPLE_RATE | 采样率 | 必须 | 必须 |
| OH_MD_KEY_AUD_CHANNEL_COUNT | 声道数 | 必须 | 必须 |
| OH_MD_KEY_AUDIO_SAMPLE_FORMAT | 输出音频流格式 | 可选 | 可选 |
| OH_MD_KEY_CHANNEL_LAYOUT | 通道布局 | 可选 | 可选 |
| OH_MD_KEY_BITRATE | 码率 | 可选 | 可选 |
mp3封装格式：
| key | 描述 | mp3 | jpg |
| --- | --- | --- | --- |
| OH_MD_KEY_AUD_SAMPLE_RATE | 采样率 | 必须 | - |
| OH_MD_KEY_AUD_CHANNEL_COUNT | 声道数 | 必须 | - |
| OH_MD_KEY_AUDIO_SAMPLE_FORMAT | 输出音频流格式 | 可选 | - |
| OH_MD_KEY_CHANNEL_LAYOUT | 通道布局 | 可选 | - |
| OH_MD_KEY_BITRATE | 码率 | 可选 | - |
| OH_MD_KEY_WIDTH | 宽度 | - | 必须 |
| OH_MD_KEY_HEIGHT | 高度 | - | 必须 |
wav封装格式：
| key | 描述 | g711mu |
| --- | --- | --- |
| OH_MD_KEY_AUD_SAMPLE_RATE | 采样率 | 必须 |
| OH_MD_KEY_AUD_CHANNEL_COUNT | 声道数 | 必须 |
| OH_MD_KEY_AUDIO_SAMPLE_FORMAT | 输出音频流格式 | 可选 |
| OH_MD_KEY_CHANNEL_LAYOUT | 通道布局 | 可选 |
| OH_MD_KEY_BITRATE | 码率 | 必须 |
具体开发指导请参考媒体数据封装。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audio-video-codec-V14
爬取时间: 2025-04-28 19:55:52
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/obtain-supported-codecs-V14
爬取时间: 2025-04-28 19:56:06
来源: Huawei Developer
因来源不同、编解码器协议不同以及设备在编解码能力部署上的不同，在不同设备上开发者可用的编解码器及其能力是有差异的。
为确保编解码行为符合预期，开发者应提前通过音视频编解码能力系列接口查询系统支持的音视频编解码器及其关联的能力参数，找到符合开发场景需求的编解码器，并正确配置编解码参数。
通用开发指导
1.  在CMake脚本中链接动态库。 上述'sample'字样仅为示例，此处由开发者根据实际工程目录自定义。
2.  添加头文件。
3.  获得音视频编解码能力实例。 支持两种获取音视频编解码能力实例的方式： 方式一：通过OH_AVCodec_GetCapability获取框架推荐的音视频编解码器能力实例。与OH_XXX_CreateByMime系列接口框架推荐策略一致。 方式二：通过OH_AVCodec_GetCapabilityByCategory获取指定软件或硬件的编解码能力实例。 若获取能力实例成功，则可继续向下执行。开发者无需关注该实例的回收问题，框架会自行回收。
4.  按需调用相应查询接口，详细的API说明请参考API文档。
场景化开发指导
基于开发者可能遇到特定场景，举例说明能力查询接口使用方法。
创建指定名称的编解码器
如系统内存在相同MIME类型的多个编码器或多个解码器。使用OH_XXX_CreateByMime系列接口只能创建系统推荐的特定编解码器。若需创建其他编解码器，开发者可先获取编解码器名称，再通过OH_XXX_CreateByName系列接口创建指定名称的编解码器。
| 接口 | 功能描述 |
| --- | --- |
| OH_AVCapability_GetName | 获取能力实例对应编解码器的名称 |
H.264软件解码器和H.264硬件解码器共存时，创建H.264软件解码器示例：
针对软硬件类别差异化配置编解码器参数
软件编解码器和硬件编解码器定义如下：
-  软件编解码器:指在CPU上进行编解码工作的编解码器，能力可灵活迭代，相比硬件编解码器具有更好的兼容性，更好的协议和规格扩展能力。
-  硬件编解码器:指在专有硬件上进行编解码工作的编解码器，其特点是已在硬件平台硬化，能力随硬件平台迭代。相比软件编解码器具有更好的功耗、耗时和吞吐表现，同时能降低CPU负载。
基于上述软件编解码器和硬件编解码器的特点，在硬件编解码器满足要求的时候，优先使用硬件编解码器，否则使用软件编解码器。开发者可基于软件还是硬件类别差异化配置编解码参数。
| 接口 | 功能描述 |
| --- | --- |
| OH_AVCapability_IsHardware | 确认能力实例对应的编解码器是否是硬件的 |
视频编码，软硬件差异化配置帧率示例：
创建多路编解码器
部分业务场景涉及创建多路编解码器，基于各类资源的限制，某一编解码器的实例数是有限的，不能无限制创建。
| 接口 | 功能描述 |
| --- | --- |
| OH_AVCapability_GetMaxSupportedInstances | 获取能力实例对应编解码器可同时运行的最大实例数，实际能成功创建的数目还受系统其他资源的约束 |
优先创建硬件解码器实例，不够时再创建软件解码器实例，示例如下：
控制编码质量
当前提供三种码控模式供开发者选用，分别是恒定码率（CBR）码控模式、动态码率（VBR）码控模式，以及恒定质量（CQ）码控模式。对于CBR和VBR码控模式，编码质量由码率参数决定。对于CQ码控模式，编码质量由质量参数决定。
| 接口 | 功能描述 |
| --- | --- |
| OH_AVCapability_IsEncoderBitrateModeSupported | 确认当前编解码器是否支持给定的码控模式 |
| OH_AVCapability_GetEncoderBitrateRange | 获取当前编解码器支持的码率范围，在CBR和VBR码控模式下使用 |
| OH_AVCapability_GetEncoderQualityRange | 获取当前编解码器支持的质量范围，在CQ码控模式下使用 |
CBR和VBR码控模式示例如下：
CQ码控模式示例如下：
查询编解码器支持复杂度范围
复杂度等级决定了编解码器使用的工具的数目，仅部分编解码器支持。
| 接口 | 功能描述 |
| --- | --- |
| OH_AVCapability_GetEncoderComplexityRange | 获取当前编解码器支持的复杂度范围 |
设置正确的音频编解码参数
在音频编解码场景中，有采样率、通道数以及码率（仅音频编码）等参数需要查询后设置。
| 接口 | 功能描述 |
| --- | --- |
| OH_AVCapability_GetAudioSupportedSampleRates | 获取当前音频编解码器支持的采样率范围 |
| OH_AVCapability_GetAudioChannelCountRange | 获取当前音频编解码器支持的通道数范围 |
| OH_AVCapability_GetEncoderBitrateRange | 获取当前编码器支持的码率范围 |
音频编码场景，确认并设置正确的编码的参数，示例如下：
查询编解码档次和级别支持情况
编解码标准由很多编码工具构成，能应对多种编码场景。对于特定应用场景，并非需要所有的工具，编解码标准按档次确定多种编码工具的开启与关闭情况。以H.264为例，存在基本档次、主档次和高档次，参考OH_AVCProfile。
级别是对编解码器所需的处理能力和储存空间的划分。以H.264为例，存在1到6.2的20个级别，参考OH_AVCLevel。
| 接口 | 功能描述 |
| --- | --- |
| OH_AVCapability_GetSupportedProfiles | 获取当前编解码器支持的档次 |
| OH_AVCapability_GetSupportedLevelsForProfile | 获取当前编解码器在给定档次的情况下支持的等级信息 |
| OH_AVCapability_AreProfileAndLevelSupported | 确认当前编解码器是否支持特定的档次和等级组合 |
确认待配置档次是否支持，并查询能支持的级别，示例如下：
已知需要的编码档次和级别组合，直接查询支持情况示例如下：
设置正确的视频宽高
视频编解码器对宽高存在对齐约束，如主流编解码器默认编解码像素格式为YUV420系列中的，会对UV分量下采样，在此情况下视频编解码的宽高至少要按2对齐。此外还有其他因素可能导致更加严格的对齐约束。
视频编解码的宽高不仅会受帧级编解码能力限制，同时也会受协议中级别对帧级能力的限制。以H.264为例，AVC_LEVEL_51限定最大每帧宏块数目为36864。
给定图像宽和高，求最大帧率的公式如下, 其中MaxMBsPerFrameLevelLimits是编解码器能支持的最大级别在协议中限定的最大每帧宏块数, MaxMBsPerFrameSubmit是编解码器上报能支持的最大每帧宏块数，实际能力取两者交集。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165930.49108737591179702229222827426618:50001231000000:2800:CC94F985DAFF532D8F87DBADE8FB9FCE2711669A80ACBAB4C28DA488879A4A72.png)
| 接口 | 功能描述 |
| --- | --- |
| OH_AVCapability_GetVideoWidthAlignment | 获取当前视频编解码器的宽对齐 |
| OH_AVCapability_GetVideoHeightAlignment | 确认当前视频编解码器的高对齐 |
| OH_AVCapability_GetVideoWidthRange | 获取当前视频编解码器支持的宽的范围 |
| OH_AVCapability_GetVideoHeightRange | 获取当前视频编解码器支持的高的范围 |
| OH_AVCapability_GetVideoWidthRangeForHeight | 获取当前视频编解码器在给定高情况下的宽的范围 |
| OH_AVCapability_GetVideoHeightRangeForWidth | 获取当前视频编解码器在给定宽情况下的高的范围 |
| OH_AVCapability_IsVideoSizeSupported | 确认当前视频编解码器是否支持给定的宽高组合 |
已知视频高和视频宽，校验是否支持，示例如下：
若遇到基于已知视频高和视频宽校验不支持或配置失败，可用下列示意的多种方式尝试找到正确的视频宽高范围。
已知视频宽，找到正确的尺寸配置，示例如下：
已知视频高，找到正确的尺寸配置，示例如下：
设置正确的视频帧率
视频编解码的帧率不仅会受编解码器秒级编解码能力限制，同时也会受协议中级别对秒级能力的限制。以H.264为例，AVC_LEVEL_51限定最大每秒宏块数目为983040。
给定图像宽和高，求最大帧率的公式如下, 其中MaxMBsPerSecondLevelLimits是编解码器能支持的最大级别在协议中限定的最大每秒宏块数, MaxMBsPerSecondSubmit是编解码器上报能支持的最大每秒宏块数，实际能力取两者交集。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165930.73854022213837318283232220919725:50001231000000:2800:8BB8978480FF0F6EC3563C780FE671ECFF687977D3F5E7C5987D33CD573B4A13.png)
| 接口 | 功能描述 |
| --- | --- |
| OH_AVCapability_GetVideoFrameRateRange | 获取当前视频编解码器支持的帧率的范围 |
| OH_AVCapability_GetVideoFrameRateRangeForSize | 获取当前视频编解码器在给定图像尺寸情况下的帧率的范围 |
| OH_AVCapability_AreVideoSizeAndFrameRateSupported | 检查视频编解码器是否支持视频大小和帧率的特定组合 |
有需求的帧率目标，确认帧率是否在可选范围内，示例如下：
基于待配置的尺寸找到合适的帧率配置，示例代码如下：
设置正确的视频像素格式信息
视频像素格式指示的编码输入图像或解码输出图像的像素排布方式，参考OH_AVPixelFormat。
| 接口 | 功能描述 |
| --- | --- |
| OH_AVCapability_GetVideoSupportedPixelFormats | 获取当前视频编解码器支持的像素格式 |
查询编解码特性支持情况并获取特性属性信息
编解码特性是指仅在特定编解码场景中使用的可选特性，参考OH_AVCapabilityFeature。
| 接口 | 功能描述 |
| --- | --- |
| OH_AVCapability_IsFeatureSupported | 确认当前编解码器是否支持给定的特性 |
| OH_AVCapability_GetFeatureProperties | 获取当前编码器支持的指定特性的属性，仅部分特性存在属性信息 |
查询H.264编码器是否支持长期参考帧特性示例如下：

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audio-encoding-V14
爬取时间: 2025-04-28 19:56:20
来源: Huawei Developer
开发者可以调用本模块的Native API接口，完成音频编码，即将音频PCM编码压缩成不同的格式。
接口不限制PCM数据的来源，开发者可以调用麦克风录制获取、也可以导入编辑后的PCM数据，通过音频编码，输出对应格式的码流，最后封装为目标格式文件。
当前支持的编码能力请参考AVCodec支持的格式。
适用场景
-  音频录制 通过录制传入PCM，然后编码出对应格式的码流，最后封装成想要的格式。
-  音频编辑 编辑PCM后导出音频文件的场景，需要编码成对应音频格式后再封装成文件。
AAC编码器默认采用的VBR可变码率模式，与配置的预期参数可能存在偏差。
开发指导
详细的API说明请参考API文档。
参考以下示例代码，完成音频编码的全流程，包括：创建编码器、设置编码参数（采样率/码率/声道数等）、开始、刷新、重置、销毁资源。
在应用开发过程中，开发者应按一定顺序调用方法，执行对应操作，否则系统可能会抛出异常或生成其他未定义的行为。具体顺序可参考下列开发步骤及对应说明。
如下为音频编码调用关系图：
-  虚线表示可选。
-  实线表示必选。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165930.76298160781074216295952297830636:50001231000000:2800:4245A8D3621F134CB0D2B60900B53FA4CA809171C729D55864E317088FF06118.png)
在 CMake 脚本中链接动态库
开发步骤
1.  添加头文件。
2.  创建编码器实例对象，OH_AVCodec *为编码器实例指针。 应用可以通过名称或媒体类型创建编码器。
3.  调用OH_AudioCodec_RegisterCallback()注册回调函数。 注册回调函数指针集合OH_AVCodecCallback，包括： 开发者可以通过处理该回调报告的信息，确保编码器正常运转。 回调中不建议进行耗时操作。
4.  调用OH_AudioCodec_Configure设置编码器。 设置必选项：采样率，码率，以及声道数，声道类型、位深。 可选项：最大输入长度。 flac编码： 需要额外标识兼容性级别(Compliance Level)和采样精度。 各音频编码类型参数范围说明： 例如对一个44100Hz采样率、2声道立体声、SAMPLE_S16LE采样格式的PCM音频，以32000bps的码率进行AAC编码的调用流程如下： 例FLAC调用流程： 例AMR编码调用流程： 例opus编码调用流程：
5.  调用OH_AudioCodec_Prepare()，编码器就绪。
6.  调用OH_AudioCodec_Start()启动编码器，进入运行态。
7.  调用OH_AudioCodec_PushInputBuffer()，写入待编码器的数据。需开发者填充完整的输入数据后调用。 每帧样点数(SAMPLES_PER_FRAME)取值： aac建议使用20ms的PCM样点数，即采样率*0.02。 flac比较特殊，需要根据如下表格进行设置。 aac编码的每帧样点数建议使用20ms的PCM样点数，即采样率*0.02。flac编码的样点数建议根据采样率按照表格传入，大于这个值也会返回错误码，如果小于有可能出现编码文件损坏问题。 在上方案例中，attr.flags代表缓冲区标记的类别。 如果是结束，需要将flags标识成AVCODEC_BUFFER_FLAGS_EOS。
8.  调用OH_AudioCodec_FreeOutputBuffer()，释放编码后的数据。 在取走编码码流后，就应及时调用OH_AudioCodec_FreeOutputBuffer()进行释放。
9.  （可选）调用OH_AudioCodec_Flush()刷新编码器。 调用OH_AudioCodec_Flush()后，编码器处于Flush状态，会将当前编码队列清空。 此时需要调用OH_AudioCodec_Start()重新开始编码。 使用情况：
10.  （可选）调用OH_AudioCodec_Reset()重置编码器。 调用OH_AudioCodec_Reset()后，编码器回到初始化的状态，需要调用OH_AudioCodec_Configure()重新配置，然后调用OH_AudioCodec_Start()重新开始编码。
11.  调用OH_AudioCodec_Stop()停止编码器。 停止后，可以通过Start重新进入已启动状态（started），但需要注意的是，如果编码器之前已输入数据，则需要重新输入编码器数据。
12.  调用OH_AudioCodec_Destroy()销毁编码器实例，释放资源。 资源不能重复销毁
| 音频编码类型 | 采样率(Hz) | 声道数 |
| --- | --- | --- |
| Flac | 8000、11025、12000、16000、22050、24000、32000、44100、48000、64000、88200、96000 | 1~8 |
| MP3 | 8000、11025、12000、16000、22050、24000、32000、44100、48000 | 1~2 |
| G711mu | 8000 | 1 |
| AAC-LC | 8000、11025、12000、16000、22050、24000、32000、44100、48000、64000、88200、96000 | 1、2、3、4、5、6、8 |
| HE-AAC、HE-AAC v2 | 16000、22050、24000、32000、44100、48000、64000、88200、96000 | 1、2、3、4、5、6、8 |
| opus | 8000、12000、16000、24000、48000 | 1~2 |
| AMR(amrnb) | 8000 | 1 |
| AMR(amrwb) | 16000 | 1 |
| 采样率 | 样点数 |
| --- | --- |
| 8000 | 576 |
| 16000 | 1152 |
| 22050 | 2304 |
| 24000 | 2304 |
| 32000 | 2304 |
| 44100 | 4608 |
| 48000 | 4608 |
| 88200 | 8192 |
| 96000 | 8192 |
| 枚举值 | 描述 |
| --- | --- |
| AVCODEC_BUFFER_FLAGS_NONE | 表示为普通帧。 |
| AVCODEC_BUFFER_FLAGS_EOS | 表示缓冲区是流结束帧。 |
| AVCODEC_BUFFER_FLAGS_CODEC_DATA | 表示缓冲区包含编解码特定数据。 |

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audio-decoding-V14
爬取时间: 2025-04-28 19:56:34
来源: Huawei Developer
开发者可以调用本模块的Native API接口，完成音频解码，即将媒体数据解码为PCM码流。
当前支持的解码能力请参考AVCodec支持的格式。
适用场景
-  音频播放 在播放音频之前，需要先解码音频，再将数据输送到硬件扬声器播放。
-  音频渲染 在对音频文件进行音效处理之前，需要先解码再由音频处理模块进行音频渲染。
-  音频编辑 音频编辑（如调整单个声道的播放倍速等）需要基于PCM码流进行，所以需要先将音频文件解码。
通过MP3音频编码流程生成的码流无法直接通过MP3音频解码流程进行解码。建议通过（PCM码流->MP3音频编码->封装->解封装->MP3音频解码）流程进行。
开发指导
详细的API说明请参考API文档。
参考以下示例代码，完成音频解码的全流程，包括：创建解码器、设置解码参数（采样率/码率/声道数等）、开始、刷新、重置、销毁资源。
在应用开发过程中，开发者应按一定顺序调用方法，执行对应操作，否则系统可能会抛出异常或生成其他未定义的行为。具体顺序可参考下列开发步骤及对应说明。
如下为音频解码调用关系图：
-  虚线表示可选。
-  实线表示必选。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165930.64286202211345228928630464239633:50001231000000:2800:5C2A7F52CF5E62DD3F476D1FDC2E37A1EFA6817CE492809BE6DAB4681FD232A3.png)
在 CMake 脚本中链接动态库
开发步骤
1.  添加头文件。
2.  创建解码器实例对象，OH_AVCodec *为解码器实例指针。
3.  调用OH_AudioCodec_RegisterCallback()注册回调函数。 注册回调函数指针集合OH_AVCodecCallback，包括： 开发者可以通过处理该回调报告的信息，确保解码器正常运转。 回调中不建议进行耗时操作。
4.  （可选）OH_AudioCodec_SetDecryptionConfig设置解密配置。 当获取到DRM信息(参考音视频解封装开发步骤第4步)后，通过此接口进行解密配置。 DRM相关接口详见DRM API文档。 此接口需在Prepare前调用。 添加头文件: 在 CMake 脚本中链接动态库: 使用示例:
5.  调用OH_AudioCodec_Configure()配置解码器。 配置选项key值说明： 各音频解码类型参数范围说明：
6.  调用OH_AudioCodec_Prepare()，解码器就绪。
7.  调用OH_AudioCodec_Start()启动解码器，进入运行态。
8.  （可选）调用OH_AVCencInfo_SetAVBuffer()，设置cencInfo。 若当前播放的节目是DRM加密节目，且由上层应用做媒体解封装，则须调用OH_AVCencInfo_SetAVBuffer()将cencInfo设置给AVBuffer，以实现AVBuffer中媒体数据的解密。 添加头文件： 在 CMake 脚本中链接动态库： 使用示例：
9.  调用OH_AudioCodec_PushInputBuffer()，写入待解码的数据。 需开发者填充完整的输入数据后调用。 如果是结束，需要对flags标识成AVCODEC_BUFFER_FLAGS_EOS。
10.  调用OH_AudioCodec_FreeOutputBuffer()，释放解码后的数据。 在取走解码PCM码流后，就应及时调用OH_AudioCodec_FreeOutputBuffer()进行释放。 从API version 11开始，Audio Vivid新增获取获取元数据。
11.  （可选）调用OH_AudioCodec_Flush()刷新解码器。 调用OH_AudioCodec_Flush()后，解码器仍处于运行态，但会将当前队列清空，将已解码的数据释放。 此时需要调用OH_AudioCodec_Start()重新开始解码。 使用情况：
12.  （可选）调用OH_AudioCodec_Reset()重置解码器。 调用OH_AudioCodec_Reset()后，解码器回到初始化的状态，需要调用OH_AudioCodec_Configure()重新配置，然后调用OH_AudioCodec_Start()重新开始解码。
13.  调用OH_AudioCodec_Stop()停止解码器。 停止后，可以通过调用OH_AudioCodec_Start()重新进入已启动状态（started），但需要注意的是，如果编解码器之前已输入数据，则需要重新输入编解码器数据。
14.  调用OH_AudioCodec_Destroy()销毁解码器实例，释放资源。 不要重复销毁解码器
| key | 描述 | AAC | Flac | Vorbis | MPEG | G711mu | AMR(amrnb、amrwb) | APE |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| OH_MD_KEY_AUD_SAMPLE_RATE | 采样率 | 必须 | 必须 | 必须 | 必须 | 必须 | 必须 | 必须 |
| OH_MD_KEY_AUD_CHANNEL_COUNT | 声道数 | 必须 | 必须 | 必须 | 必须 | 必须 | 必须 | 必须 |
| OH_MD_KEY_MAX_INPUT_SIZE | 最大输入长度 | 可选 | 可选 | 可选 | 可选 | 可选 | 可选 | 可选 |
| OH_MD_KEY_AAC_IS_ADTS | 是否adts | 可选，默认1 | - | - | - | - | - | - |
| OH_MD_KEY_AUDIO_SAMPLE_FORMAT | 输出音频流格式 | 可选（SAMPLE_S16LE，SAMPLE_F32LE） | 可选 | 可选（SAMPLE_S16LE，SAMPLE_F32LE） | 可选 | 可选（默认SAMPLE_S16LE） | 可选（SAMPLE_S16LE，SAMPLE_F32LE） | 可选 |
| OH_MD_KEY_BITRATE | 码率 | 可选 | 可选 | 可选 | 可选 | 可选 | 可选 | 可选 |
| OH_MD_KEY_IDENTIFICATION_HEADER | ID Header | - | - | 必须（和Codec_Config二选一） | - | - | - | - |
| OH_MD_KEY_SETUP_HEADER | Setup Header | - | - | 必须（和Codec_Config二选一） | - | - | - | - |
| OH_MD_KEY_CODEC_CONFIG | 编解码器特定数据 | 可选 | - | 必须（和上述ID和Setup二选一） | - | - | - | 可选 |
| 音频解码类型 | 采样率(Hz) | 声道数 |
| --- | --- | --- |
| AAC | 8000、11025、12000、16000、22050、24000、32000、44100、48000、64000、88200、96000 | 1~8 |
| Flac | 8000、11025、12000、16000、22050、24000、32000、44100、48000、64000、88200、96000、192000 | 1~8 |
| Vorbis | 8000、11025、12000、16000、22050、24000、32000、44100、48000、64000、88200、96000、176400、192000 | 1~8 |
| MPEG(MP3) | 8000、11025、12000、16000、22050、24000、32000、44100、48000 | 1~2 |
| G711mu | 8000 | 1 |
| AMR(amrnb) | 8000 | 1 |
| AMR(amrwb) | 16000 | 1 |
| APE | 8000、11025、12000、16000、22050、24000、32000、44100、48000、64000、88200、96000、176400、192000 | 1~2 |
| Opus | 8000、12000、16000、24000、48000 | 1~2 |
| AudioVivid | 32000、44100、48000、96000、192000 | 1~16 |

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/video-encoding-V14
爬取时间: 2025-04-28 19:56:48
来源: Huawei Developer
开发者可以调用本模块的Native API接口，完成视频编码，即将未压缩的视频数据压缩成视频码流。
具体实现可参考示例工程。
当前支持的编码能力请参考AVCodec支持的格式。
如果需要对HDRVivid视频进行编码，需要配置MimeType为H265 (OH_AVCODEC_MIMETYPE_VIDEO_HEVC)，本功能从API version 11开始支持。
视频编码支持以下能力：
| 支持的能力 | 使用简述 |
| --- | --- |
| 运行时配置编码器参数，包括帧率、码率、QPMin/QPMax | 通过调用OH_VideoEncoder_SetParameter()配置， 具体可参考下文中：Surface模式的步骤-9 |
| 随帧设置编码QPMin/QPMax | 通过调用OH_VideoEncoder_RegisterParameterCallback()注册随帧参数回调时配置，具体可参考下文中：Surface模式的步骤-4 |
| 分层编码，LTR设置 | 具体可参考：时域可分层视频编码 |
| 获取编码每帧平均量化参数（QPAverage）、平方误差（mseValue） | 在配置回调函数OnNewOutputBuffer()时获取，具体可参考下文中：Surface模式的步骤-3 |
| 变分辨率 | 编码器支持输入图像分辨率发生变化。目前仅Surface模式支持且图像的宽、高不能超过OH_VideoEncoder_Configure接口配置的宽、高，具体可参考下文中：Surface模式的步骤-5 |
限制约束
surface输入与buffer输入
1.  两者的数据来源不同。
2.  两者的适用场景不同：
3.  在接口调用的过程中，两种方式的接口调用方式基本一致，但存在以下差异点：
两种模式的开发步骤详细说明请参考：Surface模式和Buffer模式。
状态机调用关系
如下为状态机调用关系图：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165931.59421932554738664093763979591752:50001231000000:2800:747EFF8F74A267AD668F95FCCD2ADDA3670723F986994C6D801746B896BBAEFB.png)
1.  有两种方式可以使编码器进入Initialized状态：
2.  Initialized状态下，调用OH_VideoEncoder_Configure接口配置编码器，配置成功后编码器进入Configured状态。
3.  Configured状态下，调用OH_VideoEncoder_Prepare()进入Prepared状态。
4.  Prepared状态下，调用OH_VideoEncoder_Start接口使编码器进入Executing状态：
5.  在极少数情况下，编码器可能会遇到错误并进入Error状态。编码器的错误传递，可以通过队列操作返回无效值或者抛出异常：
6.  Executing 状态具有三个子状态：Flushed、Running和End-of-Stream：
7.  使用完编码器后，必须调用OH_VideoEncoder_Destroy接口销毁编码器实例，使编码器进入Released状态。
开发指导
详细的API说明请参考API文档。
如下为视频编码调用关系图：
-  虚线表示可选。
-  实线表示必选。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165931.57942972734357549845608346588585:50001231000000:2800:DA016E83755502254442D6A1C76AC0A467B7C3958DB9B053CE593115F42947E8.png)
在 CMake 脚本中链接动态库
上述'sample'字样仅为示例，此处由开发者根据实际工程目录自定义。
定义基础结构
本部分示例代码按照C++17标准编写，仅作参考。开发者可以参考此部分，定义自己的buffer对象。
1.  添加头文件。
2.  编码器回调buffer的信息。
3.  编码输入输出队列。
4.  全局变量 仅做参考，可以根据实际情况将其封装到对象中。
Surface模式
参考以下示例代码，开发者可以完成Surface模式下视频编码的全流程。此处以surface数据输入，编码成H.264格式为例。
本模块目前仅支持异步模式的数据轮转。
1.  添加头文件。
2.  创建编码器实例。 开发者可以通过名称或媒体类型创建编码器。示例中的变量说明如下： 创建方式示例如下：
3.  调用OH_VideoEncoder_RegisterCallback()设置回调函数。 注册回调函数指针集合OH_AVCodecCallback，包括： 回调函数的具体实现可参考示例工程。 示例如下所示： 在回调函数中，对数据队列进行操作时，需要注意多线程同步的问题。
4.  （可选）调用OH_VideoEncoder_RegisterParameterCallback()在Configure接口之前注册随帧通路回调。 详情请参考时域可分层视频编码。
5.  调用OH_VideoEncoder_Configure()配置编码器。 详细可配置选项的说明请参考视频专有键值对。 参数校验规则请参考OH_VideoEncoder_Configure()参考文档。 参数取值范围可以通过能力查询接口获取，具体示例请参考获取支持的编解码能力文档。 目前支持的所有格式都必须配置以下选项：视频帧宽度、视频帧高度、视频像素格式。示例中的变量如下： 配置非必须参数错误时，会返回AV_ERR_INVAILD_VAL错误码。但OH_VideoEncoder_Configure()不会失败，而是使用默认值继续执行。
6.  获取surface。 获取编码器Surface模式的OHNativeWindow输入，获取surface需要在调用OH_VideoEncoder_Prepare接口之前完成。 OHNativeWindow*变量类型的使用方法请参考图形子系统OHNativeWindow。
7.  调用OH_VideoEncoder_Prepare()编码器就绪。 该接口将在编码器运行前进行一些数据的准备工作。
8.  调用OH_VideoEncoder_Start()启动编码器。
9.  （可选）OH_VideoEncoder_SetParameter()在运行过程中动态配置编码器参数。
10.  写入编码图像。 在之前的第6步中，开发者已经对OH_VideoEncoder_GetSurface接口返回的OHNativeWindow*类型变量进行配置。因为编码所需的数据，由配置的surface进行持续地输入，所以开发者无需对OnNeedInputBuffer回调函数进行处理，也无需使用OH_VideoEncoder_PushInputBuffer接口输入数据。 在变分辨率场景中，此规则也同样适用。
11.  （可选）调用OH_VideoEncoder_PushInputParameter()通知编码器随帧参数配置输入完成。 在之前的第4步中，开发者已经注册随帧通路回调。 以下示例中：
12.  调用OH_VideoEncoder_NotifyEndOfStream()通知编码器结束。
13.  调用OH_VideoEncoder_FreeOutputBuffer()释放编码帧。 以下示例中：
14.  （可选）调用OH_VideoEncoder_Flush()刷新编码器。 调用OH_VideoEncoder_Flush接口后，编码器仍处于运行态，但会清除编码器中缓存的输入和输出数据及参数集如H.264格式的PPS/SPS。 此时需要调用OH_VideoEncoder_Start接口重新开始编码。
15.  （可选）调用OH_VideoEncoder_Reset()重置编码器。 调用OH_VideoEncoder_Reset接口后，编码器将回到初始化的状态，需要调用OH_VideoEncoder_Configure接口和OH_VideoEncoder_Prepare接口重新配置。
16.  （可选）调用OH_VideoEncoder_Stop()停止编码器。 调用OH_VideoEncoder_Stop接口后，编码器保留了编码实例，释放输入输出buffer。开发者可以直接调用OH_VideoEncoder_Start接口继续编码，输入的第一个buffer需要携带参数集，从IDR帧开始送入。
17.  调用OH_VideoEncoder_Destroy()销毁编码器实例，释放资源。
Buffer模式
参考以下示例代码，开发者可以完成Buffer模式下视频编码的全流程。此处以YUV文件输入，编码成H.264格式为例。
本模块目前仅支持异步模式的数据轮转。
1.  添加头文件。
2.  创建编码器实例。 与Surface模式相同，此处不再赘述。
3.  调用OH_VideoEncoder_RegisterCallback()设置回调函数。 注册回调函数指针集合OH_AVCodecCallback，包括： 开发者可以通过处理该回调报告的信息，确保编码器正常运转。 回调函数的具体实现可参考示例工程。 在回调函数中，对数据队列进行操作时，需要注意多线程同步的问题。
4.  调用OH_VideoEncoder_Configure()配置编码器。 与Surface模式相同，此处不再赘述。
5.  调用OH_VideoEncoder_Prepare()编码器就绪。 该接口将在编码器运行前进行一些数据的准备工作。
6.  调用OH_VideoEncoder_Start()启动编码器，进入运行态。 启动编码器后，回调函数将开始响应事件。所以，需要先配置输入文件、输出文件。
7.  （可选）在运行过程中动态配置编码器参数。
8.  调用OH_VideoEncoder_PushInputBuffer()写入编码图像。 送入输入队列进行编码，以下示例中： 对跨距进行偏移，以NV12图像为例，示例如下： 以NV12图像为例，width、height、wStride、hStride图像排布参考下图： 添加头文件。 使用示例： 硬件编码在处理buffer数据时（推送数据前），需要开发者拷贝宽、高对齐后的图像数据到输入回调的AVbuffer中。 一般需要获取数据的宽、高、跨距、像素格式来保证编码输入数据被正确的处理。 具体实现请参考：Buffer模式的步骤3-调用OH_VideoEncoder_RegisterCallback接口设置回调函数来获取数据的宽、高、跨距、像素格式。
9.  通知编码器结束。 以下示例中： 与“步骤-8. 写入编码图像”一样，使用同一个接口OH_VideoEncoder_PushInputBuffer，通知编码器输入结束，需要将flag标识成AVCODEC_BUFFER_FLAGS_EOS。
10.  调用OH_VideoEncoder_FreeOutputBuffer()释放编码帧。 与Surface模式相同，此处不再赘述。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165932.55747044561322808234998096920607:50001231000000:2800:6B0C69312601D247AB37FF9AE61304D5C3B0341F87433B095174CFD2B35FAB2A.png)
后续流程（包括刷新编码器、重置编码器、停止编码器、销毁编码器）与Surface模式一致，请参考Surface模式的步骤14-17。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/video-encoding-temporal-scalability-V14
爬取时间: 2025-04-28 19:57:01
来源: Huawei Developer
基础概念
时域可分层视频编码介绍
可分层视频编码，又叫可分级视频编码、可伸缩视频编码，是视频编码的扩展标准，目前常用的包含SVC（H.264编码标准采用的可伸缩扩展）和SHVC（H.265编码标准采用的可扩展标准）。
其特点是能一次编码出时域分层、空域分层、质量域分层的码流结构，满足因网络、终端能力和用户需求不同带来的的差异化需求。
时域可分层视频编码, 是指能编码出时域分层码流的视频编码，下图是通过参考关系构建的4层时域分层码流结构。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165932.71530126382862687801173115042053:50001231000000:2800:60330BB54E02A5B3C82F96EC1CD11E472E5B1E58C83F36BEFBD06C55D92D6258.png)
从高到低逐层丢弃部分层级的码流（丢弃顺序L3->L2->L1），能实现不同程度的帧率伸缩，以满足传输和解码能力的变化需求。
如下图所示，是上述4层时域分层码流结构丢弃L3后组成的新的码流结构，能在解码正常的情况下实现帧率减半的效果。其他层的丢弃同理。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165932.05432002658585270524092544890159:50001231000000:2800:F52660B9E3C6DA5067F5701B857DF882E4EE846E6504ED8E97A6921797F25910.png)
时域分层码流结构介绍
基础码流是由一个或多个独立图像组（Group Of Pictures，简称GOP）组合而成的。GOP是在编码中一组从I帧开始到I帧结束的连续的可独立解码的图像组。
时域分层码流可以在GOP内继续细分为独立的一个或多个时域图像组（Temporal Group Of Pictures, 简称TGOP），每一个TGOP由一个基本层和后续的一个或多个增强层组合而成，如上述4层时域分层码流结构中的帧0到帧7是一个TGOP。
-  基本层（Base Layer，简称BL）：是GOP中的最底层（L0）。在时域分层中，该层用最低帧率进行编码。
-  增强层（Enhance Layer，简称EL）：是BL之上的层级，由低到高可以分为多层（L1、L2、L3）。在时域分层中，最低层的EL依据BL获得的编码信息，进一步编码帧率更高的层级，更高层的EL会依据BL或低层EL，来编码比低层更高帧率的视频。
如何实现时域分层码流结构
时域分层码流结构的实现是依靠参考关系逐帧指定实现的，参考帧根据在解码图像缓存区（Decoded Picture Buffer，简称DPB）驻留的时长分为短期参考帧和长期参考帧。
-  短期参考帧（Short-Term Reference，简称STR）：是不能长期驻留在DPB中的参考帧，更新方式是先进先出，如果DPB满，旧的短期参考帧会被移出DPB。
-  长期参考帧（Long-Term Reference，简称LTR）：是能长期驻留在DPB中的参考帧，通过标记替换的方式更新，不主动标记替换就不会更新。
虽然STR个数大于1时，也能实现一定的跨帧参考结构，但受限于存在时效过短，时域分层结构支持的跨度有限。LTR则不存在上述问题，也能覆盖短期参考帧跨帧场景。优选使用LTR实现时域分层码流结构。
适用场景
基于上述描述的时域分层编码特点，推荐以下场景使用：
-  场景1：播放侧无缓存或低缓存的实时编码传输场景，例如视频会议、视频直播、协同办公等。
-  场景2：有视频预览播放或倍速播放需求的视频编码录制场景。
若开发场景不涉及动态调整时域参考结构，且分层结构简单，则推荐使用全局时域可分层特性，否则使能长期参考帧特性。
约束和限制
-  不可以混用全局时域可分层特性和长期参考帧特性。 由于底层实现归一，全局时域可分层特性和长期参考帧特性不能同时开启。
-  叠加强制IDR配置时，请使用随帧通路配置。 参考帧仅在GOP内有效，刷新I帧后，DPB随之清空，参考帧也会被清空，因此参考关系的指定受I帧刷新位置影响很大。 使能时域分层能力后，若需要通过OH_MD_KEY_REQUEST_I_FRAME临时请求I帧，应使用生效时机确定的随帧通路配置方式准确告知框架I帧刷新位置以避免参考关系错乱，参考随帧通路配置相关指导，避免使用生效时机不确定的OH_VideoEncoder_SetParameter方式。详情请参考视频编码Surface模式"步骤-4：调用OH_VideoEncoder_RegisterParameterCallback()在Configur接口之前注册随帧通路回调。"。
-  支持OH_AVBuffer回调通路，不支持OH_AVMemory回调通路。 新特性依赖随帧特性，应避免使用OH_AVMemory回调OH_AVCodecAsyncCallback，应使用OH_AVBuffer回调OH_AVCodecCallback。
-  支持时域P分层，不支持时域B分层。 时域可分层编码按分层帧类型分为基于P帧的时域分层和基于B帧的时域编码，当前支持分层P编码，不支持分层B编码。
-  均匀分层模式当前只支持TGOP为2或4。
全局时域可分层特性（Feature_Temporal_Scalability）
接口介绍
全局时域可分层特性，适用于编码稳定和简单的时域分层结构，初始配置，全局生效，不支持动态修改。开发配置参数如下：
| 配置参数 | 语义 |
| --- | --- |
| OH_MD_KEY_VIDEO_ENCODER_ENABLE_TEMPORAL_SCALABILITY | 全局时域分层编码使能参数 |
| OH_MD_KEY_VIDEO_ENCODER_TEMPORAL_GOP_SIZE | 全局时域分层编码TGOP大小参数 |
| OH_MD_KEY_VIDEO_ENCODER_TEMPORAL_GOP_REFERENCE_MODE | 全局时域分层编码TGOP参考模式 |
-  全局时域分层编码使能参数：在配置阶段配置，仅特性支持才会真正使能成功。
-  全局时域分层编码TGOP大小参数：可选配置，影响时域关键帧之间的间隔，用户需要基于自身业务场景下抽帧需求自定义关键帧密度，可在[2, GopSize)范围内配置，若不配置则使用默认值。
-  全局时域分层编码TGOP参考模式参数：可选配置，影响非关键帧参考模式。包括相邻参考ADJACENT_REFERENCE、跨帧参考JUMP_REFERENCE 和均匀分层UNIFORMLY_SCALED_REFERENCE。相邻参考相对跨帧参考拥有更好的压缩性能，跨帧参考相对相邻参考拥有更好的丢帧自由度，均匀分层模式丢帧后的码流分布更均匀，如不配置则使用默认值。 均匀分层模式当前只支持TGOP为2或4。
使用举例1：TGOP=4，相邻参考模式。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165932.54540769582229766831354998141010:50001231000000:2800:B6817C62E4025553209446955EC4543085D1E109DA542D5F5A8DE5701731AAD3.png)
使用举例2：TGOP=4，跨帧参考模式。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165932.00533536014873370730297276767264:50001231000000:2800:6ED6050D72AB6B68065C42898B211DB7282E333ABE57691EB3E55B365BED009E.png)
使用举例3：TGOP=4，均匀分层模式。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165932.36824157493057676463263879492173:50001231000000:2800:7447717E997E2BDBA19AE95163659D891A14F36DEE536C4FAE7B0C0638B7D1E7.png)
开发指导
基础编码流程请参考视频编码开发指导，下面仅针对与基础视频编码过程中存在的区别做具体说明。
1.  在初始阶段创建编码实例时，校验当前视频编码器是否支持全局时域可分层特性。 若支持，则可以使能全局时域可分层特性。
2.  在配置阶段，配置全局时域分层编码特性参数。
3.  （可选）在运行阶段输出轮转中，获取码流对应时域层级信息。 开发者可基于已配置的TGOP参数，按编码出帧数目周期性获取。 通过配置周期获取示例代码如下：
4.  （可选）在运行阶段输出轮转中，使用步骤3获取的时域层级信息，自适应传输或自适应解码。 基于获取的时域可分层码流和对应的层级信息，开发者可选择需要的层级进行传输，或携带至对端自适应选帧解码。
长期参考帧特性（Feature_Long-Term_Reference）
接口介绍
长期参考帧特性提供帧级灵活的参考关系配置。适用于灵活和复杂的时域分层结构。
| 配置参数 | 语义 |
| --- | --- |
| OH_MD_KEY_VIDEO_ENCODER_LTR_FRAME_COUNT | 长期参考帧个数参数 |
| OH_MD_KEY_VIDEO_ENCODER_PER_FRAME_MARK_LTR | 当前帧标记为LTR帧 |
| OH_MD_KEY_VIDEO_ENCODER_PER_FRAME_USE_LTR | 当前帧参考的LTR帧的POC号 |
使用举例，实现时域可分层视频编码介绍中的4层时域分层结构的配置如下：
1.  在配置阶段，将OH_MD_KEY_VIDEO_ENCODER_LTR_FRAME_COUNT 配置为5。
2.  在运行阶段输入轮转中，按如下表所示随帧配置LTR相关参数，下表中\表示不做配置。
| 配置\POC | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| MARK_LTR | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 |
| USE_LTR | \ | \ | 0 | \ | 0 | \ | 4 | \ | 0 | \ | 8 | \ | 8 | \ | 12 | \ | 8 |
开发指导
基础编码流程请参考视频编码开发指导，下面仅针与基础视频编码过程中存在的区别做具体说明。
1.  在初始阶段创建编码实例时，校验当前视频编码器是否支持LTR特性。 若支持，且支持的LTR数目满足自身码流结构需求，则可以使能LTR特性。
2.  在配置之前注册回调时，注册随帧通路回调。 Buffer输入模式示例： Surface输入模式示例：
3.  在配置阶段，配置同时存在LTR最大数目。
4.  （可选）在运行阶段输出轮转中，获取码流对应时域层级信息。 同全局时域分层特性。 因在输入轮转有配置LTR参数，也可在输入轮转中记录，输出轮转中找到对应的输入参数。
5.  （可选）在运行阶段输出轮转中，使用步骤4获取的时域层级信息，自适应传输或自适应解码。 同全局时域分层特性。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/video-encoding-configuration-typical-scenarios-V14
爬取时间: 2025-04-28 19:57:15
来源: Huawei Developer
此文档描述了AVCodec视频编码能力在不同应用场景下的推荐配置参数，供调用者根据实际应用场景进行视频编码应用的开发。
视频编码在视频通话、视频会议、直播、视频编辑、视频分享等场景均有广泛使用，按照体验要求，上述场景可归纳划分为低时延、实时流媒体、离线编码三大类别应用场景。
本文将给出三大类别应用场景下视频编码的推荐参数配置，供调用者根据不同的应用场景下的需求进行参数配置选择。
通用开发步骤
在CMake脚本中链接动态库
上述'sample'字样仅为示例，此处由调用者根据实际工程目录自定义。
添加头文件
低时延场景
低时延编码场景包括视频通话、视频会议、连麦直播等对端到端时延要求较高的交互式应用。
开发指导
基础编码流程请参考视频编码，下面仅针对编码器配置阶段做具体说明。
1.  校验特性。 在创建编码器实例前，校验视频编码器是否支持低时延特性。若支持，则配置编码器参数阶段可以使能低时延特性。否则不能配置此参数。
2.  配置编码器参数。 在配置编码器参数阶段，配置适合低时延编码场景的参数。 低时延编码场景，典型分辨率的编码参数（以H.265为例）推荐如下： 示例中的变量说明如下： 接入帧间隔-1表示只有第一帧为接入帧，开发者可以根据传输情和画质情况，在运行过程中动态配置编码器参数，实现插入新的接入帧（IDR）功能。
3.  （可选）在运行过程中动态配置编码器参数。 详情可参考视频编码Surface模式“步骤-9：OH_VideoEncoder_SetParameter()在运行过程中动态配置编码器参数”。 如果需要适配网络波动，推荐结合采用时域可分层视频编码配置。
| 分辨率 | 帧率（fps） | 码率（kpbs） | 接入帧间隔（ms） | 码控模式 |
| --- | --- | --- | --- | --- |
| 1902x1080 | 30 | 1500 | -1 | CBR |
| 1280x720 | 30 | 1000 | -1 | CBR |
| 960x540 | 30 | 700 | -1 | CBR |
| 640x360 | 30 | 550 | -1 | CBR |
| 320x180 | 20 | 200 | -1 | CBR |
实时流媒体编码
实时流媒体编码场景包括泛娱乐直播、游戏直播等对视频端到端时延要求不高的应用场景。
开发指导
基础编码流程请参考视频编码，下面仅针对编码器配置阶段，配置实时流媒体编码场景的参数做具体说明。
娱乐直播场景，典型分辨率的编码参数（以H.265为例）推荐如下：
| 分辨率 | 帧率（fps） | 码率（kpbs） | 接入帧间隔（ms） | 码控模式 |
| --- | --- | --- | --- | --- |
| 1080x1920 | 25 | 3000 | 2000 | VBR |
| 720x1080 | 25 | 1500 | 2000 | VBR |
| 544x960 | 25 | 1000 | 2000 | VBR |
| 480x864 | 25 | 800 | 2000 | VBR |
游戏直播场景，典型分辨率的编码参数（以H.265为例）推荐如下：
| 分辨率 | 帧率（fps） | 码率（kpbs） | 接入帧间隔（ms） | 码控模式 |
| --- | --- | --- | --- | --- |
| 1080x1920 | 60 | 6000 | 5000 | VBR |
离线编码场景
离线编码场景包括视频编辑、视频分享等多种应用场景。
开发指导
基础编码流程请参考视频编码，下面仅针对编码器配置阶段，配置离线编码场景的编码参数做具体说明。
视频编辑场景，典型分辨率的编码参数（以H.265为例）推荐如下：
| 分辨率 | 帧率（fps） | 码率（kpbs） | 接入帧间隔（ms） | 码控模式 |
| --- | --- | --- | --- | --- |
| 3840x2160 | 30 | 25000 | 5000 | VBR |
| 2560x1440 | 30 | 15000 | 5000 | VBR |
| 1920x1080 | 30 | 10000 | 5000 | VBR |
| 1280x720 | 30 | 5000 | 5000 | VBR |
| 854x480 | 30 | 2000 | 5000 | VBR |
视频分享场景，典型分辨率的编码参数（以H.265为例）推荐如下：
| 分辨率 | 帧率（fps） | 码率（kpbs） | 接入帧间隔（ms） | 码控模式 |
| --- | --- | --- | --- | --- |
| 3840x2160 | 30 | 5600 | 5000 | VBR |
| 2560x1440 | 30 | 4900 | 5000 | VBR |
| 1920x1080 | 30 | 2100 | 5000 | VBR |
| 1280x720 | 30 | 1400 | 5000 | VBR |
| 854x480 | 30 | 400 | 5000 | VBR |

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/video-decoding-V14
爬取时间: 2025-04-28 19:57:29
来源: Huawei Developer
开发者可以调用本模块的Native API接口，完成视频解码，即将媒体数据解码成YUV文件或送显。
具体实现可参考示例工程。
当前支持的解码能力请参考AVCodec支持的格式。
如果需要对HDRVivid视频进行解码，需要配置MimeType为H265 (OH_AVCODEC_MIMETYPE_VIDEO_HEVC)，本功能从API version 11开始支持。
通过视频解码，应用可以实现以下重点能力，包括：
| 支持的能力 | 使用简述 |
| --- | --- |
| 变分辨率 | 解码器支持输入码流分辨率发生变化，发生变化后会触发OH_VideoDecoder_RegisterCallback接口设置的回调函数OnStreamChanged()。具体可参考下文中：Surface模式步骤-3或Buffer模式步骤-3 |
| 动态切换surface | 通过调用OH_VideoDecoder_SetSurface接口配置，仅Surface模式支持。具体可参考下文中：Surface模式步骤-6 |
| 低时延解码 | 通过调用OH_VideoDecoder_Configure接口配置，具体可参考下文中：Surface模式的步骤-5或Buffer模式步骤-5 |
限制约束
1.  在Buffer模式下，开发者通过输出回调函数OH_AVCodecOnNewOutputBuffer获取到OH_AVBuffer的指针实例后，必须通过调用OH_VideoDecoder_FreeOutputBuffer接口 来通知系统该实例已被使用完毕。这样系统才能够将后续解码的数据写入到相应的位置。如果开发者在调用OH_AVBuffer_GetNativeBuffer接口时获取到OH_NativeBuffer指针实例，并且该实例的生命周期超过了当前的OH_AVBuffer指针实例，那么需要进行一次数据的拷贝操作。在这种情况下，开发者需要自行管理新生成的OH_NativeBuffer实例的生命周期，确保其正确使用和释放。
surface输出与buffer输出
1.  两者数据的输出方式不同。
2.  两者的适用场景不同：
3.  在接口调用的过程中，两种方式的接口调用方式基本一致，但存在以下差异点：
两种模式的开发步骤详细说明请参考：Surface模式和Buffer模式。
状态机调用关系
如下为状态机调用关系图：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165932.99639277747083473793584026226314:50001231000000:2800:CCFE0B94F5DE6E6FF3B825F611D2CF9C57D47A85D1423F36C865948A5868F208.png)
1.  有两种方式可以使解码器进入Initialized状态：
2.  Initialized状态下，调用OH_VideoDecoder_Configure接口配置解码器，配置成功后解码器进入Configured状态。
3.  Configured状态下，调用OH_VideoDecoder_Prepare接口进入Prepared状态。
4.  Prepared状态下，调用OH_VideoDecoder_Start接口使解码器进入Executing状态：
5.  在极少数情况下，解码器可能会遇到错误并进入Error状态。解码器的错误传递，可以通过队列操作返回无效值或者抛出异常：
6.  Executing状态具有三个子状态：Flushed、Running和End-of-Stream：
7.  使用完解码器后，必须调用OH_VideoDecoder_Destroy接口销毁解码器实例，使解码器进入Released状态。
开发指导
详细的API说明请参考API文档。
如下为视频解码调用关系图：
-  虚线表示可选。
-  实线表示必选。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165932.47106791541040252337957954439523:50001231000000:2800:13EE43DF393609621C6779137A3664A1502EA7AF4E2A87A4E400ECBA9FF20616.png)
在 CMake 脚本中链接动态库
上述'sample'字样仅为示例，此处由开发者根据实际工程目录自定义。
定义基础结构
本部分示例代码按照C++17标准编写，仅作参考。开发者可以参考此部分，定义自己的buffer对象。
1.  添加头文件。
2.  解码器回调buffer的信息。
3.  解码输入输出队列。
4.  全局变量 仅做参考，可以根据实际情况将其封装到对象中。
Surface模式
参考以下示例代码，开发者可以完成Surface模式下视频解码的全流程。此处以H.264码流文件输入，解码送显输出为例。
本模块目前仅支持异步模式的数据轮转。
1.  添加头文件。
2.  创建解码器实例。 开发者可以通过名称或媒体类型创建解码器。示例中的变量说明如下：
3.  调用OH_VideoDecoder_RegisterCallback()设置回调函数。 注册回调函数指针集合OH_AVCodecCallback，包括： 开发者可以通过处理该回调报告的信息，确保解码器正常运转。 回调函数的具体实现可参考示例工程。
4.  （可选）OH_VideoDecoder_SetDecryptionConfig设置解密配置。在获取到DRM信息（参考音视频解封装开发步骤第4步），完成DRM许可证申请后，通过此接口进行解密配置。此接口需在Prepare前调用。在Surface模式下，DRM解密能力既支持安全视频通路，也支持非安全视频通路。DRM相关接口详见DRM API文档。 添加头文件。 在 CMake 脚本中链接动态库。 根据DRM码流要求的内容保护级别和硬件设备支持的内容保护级别创建对应的通路。 如果DRM码流要求的内容保护级别是硬件级保护，则推荐使用安全视频通路，示例如下： 如果DRM码流要求的内容保护级别是软件级保护，则推荐使用非安全视频通路，示例如下：
5.  调用OH_VideoDecoder_Configure()配置解码器。 详细可配置选项的说明请参考视频专有键值对。 参数校验规则请参考OH_VideoDecoder_Configure() 参考文档。 参数取值范围可以通过能力查询接口获取，具体示例请参考获取支持的编解码能力。 目前支持的所有格式都必须配置以下选项：视频帧宽度、视频帧高度、视频像素格式。
6.  设置surface。 本例中的nativeWindow，有两种方式获取： Surface模式，开发者可以在解码过程中执行该步骤，即动态切换surface。
7.  （可选）OH_VideoDecoder_SetParameter()动态配置解码器surface参数。 详细可配置选项的说明请参考视频专有键值对。
8.  调用OH_VideoDecoder_Prepare()解码器就绪。 该接口将在解码器运行前进行一些数据的准备工作。
9.  调用OH_VideoDecoder_Start()启动解码器。
10.  （可选）调用OH_AVCencInfo_SetAVBuffer()，设置cencInfo。 若当前播放的节目是DRM加密节目，应用自行实现媒体解封装功能而非使用系统解封装功能时，需调用OH_AVCencInfo_SetAVBuffer()将cencInfo设置到AVBuffer，这样AVBuffer携带待解密的数据以及cencInfo，以实现AVBuffer中媒体数据的解密。当应用使用系统解封装功能时，则无需调用此接口。 添加头文件。 在 CMake 脚本中链接动态库。 使用示例：
11.  调用OH_VideoDecoder_PushInputBuffer()写入解码码流。 送入输入队列进行解码，以下示例中：
12.  调用OH_VideoDecoder_RenderOutputBuffer()/OH_VideoDecoder_RenderOutputBufferAtTime()显示并释放解码帧， 或调用OH_VideoDecoder_FreeOutputBuffer()释放解码帧。 以下示例中： 如果要获取buffer的属性，如pixel_format、stride等可通过调用OH_NativeWindow_NativeWindowHandleOpt接口获取。
13.  （可选）调用OH_VideoDecoder_Flush()刷新解码器。 调用OH_VideoDecoder_Flush接口后，解码器仍处于运行态，但会清除解码器中缓存的输入和输出数据及参数集如H.264格式的PPS/SPS。 此时需要调用OH_VideoDecoder_Start接口重新开始解码。 以下示例中： Flush之后，重新调用OH_VideoDecoder_Start接口时，需要重新传PPS/SPS。
14.  （可选）调用OH_VideoDecoder_Reset()重置解码器。 调用OH_VideoDecoder_Reset接口后，解码器回到初始化的状态，需要调用OH_VideoDecoder_Configure接口、OH_VideoDecoder_SetSurface接口和OH_VideoDecoder_Prepare接口重新配置。
15.  （可选）调用OH_VideoDecoder_Stop()停止解码器。 调用OH_VideoDecoder_Stop()后，解码器保留了解码实例，释放输入输出buffer。开发者可以直接调用OH_VideoDecoder_Start接口继续解码，输入的第一个buffer需要携带参数集，从IDR帧开始送入。
16.  调用OH_VideoDecoder_Destroy()销毁解码器实例，释放资源。
Buffer模式
参考以下示例代码，开发者可以完成Buffer模式下视频解码的全流程。此处以H.264文件输入，解码成YUV文件为例。
本模块目前仅支持异步模式的数据轮转。
1.  添加头文件。
2.  创建解码器实例。 与Surface模式相同，此处不再赘述。
3.  调用OH_VideoDecoder_RegisterCallback()设置回调函数。 注册回调函数指针集合OH_AVCodecCallback，包括： 开发者可以通过处理该回调报告的信息，确保解码器正常运转。 回调函数的具体实现可参考示例工程。 在回调函数中，对数据队列进行操作时，需要注意多线程同步的问题。
4.  （可选）OH_VideoDecoder_SetDecryptionConfig设置解密配置。在获取到DRM信息（参考音视频解封装开发步骤第4步），完成DRM许可证申请后，通过此接口进行解密配置。此接口需在Prepare前调用。在Buffer模式下，DRM解密能力仅支持非安全视频通路。DRM相关接口详见DRM API文档。 添加头文件。 在 CMake 脚本中链接动态库。 使用示例：
5.  调用OH_VideoDecoder_Configure()配置解码器。 与Surface模式相同，此处不再赘述。
6.  调用OH_VideoDecoder_Prepare()解码器就绪。 该接口将在解码器运行前进行一些数据的准备工作。
7.  调用OH_VideoDecoder_Start()启动解码器。
8.  （可选）调用OH_AVCencInfo_SetAVBuffer()，设置cencInfo。 与Surface模式相同，此处不再赘述。 使用示例：
9.  调用OH_VideoDecoder_PushInputBuffer()写入解码码流。 与Surface模式相同，此处不再赘述。
10.  调用OH_VideoDecoder_FreeOutputBuffer()释放解码帧。 以下示例中： NV12/NV21图像如果需要依次将Y,U,V三个分量拷贝至另一块buffer中，以NV12图像为例，按行拷贝示例如下： 以NV12图像为例，width、height、wStride、hStride图像排布参考下图： 添加头文件。 使用示例： 硬件解码在处理buffer数据时（释放数据前），输出回调开发者收到的AVbuffer是宽、高对齐后的图像数据。 一般需要获取数据的宽、高、跨距、像素格式来保证解码输出数据被正确的处理。 具体实现请参考：Buffer模式的步骤3-调用OH_VideoDecoder_RegisterCallback()设置回调函数来获取数据的宽、高、跨距、像素格式。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165933.49773325994831942188453533581004:50001231000000:2800:50564E439025F2F06CE6F2402360AE1C8C2A6DE4CC7386981F4C7A78FA0FA733.png)
后续流程（包括刷新解码器、重置解码器、停止解码器、销毁解码器）与Surface模式基本一致，请参考Surface模式的步骤13-16。
示例代码

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/parallel-decoding-nativewindow-V14
爬取时间: 2025-04-28 19:57:43
来源: Huawei Developer
场景介绍
为了解码Surface模式的正常创建，在XComponent尚未创建或OpenGL后处理（NativeImage）尚未初始化的情况下，可以创建一个空的surface，以确保视频解码器能够正常创建和运行。
开发步骤
以下步骤描述了在surface的消费端没有创建之前，如何并行创建视频解码器和NativeWindow，让视频解码器正常创建执行。
添加动态链接库
上述'sample'字样仅为示例，此处由开发者根据实际工程目录自定义。
头文件
1.  创建OH_NativeImage实例。
2.  获取对应的数据生产者端NativeWindow。
3.  设置NativeWindow的宽高。
4.  注册NativeImage的回调函数。 注册OH_NativeImage的监听者OH_OnFrameAvailableListener，包括： 在此示例中，回调函数的实现仅仅是将buffer取出来并释放，开发者可以根据业务需求自行拓展。
5.  配置解码器。 具体开发指导请参考视频解码Surface模式“步骤-5：调用OH_VideoDecoder_Configure()配置解码器”。
6.  设置surface。 在应用业务真正的surface消费端创建成功之前，可以先使用上面临时创建的消费端连接解码器。 示例中的变量说明如下：
7.  启动解码器。 具体开发指导请参考视频解码Surface模式“步骤-9：调用OH_VideoDecoder_Start()启动解码器”。
8.  设置surface。 在应用业务真正的surface消费端创建成功后，可以调用OH_VideoDecoder_SetSurface接口，将解码输出重定向到新的surface上。 本例中的nativeWindow，有两种方式获取：
9.  销毁OH_NativeImage实例。 在调用OH_VideoDecoder_Destroy接口后，调用OH_NativeImage_Destroy接口销毁OH_NativeImage实例。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audiovivid-V14
爬取时间: 2025-04-28 19:57:57
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audiovivid-intro-V14
爬取时间: 2025-04-28 19:58:11
来源: Huawei Developer
Audio Vivid（菁彩三维声）是全球首个基于AI技术的音频编解码标准，由世界超高清视频产业联盟（UWA联盟）与数字音视频编解码技术标准工作组（AVS）联合制定，共同发布。包含音频PCM数据以及元数据的音频格式，相比传统立体声音源，Audio Vivid包含音频内容源的元数据信息，能够还原物理和感知世界中的真实听感，打造极致的沉浸式听觉体验。
HarmonyOS打造全链路高清空间音频系统，包含Audio Vivid编解码、空间音频渲染算法等关键能力，并在各类终端产品逐步构建全场景空间音频特性，从传统立体声升级到三维声，获得更好的音质、更沉浸的空间感。
HarmonyOS支持播放Audio Vivid格式音源，并在耳机实现双耳空间音频渲染重放，在手机、平板、PC等支持外放空间音频渲染重放。系统的空间音频渲染能力无感接入，不需要做特定适配。
搭配HarmonyOS高清空间音频渲染能力，将音乐中的人声和各种乐器声作为独立的声音对象，重新定义各种声音对象的位置、移动轨迹和声音大小、远近等要素，实现声音在听众四周及上方全面萦绕，实现更佳的空间音频沉浸式体验，获得影院、音乐厅等的临场感与艺术体验。佩戴支持头动跟踪的耳机收听空间音频，还能实现动态头动跟踪，让声音围绕听众重新定位，还原逼真的临场感。
以下主要介绍使用HarmonyOS进行Audio Vivid格式音源的端到端播放的流程。
Audio Vivid端到端播放包括调用系统编解码能力进行解封装、解码，以及调用系统播放能力进行渲染播放两个部分。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audiovivid-avdemuxer-V14
爬取时间: 2025-04-28 19:58:24
来源: Huawei Developer
获取到Audio Vivid封装的mp4文件后，先调用解封装相关接口，选中音频轨，读取每一帧Audio Vivid，送入解码器中。详细的API请参考音频解封装API参考。
在Cmake脚本中链接到动态库
添加头文件
开发步骤

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audiovivid-audiodecoder-V14
爬取时间: 2025-04-28 19:58:38
来源: Huawei Developer
获取解封装后的数据，送入解码器中，使用解码器获取PCM和Metadata元数据。详细的API请参考音频解码API参考。
AudioVivid解码当前支持的规格如下表所示。
| 支持采样率  | 32000，44100，48000，96000，192000  |
| --- | --- |
| 支持码率范围  | 16000~3075000  |
| 支持声道数  | 1~16  |
| 支持的位深  | S16，S24  |
支持采样率
32000，44100，48000，96000，192000
支持码率范围
16000~3075000
支持声道数
1~16
支持的位深
S16，S24
在Cmake脚本中链接到动态库
添加头文件
定义相关实例
定义CodecBufferInfo
解码码流的属性定义，为后面传给播放的码流数据封装。
定义解码工作队列
定义回调函数
开发步骤

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audiovivid-audiorenderer-V14
爬取时间: 2025-04-28 19:58:52
来源: Huawei Developer
在获取到解码后的Audio Vivid的PCM数据和元数据之后，可以调用OHAudio的相关播放接口，进行Audio Vivid格式音源的渲染播放。详细的API说明请参考OHAudio API参考。
在Cmake脚本中链接到动态库
添加头文件
开发步骤
开发者可以通过以下几个步骤来实现一个简单的播放功能。
1.  OHAudio提供OH_AudioStreamBuilder接口，遵循构造器设计模式，用于构建音频流。在Audio Vivid播放场景下，需要选择OH_AudioStream_Type为AUDIOSTREAM_TYPE_RENDERER，创建一个渲染播放类型的音频构造器。
2.  创建音频播放构造器后，可以设置音频流所需要的参数，可以参考以下案例。 Audio Vivid音源搭配系统空间音频渲染算法，播放效果和体验最佳。系统会根据输出音频流的工作场景（OH_AudioStream_Usage），选择使用对应的空间音频渲染效果，当前支持的工作场景包括音乐、电影和有声读物。
3.  OHAudio使用回调模式进行音频流数据的写入，以及各种音频事件的上报，应用可以按需选择需要监听的音频事件。 为了避免不可预期的行为，在设置音频回调函数时，请确认OH_AudioRenderer_Callbacks的每一个回调都被自定义的回调方法或空指针初始化。 对于Audio Vivid播放场景，需要另外使用OH_AudioRenderer_WriteDataWithMetadataCallback进行PCM和元数据的写入。
4.  可以使用以下接口，实现对音频流的控制，完成开始播放、暂停播放、停止播放、清除缓存等基本操作。 在不再使用该条音频流时，可以释放播放实例，以便更好地管理内存。 接口 说明 OH_AudioRenderer_Start 开始播放 OH_AudioRenderer_Pause 暂停播放 OH_AudioRenderer_Stop 停止播放 OH_AudioRenderer_Flush 释放缓存数据 OH_AudioRenderer_Release 释放播放实例
5.  当构造器不再使用时，需要释放相关资源。
| 接口  | 说明  |
| --- | --- |
| OH_AudioRenderer_Start  | 开始播放  |
| OH_AudioRenderer_Pause  | 暂停播放  |
| OH_AudioRenderer_Stop  | 停止播放  |
| OH_AudioRenderer_Flush  | 释放缓存数据  |
| OH_AudioRenderer_Release  | 释放播放实例  |

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/hdr-vivid-video-recorder-V14
爬取时间: 2025-04-28 20:01:30
来源: Huawei Developer
开发者可以调用本模块的Native API接口，实现在视频录制中支持HDR Vivid标准。
视频录制的主要流程是“相机采集 > 编码 > 封装成mp4文件”。
HDR Vivid视频编码
应用创建H.265编码器，配置profile(main 10)相机底层包含HDR Vivid的surfacebuffer内容，编码器消费surfacebuffer编码生成对应码流。
仅在Surface模式下支持HDR Vivid视频编码。
在 CMake 脚本中链接动态库
开发步骤
1.  应用可以通过名称或媒体类型创建编码器。示例中的变量说明如下：
2.  具体可参考：视频编码Surface模式中的“步骤3：调用OH_VideoEncoder_RegisterCallback()设置回调函数”。
3.  具体可参考：视频编码Surface模式中的“步骤6：获取surface”。
4.  具体可参考：视频编码Surface模式中的“步骤8：调用OH_VideoEncoder_Start()启动编码器”。
HDR Vivid视频封装
调用Muxer可以将HDR Vivid码流封装成文件，码流格式需指定为hevc码流，并设置宽、高、isHDRVivid信息。Color信息通常需要从编码获取并设置给封装器。
在 CMake 脚本中链接动态库
开发步骤
处理视频帧数据

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/hdrvivid2sdr-V14
爬取时间: 2025-04-28 20:01:44
来源: Huawei Developer
调用者在视频分享或者编辑场景时，有时需要将HDR Vivid视频转换为SDR视频，可以调用AVCodec能力实现该功能。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165934.76607939567000648309599404394776:50001231000000:2800:73807EC8BEE7C10E5ACB08FF5A09109984669F9D78C59784C3A860E3D9AE42A3.png)
限制约束
在 CMake 脚本中链接动态库
开发步骤
1.  如果非HDR Vivid视频，则参考视频解码进行解码处理，此处不再赘述。 如果判断为HDR Vivid视频，则继续执行以下步骤。 如果输入源非HDR Vivid视频，会通过回调函数OH_AVCodecOnError()报告错误码AV_ERR_VIDEO_UNSUPPORTED_COLOR_SPACE_CONVERSION。
2.  查询系统支持的解码器能力，根据查询结果基于name创建硬解码器。 示例中的变量说明如下： 由于目前仅硬件解码器支持该能力，因此必须根据解码器name进行创建。
3.  具体可参考：HDR Vivid视频播放-HDR Vivid视频解码中的“步骤3：配置异步回调函数”
4.  需配置项：视频帧宽度、视频帧高度、视频像素格式、指定输出为SDR。具体示例如下： 通过配置OH_MD_KEY_VIDEO_DECODER_OUTPUT_COLOR_SPACE，支持在解码后输出SDR图像，目前输入仅支持为HDR Vivid的码流，输出仅支持配置为OH_COLORSPACE_BT709_LIMIT。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/file-muxing-demuxing-V14
爬取时间: 2025-04-28 20:02:38
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/audio-video-muxer-V14
爬取时间: 2025-04-28 20:02:52
来源: Huawei Developer
开发者可以调用本模块的Native API接口，完成音视频封装，即将音频、视频等编码后的媒体数据，按一定的格式存储到文件里。
当前支持的封装能力请参考AVCodec支持的格式
如果需要对HDRVivid视频码流进行封装，需要配置MimeType为H265 (OH_AVCODEC_MIMETYPE_VIDEO_HEVC)，本功能从API version 11开始支持。
适用场景
-  录像、录音 保存录像、录音文件时，需要先对音视频流进行编码，再封装为文件。
-  音视频编辑 完成编辑后的音视频，需要封装为文件。
-  音视频转码 转码后，保存文件时需要封装。
开发指导
详细的API说明请参考API文档。
如果调用封装模块写本地文件，需要向用户申请授权：ohos.permission.READ_MEDIA, ohos.permission.WRITE_MEDIA。
在 CMake 脚本中链接动态库
开发步骤
参考以下示例代码，完成音视频封装的全流程。以封装mp4格式的音视频文件为例。
不同的封装格式需要配置的key请参考AVCodec支持的格式。
1.  添加头文件。
2.  调用OH_AVMuxer_Create()创建封装器实例对象。
3.  （可选）调用OH_AVMuxer_SetRotation()设置旋转角度。
4.  添加文件级数据。
5.  添加音频轨。 方法一：用OH_AVFormat_Create创建format 方法二：用OH_AVFormat_CreateAudioFormat创建format
6.  添加视频轨。 方法一：用OH_AVFormat_Create创建format 方法二：用OH_AVFormat_CreateVideoFormat创建format
7.  添加封面轨。 方法一：用OH_AVFormat_Create创建format 方法二：用OH_AVFormat_CreateVideoFormat创建format
8.  调用OH_AVMuxer_Start()开始封装。
9.  调用OH_AVMuxer_WriteSampleBuffer()，写入封装数据。 封装数据包括视频、音频、封面等数据。
10.  调用OH_AVMuxer_Stop()，停止封装。
1.  调用OH_AVMuxer_Destroy()销毁实例，释放资源。 注意不能重复销毁，否则将会导致程序崩溃。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/avsession-kit-V14
爬取时间: 2025-04-28 20:04:18
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/avsession-overview-V14
爬取时间: 2025-04-28 20:04:31
来源: Huawei Developer
AVSession Kit（Audio & Video Session Kit，音视频播控服务）是系统提供的音视频管控服务，用于统一管理系统中所有音视频行为，帮助开发者快速构建音视频统一展示和控制能力。
能力范围
-  提供音视频统一管控能力，音视频类应用接入AVSession后，可以发送应用的数据（比如正在播放的歌曲、歌曲的播放状态等），用户可以通过系统播控中心、语音助手等应用切换多个应用、多个设备播放。
-  提供音频后台约束能力，音频接入AVSession后，可以进行后台音频播放。此功能需要同时申请后台任务。
亮点/特征
-  投播体验一致 提供音视频统一管控能力，音视频类应用接入AVSession后，可以发送应用的数据（比如正在播放的歌曲、歌曲的播放状态等）。 用户可以通过系统播控中心、语音助手等应用切换多个应用、多个设备播放。
-  规范后台播放管理 通过播控中心，应用后台播放可见可控，音视频应用可统一控制。 系统针对后台播放进行强制管控，未接入AVSession的应用在退到后台时，将会被强制暂停音频播放。解决应用在后台恶意播放，而用户无法找到对应应用无法关闭的问题。
基础概念
在开发前，需要先了解以下基础概念：
-  媒体会话（AVSession） 媒体会话的一端连接被控的音视频应用，另一端连接音视频应用的控制端（如播控中心、语音助手等）。媒体会话提供了音视频应用和音视频应用控制端之间进行信息交换的通道。
-  媒体会话提供方 媒体会话提供方指接入媒体会话的音视频应用。音视频应用接入媒体会话后，需要向媒体会话提供播放的媒体信息，例如播放曲目名称、播放状态等。同时，音视频应用需要通过媒体会话接收控制端发出的控制命令并进行正确响应。
-  媒体会话控制方 媒体会话控制方指接入媒体会话并具有全局管控音视频行为功能的应用，例如系统播控中心、语音助手等。为便于开发者理解，下文将多处使用系统应用播控中心，作为媒体会话控制方举例。播控中心等系统应用接入媒体会话后，可以通过监听媒体会话获取最新的媒体信息，也可以通过媒体会话向音视频应用发出控制命令。
-  媒体会话控制器（AVSessionController） 媒体会话控制器的持有者，一般指媒体会话控制方，可以控制媒体会话提供方的应用播放行为，也可以获取应用的播放信息，还可以监听音视频应用播放状态的变化，用于确保媒体会话信息在音视频应用和播控中心之间的同步。
-  媒体会话管理器（AVSessionManager） 媒体会话管理器提供了管理媒体会话的能力，可以创建媒体会话、创建媒体会话控制器、发送系统控制事件，也支持对媒体会话的状态进行监听。
媒体会话交互过程
媒体会话分为本地和分布式两种场景。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165934.75636636017456037686782850208958:50001231000000:2800:8AE20CF85CE46AA8EA9518F5CAEC847ED80B3DCB94F870C9CEAE048EA1A0E867.png)
-  本地媒体会话 本地媒体会话在本地设备中的媒体会话提供方和媒体会话控制方之间建立连接，实现系统中音视频应用统一的媒体播放控制和媒体信息显示。
-  分布式媒体会话 分布式媒体会话在跨设备场景中的媒体会话提供方和媒体会话控制方之间建立连接，实现音视频应用跨设备的媒体播放控制和媒体信息显示。例如，将设备A中播放的内容投播到设备B，并在设备B中进行播放控制。
约束和限制
所有需要进行后台播放的音视频应用，都需要同时接入AVSession和后台任务管理，未接入的应用在退到后台时，将会被强制暂停音频播放。
与相关Kit的关系
应用实现后台播放时，需要使用BackgroundTasks Kit（后台任务管理）的能力，申请对应的长时任务，避免进入挂起（Suspend）状态。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/local-avsession-V14
爬取时间: 2025-04-28 20:04:45
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/local-avsession-overview-V14
爬取时间: 2025-04-28 20:05:38
来源: Huawei Developer
交互过程
本地媒体会话的数据源均在设备本地，交互过程如图所示。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165935.08908332397338156070884661764625:50001231000000:2800:41FF8348D954673291A0DB4C122F0C009006C53E3D617E007FDBE5ABAE6CAA94.png)
此过程中涉及两大角色，媒体会话提供方和媒体会话控制方。
媒体会话控制方为系统应用，三方应用可以成为媒体会话提供方。
本地媒体会话中，媒体会话提供方通过媒体会话管理器和媒体会话控制方进行信息交互：
1.  媒体会话提供方通过AVSessionManager创建AVSession对象。
2.  媒体会话提供方通过AVSession对象，设置会话元数据（媒体ID、标题、媒体时长等）、会话播放属性（播放状态、播放倍速、播放位置等）等。
3.  媒体会话控制方通过AVSessionManager创建AVSessionController对象。
4.  媒体会话控制方通过AVSessionController对象可以监听对应会话元数据变化、播放属性变化等。
5.  媒体会话控制方通过AVSessionController对象还可以向媒体会话发送控制命令。
6.  媒体会话提供方通过AVSession对象可以监听来自媒体会话控制方的控制命令，例如：“play”播放、“playNext”播放下一首、“fastForward”快进、 “setSpeed”设置播放倍数等。
媒体会话管理器
媒体会话管理器（AVSessionManager），提供了管理AVSession的能力，可以创建AVSession、创建AVSessionController、发送系统控制事件，也支持对AVSession的状态进行监听。
实际上，AVSessionManager与AVSession、AVSessionController对象不同，并不是一个具体的对象，它是媒体会话的根命名域。在实际编程过程中，可以通过如下方式引入：
```typescript
import { avSession as AVSessionManager } from '@kit.AVSessionKit';
```
根命名域中的所有方法都可以作为AVSessionManager的方法。
例如，媒体会话提供方通过AVSessionManager创建媒体会话的示例如下所示：
```typescript
// 创建session
let context: Context = getContext(this);
async function createSession() {
let session: AVSessionManager.AVSession = await AVSessionManager.createAVSession(context, 'SESSION_NAME', 'audio');
console.info(`session create done : sessionId : ${session.sessionId}`);
}
```
更多关于AVSessionManager的方法，可以参考API文档。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/using-avsession-developer-V14
爬取时间: 2025-04-28 20:05:52
来源: Huawei Developer
音视频应用在实现音视频功能的同时，需要作为媒体会话提供方接入媒体会话，在媒体会话控制方（例如播控中心）中展示媒体相关信息，及响应媒体会话控制方下发的播控命令。
基本概念
-  媒体会话元数据（AVMetadata）： 用于描述媒体数据相关属性，包含标识当前媒体的ID（assetId），上一首媒体的ID（previousAssetId），下一首媒体的ID（nextAssetId），标题（title），专辑作者（author），专辑名称（album），词作者（writer），媒体时长（duration）等属性。
-  媒体播放状态（AVPlaybackState）：用于描述媒体播放状态的相关属性，包含当前媒体的播放状态（state）、播放位置（position）、播放倍速（speed）、缓冲时间（bufferedTime）、循环模式（loopMode）、是否收藏（isFavorite）、正在播放的媒体Id（activeItemId）、自定义媒体数据（extras）等属性。
接口说明
媒体会话提供方使用的关键接口如下表所示。接口返回值有两种返回形式：callback和promise，下表中为callback形式接口，promise和callback只是返回值方式不一样，功能相同。
更多API说明请参见API文档。
| 接口名 | 说明 |
| --- | --- |
| createAVSession(context: Context, tag: string, type: AVSessionType, callback: AsyncCallback<AVSession>): void10+ | 创建媒体会话。 一个UIAbility只能存在一个媒体会话，重复创建会失败。 |
| setAVMetadata(data: AVMetadata, callback: AsyncCallback<void>): void10+ | 设置媒体会话元数据。 |
| setAVPlaybackState(state: AVPlaybackState, callback: AsyncCallback<void>): void10+ | 设置媒体会话播放状态。 |
| setLaunchAbility(ability: WantAgent, callback: AsyncCallback<void>): void10+ | 设置启动UIAbility。 |
| getController(callback: AsyncCallback<AVSessionController>): void10+ | 获取当前会话自身控制器。 |
| getOutputDevice(callback: AsyncCallback<OutputDeviceInfo>): void10+ | 获取播放设备相关信息。 |
| activate(callback: AsyncCallback<void>): void10+ | 激活媒体会话。 |
| deactivate(callback: AsyncCallback<void>): void10+ | 禁用当前会话。 |
| destroy(callback: AsyncCallback<void>): void10+ | 销毁媒体会话。 |
| setAVQueueItems(items: Array<AVQueueItem>, callback: AsyncCallback<void>): void 10+ | 设置媒体播放列表。 |
| setAVQueueTitle(title: string, callback: AsyncCallback<void>): void10+ | 设置媒体播放列表名称。 |
| dispatchSessionEvent(event: string, args: {[key: string]: Object}, callback: AsyncCallback<void>): void10+ | 设置会话内自定义事件。 |
| setExtras(extras: {[key: string]: Object}, callback: AsyncCallback<void>): void10+ | 设置键值对形式的自定义媒体数据包。 |
| getOutputDeviceSync(): OutputDeviceInfo10+ | 使用同步方法获取当前输出设备信息。 |
创建媒体会话。
一个UIAbility只能存在一个媒体会话，重复创建会失败。
开发步骤
音视频应用作为媒体会话提供方接入媒体会话的基本步骤如下所示：
1.  通过AVSessionManager的方法创建并激活媒体会话。
```typescript
import { avSession as AVSessionManager } from '@kit.AVSessionKit';
// 开始创建并激活媒体会话
// 创建session
let context: Context = getContext(this);
async function createSession() {
let type: AVSessionManager.AVSessionType = 'audio';
let session = await AVSessionManager.createAVSession(context, 'SESSION_NAME', type);
await session.activate();
console.info(`session create done : sessionId : ${session.sessionId}`);
}
```
2.  跟随媒体信息的变化，及时设置媒体会话信息。需要设置的媒体会话信息主要包括： 音视频应用设置的媒体会话信息，会被媒体会话控制方通过AVSessionController相关方法获取后进行显示或处理。
```typescript
import { avSession as AVSessionManager } from '@kit.AVSessionKit';
import { BusinessError } from '@kit.BasicServicesKit';
let context: Context = getContext(this);
async function setSessionInfo() {
// 假设已经创建了一个session，如何创建session可以参考之前的案例
let session = await AVSessionManager.createAVSession(context, 'SESSION_NAME', 'audio');
// 播放器逻辑··· 引发媒体信息与播放状态的变更
// 设置必要的媒体信息
let metadata: AVSessionManager.AVMetadata = {
assetId: '0', // 由应用指定，用于标识应用媒体库里的媒体
title: 'TITLE',
mediaImage: 'IMAGE',
artist: 'ARTIST'
};
session.setAVMetadata(metadata).then(() => {
console.info(`SetAVMetadata successfully`);
}).catch((err: BusinessError) => {
console.error(`Failed to set AVMetadata. Code: ${err.code}, message: ${err.message}`);
});
// 简单设置一个播放状态 - 暂停 未收藏
let playbackState: AVSessionManager.AVPlaybackState = {
state:AVSessionManager.PlaybackState.PLAYBACK_STATE_PAUSE,
isFavorite:false
};
session.setAVPlaybackState(playbackState, (err) => {
if (err) {
console.error(`Failed to set AVPlaybackState. Code: ${err.code}, message: ${err.message}`);
} else {
console.info(`SetAVPlaybackState successfully`);
}
});
// 设置一个播放列表
let queueItemDescription_1: AVSessionManager.AVMediaDescription = {
assetId: '001',
title: 'music_name',
subtitle: 'music_sub_name',
description: 'music_description',
mediaImage: "PIXELMAP_OBJECT",
extras: {'extras':'any'}
};
let queueItem_1: AVSessionManager.AVQueueItem = {
itemId: 1,
description: queueItemDescription_1
};
let queueItemDescription_2: AVSessionManager.AVMediaDescription = {
assetId: '002',
title: 'music_name',
subtitle: 'music_sub_name',
description: 'music_description',
mediaImage: "PIXELMAP_OBJECT",
extras: {'extras':'any'}
};
let queueItem_2: AVSessionManager.AVQueueItem = {
itemId: 2,
description: queueItemDescription_2
};
let queueItemsArray = [queueItem_1, queueItem_2];
session.setAVQueueItems(queueItemsArray).then(() => {
console.info(`SetAVQueueItems successfully`);
}).catch((err: BusinessError) => {
console.error(`Failed to set AVQueueItem, error code: ${err.code}, error message: ${err.message}`);
});
// 设置媒体播放列表名称
let queueTitle = 'QUEUE_TITLE';
session.setAVQueueTitle(queueTitle).then(() => {
console.info(`SetAVQueueTitle successfully`);
}).catch((err: BusinessError) => {
console.info(`Failed to set AVQueueTitle, error code: ${err.code}, error message: ${err.message}`);
});
}
```
3.  设置用于被媒体会话控制方拉起的UIAbility。当用户操作媒体会话控制方的界面时，例如点击播控中心的卡片，可以拉起此处配置的UIAbility。 设置UIAbility时通过WantAgent接口实现，更多关于WantAgent的信息请参考WantAgent。
```typescript
import { avSession as AVSessionManager } from '@kit.AVSessionKit';
import { wantAgent } from '@kit.AbilityKit';
let context: Context = getContext(this);
async function getWantAgent() {
let type: AVSessionManager.AVSessionType = 'audio';
// 假设已经创建了一个session，如何创建session可以参考之前的案例
let session = await AVSessionManager.createAVSession(context, 'SESSION_NAME', type);
let wantAgentInfo: wantAgent.WantAgentInfo = {
wants: [
{
bundleName: 'com.example.musicdemo',
abilityName: 'MainAbility'
}
],
// OperationType.START_ABILITIES
operationType: 2,
requestCode: 0,
wantAgentFlags: [wantAgent.WantAgentFlags.UPDATE_PRESENT_FLAG]
}
wantAgent.getWantAgent(wantAgentInfo).then((agent) => {
session.setLaunchAbility(agent);
})
}
```
4.  设置一个即时的自定义会话事件，以供媒体控制方接收到事件后进行相应的操作。 通过dispatchSessionEvent方法设置的数据不会保存在会话对象或AVSession服务中。
```typescript
import { avSession as AVSessionManager } from '@kit.AVSessionKit';
import { BusinessError } from '@kit.BasicServicesKit';
let context: Context = getContext(this);
async function dispatchSessionEvent() {
// 假设已经创建了一个session，如何创建session可以参考之前的案例
let type: AVSessionManager.AVSessionType = 'audio';
let session = await AVSessionManager.createAVSession(context, 'SESSION_NAME', type);
let eventName = 'dynamic_lyric';
await session.dispatchSessionEvent(eventName, {lyric : 'This is my lyric'}).then(() => {
console.info(`Dispatch session event successfully`);
}).catch((err: BusinessError) => {
console.error(`Failed to dispatch session event. Code: ${err.code}, message: ${err.message}`);
})
}
```
5.  设置与当前会话相关的自定义媒体数据包，以供媒体控制方接收到事件后进行相应的操作。 通过setExtras方法设置的数据包会被存储在AVSession服务中，数据的生命周期与会话一致；会话对应的Controller可以使用getExtras来获取该数据。
```typescript
import { avSession as AVSessionManager } from '@kit.AVSessionKit';
import { BusinessError } from '@kit.BasicServicesKit';
let context: Context = getContext(this);
async function setExtras() {
// 假设已经创建了一个session，如何创建session可以参考之前的案例
let type: AVSessionManager.AVSessionType = 'audio';
let session = await AVSessionManager.createAVSession(context, 'SESSION_NAME', type);
await session.setExtras({extra : 'This is my custom meida packet'}).then(() => {
console.info(`Set extras successfully`);
}).catch((err: BusinessError) => {
console.error(`Failed to set extras. Code: ${err.code}, message: ${err.message}`);
})
}
```
6.  注册播控命令事件监听，便于响应用户通过媒体会话控制方，例如播控中心，下发的播控命令。 在Session侧注册的监听分为固定播控命令和高级播控事件两种。 6.1 固定控制命令的监听 媒体会话提供方在注册相关固定播控命令事件监听时，监听的事件会在媒体会话控制方的getValidCommands()方法中体现，即媒体会话控制方会认为对应的方法有效，进而根据需要触发相应暂不使用时的事件。为了保证媒体会话控制方下发的播控命令可以被正常执行，媒体会话提供方请勿进行无逻辑的空实现监听。 Session侧的固定播控命令主要包括播放、暂停、上一首、下一首等基础操作命令，详细介绍请参见AVControlCommand 6.2 高级播控事件的监听 Session侧的可以注册的高级播控事件主要包括：
```typescript
import { avSession as AVSessionManager } from '@kit.AVSessionKit';
let context: Context = getContext(this);
async function setListenerForMesFromController() {
// 假设已经创建了一个session，如何创建session可以参考之前的案例
let type: AVSessionManager.AVSessionType = 'audio';
let session = await AVSessionManager.createAVSession(context, 'SESSION_NAME', type);
// 一般在监听器中会对播放器做相应逻辑处理
// 不要忘记处理完后需要通过set接口同步播放相关信息，参考上面的用例
session.on('play', () => {
console.info(`on play , do play task`);
// 如暂不支持该指令，请勿注册；或在注册后但暂不使用时，通过session.off('play')取消监听
// 处理完毕后，请使用SetAVPlayState上报播放状态
});
session.on('pause', () => {
console.info(`on pause , do pause task`);
// 如暂不支持该指令，请勿注册；或在注册后但暂不使用时，通过session.off('pause')取消监听
// 处理完毕后，请使用SetAVPlayState上报播放状态
});
session.on('stop', () => {
console.info(`on stop , do stop task`);
// 如暂不支持该指令，请勿注册；或在注册后但暂不使用时，通过session.off('stop')取消监听
// 处理完毕后，请使用SetAVPlayState上报播放状态
});
session.on('playNext', () => {
console.info(`on playNext , do playNext task`);
// 如暂不支持该指令，请勿注册；或在注册后但暂不使用时，通过session.off('playNext')取消监听
// 处理完毕后，请使用SetAVPlayState上报播放状态，使用SetAVMetadata上报媒体信息
});
session.on('playPrevious', () => {
console.info(`on playPrevious , do playPrevious task`);
// 如暂不支持该指令，请勿注册；或在注册后但暂不使用时，通过session.off('playPrevious')取消监听
// 处理完毕后，请使用SetAVPlayState上报播放状态，使用SetAVMetadata上报媒体信息
});
session.on('fastForward', () => {
console.info(`on fastForward , do fastForward task`);
// 如暂不支持该指令，请勿注册；或在注册后但暂不使用时，通过session.off('fastForward')取消监听
// 处理完毕后，请使用SetAVPlayState上报播放状态和播放position
});
session.on('rewind', () => {
console.info(`on rewind , do rewind task`);
// 如暂不支持该指令，请勿注册；或在注册后但暂不使用时，通过session.off('rewind')取消监听
// 处理完毕后，请使用SetAVPlayState上报播放状态和播放position
});
session.on('seek', (time) => {
console.info(`on seek , the seek time is ${time}`);
// 如暂不支持该指令，请勿注册；或在注册后但暂不使用时，通过session.off('seek')取消监听
// 处理完毕后，请使用SetAVPlayState上报播放状态和播放position
});
session.on('setSpeed', (speed) => {
console.info(`on setSpeed , the speed is ${speed}`);
// do some tasks ···
});
session.on('setLoopMode', (mode) => {
console.info(`on setLoopMode , the loop mode is ${mode}`);
// 如暂不支持该指令，请勿注册；或在注册后但暂不使用时，通过session.off('setLoopMode')取消监听
// 应用自定下一个模式，处理完毕后，请使用SetAVPlayState上报切换后的LoopMode
});
session.on('toggleFavorite', (assetId) => {
console.info(`on toggleFavorite , the target asset Id is ${assetId}`);
// 如暂不支持该指令，请勿注册；或在注册后但暂不使用时，通过session.off('toggleFavorite')取消监听
// 处理完毕后，请使用SetAVPlayState上报收藏结果isFavorite
});
}
```
7.  获取当前媒体会话自身的控制器，与媒体会话对应进行通信交互。
```typescript
import { avSession as AVSessionManager } from '@kit.AVSessionKit';
let context: Context = getContext(this);
async function createControllerFromSession() {
// 假设已经创建了一个session，如何创建session可以参考之前的案例
let type: AVSessionManager.AVSessionType = 'audio';
let session = await AVSessionManager.createAVSession(context, 'SESSION_NAME', type);
// 通过已有session获取一个controller对象
let controller = await session.getController();
// controller可以与原session对象进行基本的通信交互，比如下发播放命令
let avCommand: AVSessionManager.AVControlCommand = {command:'play'};
controller.sendControlCommand(avCommand);
// 或者做状态变更监听
controller.on('playbackStateChange', 'all', (state) => {
// do some things
});
// controller可以做的操作还有很多，具体可以参考媒体会话控制方相关的说明
}
```
8.  音视频应用在退出，并且不需要继续播放时，及时取消监听以及销毁媒体会话释放资源。 取消播控命令监听的示例代码如下所示 ： 销毁媒体会话示例代码如下所示：
```typescript
import { avSession as AVSessionManager } from '@kit.AVSessionKit';
let context: Context = getContext(this);
async function unregisterSessionListener() {
// 假设已经创建了一个session，如何创建session可以参考之前的案例
let type: AVSessionManager.AVSessionType = 'audio';
let session = await AVSessionManager.createAVSession(context, 'SESSION_NAME', type);
// 取消指定session下的相关监听
session.off('play');
session.off('pause');
session.off('stop');
session.off('playNext');
session.off('playPrevious');
session.off('skipToQueueItem');
session.off('handleKeyEvent');
session.off('outputDeviceChange');
session.off('commonCommand');
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/using-ohavsession-developer-V14
爬取时间: 2025-04-28 20:06:06
来源: Huawei Developer
OHAVSession系统提供的通过使用C API实现媒体会话提供方，从而在媒体会话控制方（例如播控中心）中展示媒体相关信息，及响应媒体会话控制方下发的播控命令。
使用入门
开发者要使用OHAVSession实现媒体会话，需要添加对应的头文件。
在 CMake 脚本中链接动态库
添加头文件
开发步骤及注意事项
开发者可以通过以下几个步骤在NDK接入本地会话。
1.  创建会话并激活媒体，需要传入会话类型AVSession_Type，自定义的TAG，以及应用的包名、ability名字。 AVSession_Type包含如下四种类型：
2.  应用内播放对应的媒体资源时，同步设置媒体元数据信息。 要设置元数据，要使用OH_AVMetadataBuilder构造具体的数据，最后生成一个 OH_AVMetadata。生成OH_AVMetadata后，使用OH_AVMetadata的各个功能接口进行资源的设置。 使用OH_AVMetadataBuilder构造元数据示例： 在不使用AVMetadta之后，开发者应该执行OH_AVMetadataBuilder_Destroy接口来销毁元数 据，且不要继续使用。
3.  跟随媒体播放状态的变化，及时更新媒体播放状态。 媒体播放状态，包含状态值、播放位置、播放速度、收藏状态等，可以按需使用对应的接口进行 设置。
4.  注册播控命令事件监听，便于响应用户通过媒体会话控制方，例如播控中心下发的播控命令。 媒体会话提供方在注册相关固定播控命令事件监听时，监听的事件会在媒体会话控制方的getValidCommands()方法中体现，即  体会话控制方会认为对应的方法有效，进而根据需要触发相应暂不使用时的事件。为了 保证媒体会话控制方下发的播控命令可以被正常执行，媒体会话提供方请勿进行无逻辑的空实现监听。 Session侧目前支持的播控命令包括： 相关回调接口如下：
5.  音视频应用在退出，并且不需要继续播放时，及时取消监听以及销毁媒体会话释放资源。示例代码如下所示 ：
| 接口 | 说明 |
| --- | --- |
| OH_AVSession_RegisterCommandCallback(OH_AVSession* avsession, AVSession_ControlCommand command, OH_AVSessionCallback_OnCommand callback, void* userData); | 注册通用播控的回调，支持：播 放、暂停、停止、上一首、下一首回调。 |
| OH_AVSession_RegisterForwardCallback(OH_AVSession* avsession, OH_AVSessionCallback_OnFastForward callback, void* userData); | 注册快进的回调。 |
| OH_AVSession_RegisterRewindCallback(OH_AVSession* avsession, OH_AVSessionCallback_OnRewind callback, void* userData); | 注册快退的回调。 |
| OH_AVSession_RegisterSeekCallback(OH_AVSession* avsession, OH_AVSessionCallback_OnSeek callback, void* userData); | 注册跳转的回调。 |
| OH_AVSession_RegisterToggleFavoriteCallback(OH_AVSession* avsession, OH_AVSessionCallback_OnToggleFavorite callback, void* userData) | 注册收藏的回调。 |

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/avsession-access-scene-V14
爬取时间: 2025-04-28 20:06:19
来源: Huawei Developer
音视频应用在实现音视频功能的同时，需要接入媒体会话即AVSession Kit，下文将提供一些典型的接入AVSession的展示和控制场景，方便开发者根据场景进行适配。
对于不同的场景，将会在系统的播控中心看到不同的UI呈现。同时，在不同的场景下，应用的接入处理也需要遵循不同的规范约束。
哪些场景下需要接入AVSession
AVSession会对后台的音频播放、VOIP通话做约束，所以通常来说，长音频应用、听书类应用、长视频应用、VOIP类应用等都需要接入AVSession。当应用在没有创建接入AVSession的情况下进行了上述业务，那么系统会在检测到应用后台时，停止对应的音频播放，静音通话声音，以达到约束应用行为的目的。这种约束，应用上架前在本地就可以验证。
对于其他使用到音频播放的应用，比如游戏，直播等场景，接入AVSession不是必选项，只是可选，取决于应用是否有后台播放的使用诉求。若应用需要后台播放，那么接入AVSession仍然是必须的，否则业务的正常功能会受到限制。
当应用需要实现后台播放等功能时，需要使用BackgroundTasks Kit（后台任务管理）的能力，申请对应的长时任务，避免进入挂起（Suspend）状态。
接入流程
应用接入AVSession流程分为如下几个步骤：
创建不同类型的会话
AVSession在构造方法中支持不同的类型参数，由AVSessionType定义，不同的类型代表了不同场景的控制能力，对于播控中心来说，会展示不同的控制模版。
-  audio类型，播控中心的控制样式为：收藏，上一首，播放/暂停，下一首，循环模式。
-  video类型，播控中心的控制样式为：快退，上一首，播放/暂停，下一首，快进。
-  voice_call类型，通话类型。
使用代码示例：
```typescript
import { avSession as AVSessionManager } from '@kit.AVSessionKit';
// 开始创建并激活媒体会话
// 创建session
let context: Context = getContext(this);
async function createSession() {
let type: AVSessionManager.AVSessionType = 'audio';
let session = await AVSessionManager.createAVSession(context,'SESSION_NAME', type);
// 激活接口要在元数据、控制命令注册完成之后再执行
await session.activate();
console.info(`session create done : sessionId : ${session.sessionId}`);
}
```
创建后台任务
当应用需要实现后台播放等功能时，需要使用BackgroundTasks Kit（后台任务管理）的能力，申请对应的长时任务，避免进入挂起（Suspend）状态。
对媒体类播放来说，需要申请AUDIO_PLAYBACK BackgroundMode的长时任务。
设置元数据
通用元数据
应用可以通过setAVMetadata把会话的一些元数据信息设置给系统，从而在播控中心界面进行展示，包括不限制：当前媒体的ID（assetId），上一首媒体的ID（previousAssetId），下一首媒体的ID（nextAssetId），标题（title），专辑作者（author），专辑名称（album），词作者（writer），媒体时长（duration）等。
```typescript
import { avSession as AVSessionManager } from '@kit.AVSessionKit';
import { BusinessError } from '@kit.BasicServicesKit';
let context: Context = getContext(this);
async function setSessionInfo() {
// 假设已经创建了一个session，如何创建session可以参考之前的案例
let session = await AVSessionManager.createAVSession(context, 'SESSION_NAME', 'audio');
// 设置必要的媒体信息
let metadata: AVSessionManager.AVMetadata = {
assetId: '0', // 由应用指定，用于标识应用媒体库里的媒体
title: 'TITLE',
mediaImage: 'IMAGE',
artist: 'ARTIST',
};
session.setAVMetadata(metadata).then(() => {
console.info(`SetAVMetadata successfully`);
}).catch((err: BusinessError) => {
console.error(`Failed to set AVMetadata. Code: ${err.code}, message: ${err.message}`);
});
}
```
歌词
对于长音频来说，播控中心提供了歌词的展示页面，对于应用来说，接入也比较简单，只需要把歌词内容设置给系统。播控中心会解析歌词内容，并根据播放进度进行同步的刷新。
```typescript
import { avSession as AVSessionManager } from '@kit.AVSessionKit';
import { BusinessError } from '@kit.BasicServicesKit';
let context: Context = getContext(this);
async function setListener() {
// 假设已经创建了一个session，如何创建session可以参考之前的案例
let type: AVSessionManager.AVSessionType = 'audio';
let session = await AVSessionManager.createAVSession(context, 'SESSION_NAME', type);
// 把歌词信息设置给AVSession
let metadata: AVSessionManager.AVMetadata = {
assetId: '0',
title: 'TITLE',
mediaImage: 'IMAGE',
// LRC中有两类元素：一种是时间标签+歌词，一种是ID标签。
// 例如：[00:25.44]xxx\r\n[00:26.44]xxx\r\n
lyric: "lrc格式歌词内容",
};
session.setAVMetadata(metadata).then(() => {
console.info(`SetAVMetadata successfully`);
}).catch((err: BusinessError) => {
console.error(`Failed to set AVMetadata. Code: ${err.code}, message: ${err.message}`);
});
}
```
历史歌单
针对音乐/听书类应用，播控中心提供一系列快捷播放能力，包括一键启动冷启动续播、以及历史歌单功能，其中歌单功能中支持显示的音频媒体内容有：音乐歌单、有声书专辑、播客专辑等。视频媒体内容、直播类媒体内容暂不支持歌单。应用在端侧注册并适配后台启动模式的播放意图，即可实现接入上述功能。接入后的体验自检，可以参考快捷播放。
开发者可参考下述示例，完成注册播放意图、设置歌单信息、实现意图启动播放。
注册播放意图
应用按照播放业务，选择PlayMusicList意图（音乐类应用）或者PlayAudio意图（听书类应用）其一注册。编辑对应的意图配置PROJECT_HOME/entry/src/main/resources/base/profile/insight_intent.json文件，实现播放意图注册。
注册示例如下：
```typescript
{
// 应用支持的意图列表
"insightIntents": [
{
// 意图名称
// 名称应当遵循意图框架规范，当前仅支持预置垂域意图，不允许自定义
// 应用内意图名称唯一，不允许出现相同的名称定义
"intentName": "PlayMusicList", // 音乐类PlayMusicList，听书或有声书类PlayAudio
// 意图所属的垂域
"domain": "MusicDomain", // 音乐类MusicDomain，听书或有声书类AudioDomain
// 意图版本号
// 插件引用意图时会校验该版本号，只有和插件定义的版本号一致才能正常调用
"intentVersion": "1.0.1",
// 意图调用代码逻辑入口
"srcEntry": "./ets/entryability/InsightIntentExecutorImpl.ets",
"uiAbility": {
// 意图所在module、ability，以及代码相对路径入口
"ability": "EntryAbility",
// UIAbility支持前后台两种执行模式
"executeMode": [
"background", // 播控一键冷启动、历史歌单功能需要应用支持意图后台启动
"foreground"
]
}
}
]
}
```
设置歌单信息
通过setAVMetadata接口设置当前播放的歌单（专辑）信息，歌单（专辑）信息由下面几个字段组成：
系统媒体信息根据应用上报实时刷新，若应用接入歌单功能，则确保在AVMetadata中一直携带歌单数据。
```typescript
import { avSession as AVSessionManager } from '@kit.AVSessionKit';
import { BusinessError } from '@kit.BasicServicesKit';
let context: Context = getContext(this);
async function setListener() {
// 假设已经创建了一个session，如何创建session可以参考之前的案例
let type: AVSessionManager.AVSessionType = 'audio';
let session = await AVSessionManager.createAVSession(context, 'SESSION_NAME', type);
// 将歌单信息设置给AVSession
let metadata: AVSessionManager.AVMetadata = {
// 下面内容均由应用设置
assetId: 'musicid123',
avQueueName: 'myQueue',
avQueueId: 'myQueue123',
avQueueImage: "PIXELMAP_OBJECT",
};
session.setAVMetadata(metadata).then(() => {
console.info(`SetAVMetadata successfully`);
}).catch((err: BusinessError) => {
console.error(`Failed to set AVMetadata. Code: ${err.code}, message: ${err.message}`);
});
// 上报播放状态，参考播放状态
}
```
实现意图启动播放
当应用接入歌单功能后，用户在系统播控中心界面可以播放应用的歌单，也可以在应用进程释放后，通过系统播控中心再次冷启动应用后台播放。
```typescript
import { insightIntent, InsightIntentExecutor } from '@kit.AbilityKit';
import { window } from '@kit.ArkUI';
import { BusinessError } from '@kit.BasicServicesKit';
/**
* 意图调用样例
*/
export default class InsightIntentExecutorImpl extends InsightIntentExecutor {
/**
* override 执行后台启动UIAbility意图
*
* @param intentName 意图名称
* @param intentParam 意图参数
* @returns 意图调用结果
*/
async onExecuteInUIAbilityBackgroundMode(intentName: string, intentParam: Record<string, Object>):
Promise<insightIntent.ExecuteResult> {
// 根据意图名称分发处理逻辑
switch (intentName) {
case 'PlayMusicList':
// PlayMusicList参照如下解析方式
let entityId: string = (intentParam.items as Array<object>)?.[0]?.['entityId'];
return this.playFunc(entityId);
case 'PlayAudio':
// PlayAudio参照如下解析方式
let data = intentParam as Record<string, string>;
return this.playFunc(data.entityId);
default:
break;
}
return Promise.resolve({
code: -1,
result: {
message: 'unknown intent'
}
} as insightIntent.ExecuteResult)
}
/**
* 实现调用播放功能
*
* @param entityId 播放内容id
*/
private playFunc(entityId: string): Promise<insightIntent.ExecuteResult> {
// entityId 不为空，表示用户指定内容（歌单/专辑）播放
// entityId 为空（entityId.length == 0），由应用自行决定播放内容，续播历史内容或者是推荐内容
// 此时是后台拉起应用播放，除了初始化播放资源，还需要设置AVMetadata，注册播控控制回调，并申请播音长时任务
// TODO 实现具体的播放业务
return Promise.resolve({
code: 0,
result: {
message: 'Intent execute succeed'
}
} as insightIntent.ExecuteResult)
}
}
```
媒体资源金标
对于长音频，播控中心提供了媒体资源金标的展示，媒体资源金标又可称为应用媒体音频音源的标识，目前暂时只支持展示AudioVivid标识。
对于应用来说，接入只需要在AVMetadata中通知系统，当前播放音频的音源标识，播控就会同步展示。
```typescript
import { avSession as AVSessionManager } from '@kit.AVSessionKit';
import { BusinessError } from '@kit.BasicServicesKit';
let context: Context = getContext(this);
async function setListener() {
// 假设已经创建了一个session，如何创建session可以参考之前的案例
let type: AVSessionManager.AVSessionType = 'audio';
let session = await AVSessionManager.createAVSession(context, 'SESSION_NAME', type);
// 把媒体音源信息设置给AVSession
let metadata: AVSessionManager.AVMetadata = {
assetId: '0',
title: 'TITLE',
mediaImage: 'IMAGE',
// 标识该媒体音源是AudioVivid
displayTags: AVSessionManager.DisplayTag.TAG_AUDIO_VIVID,
};
session.setAVMetadata(metadata).then(() => {
console.info(`SetAVMetadata successfully`);
}).catch((err: BusinessError) => {
console.error(`Failed to set AVMetadata. Code: ${err.code}, message: ${err.message}`);
});
}
```
设置播放状态
通用播放状态
应用可以通过setAVPlaybackState。把当前的播放状态设置给系统，以在播控中心界面进行展示。
播放状态一般是在资源播放后会进行变化的内容，包括：当前媒体的播放状态（state）、播放位置（position）、播放倍速（speed）、缓冲时间（bufferedTime）、循环模式（loopMode）、是否收藏（isFavorite）、正在播放的媒体Id（activeItemId）、自定义媒体数据（extras）等。
```typescript
import { avSession as AVSessionManager } from '@kit.AVSessionKit';
import { BusinessError } from '@kit.BasicServicesKit';
let context: Context = getContext(this);
async function setSessionInfo() {
// 假设已经创建了一个session，如何创建session可以参考之前的案例
let session = await AVSessionManager.createAVSession(context, 'SESSION_NAME', 'audio');
// 播放器逻辑··· 引发媒体信息与播放状态的变更
// 简单设置一个播放状态 - 暂停 未收藏
let playbackState: AVSessionManager.AVPlaybackState = {
state:AVSessionManager.PlaybackState.PLAYBACK_STATE_PAUSE,
isFavorite:false
};
session.setAVPlaybackState(playbackState, (err: BusinessError) => {
if (err) {
console.error(`Failed to set AVPlaybackState. Code: ${err.code}, message: ${err.message}`);
} else {
console.info(`SetAVPlaybackState successfully`);
}
});
}
```
进度条
应用如果支持在播控中心展示进度，那么在媒体资源播放中，需要设置资源的时长、播放状态（暂停、播放）、播放位置、倍速，播控中心会使用这些信息进行进度的展示：
```typescript
import { avSession as AVSessionManager } from '@kit.AVSessionKit';
import { BusinessError } from '@kit.BasicServicesKit';
let context: Context = getContext(this);
async function setListener() {
// 假设已经创建了一个session，如何创建session可以参考之前的案例
let type: AVSessionManager.AVSessionType = 'audio';
let session = await AVSessionManager.createAVSession(context, 'SESSION_NAME', type);
// 设置媒体资源时长
let metadata: AVSessionManager.AVMetadata = {
assetId: '0',
title: 'TITLE',
mediaImage: 'IMAGE',
duration: 23000, // 资源的时长，以ms为单位
};
session.setAVMetadata(metadata).then(() => {
console.info(`SetAVMetadata successfully`);
}).catch((err: BusinessError) => {
console.error(`Failed to set AVMetadata. Code: ${err.code}, message: ${err.message}`);
});
// 设置状态： 播放状态，进度位置，播放倍速，缓存的时间
let playbackState: AVSessionManager.AVPlaybackState = {
state: AVSessionManager.PlaybackState.PLAYBACK_STATE_PLAY, // 播放状态
position: {
elapsedTime: 1000, // 已经播放的位置，以ms为单位
updateTime: new Date().getTime(), // 应用更新当前位置时的时间戳，以ms为单位
},
speed: 1.0, // 可选，默认是1.0，播放的倍速，按照应用内支持的speed进行设置，系统不做校验
bufferedTime: 14000, // 可选，资源缓存的时间，以ms为单位
};
session.setAVPlaybackState(playbackState, (err) => {
if (err) {
console.error(`Failed to set AVPlaybackState. Code: ${err.code}, message: ${err.message}`);
} else {
console.info(`SetAVPlaybackState successfully`);
}
});
}
```
系统的播控中心会根据应用设置的信息自行进行播放进度的计算，而不需要应用实时更新播放进度；
但是应用需要如下状态发生变化的时候，再更新AVPlaybackState，否则系统会发生计算错误：
应用在真实播放开始时，再上报进度起始position；若播放存在buffer状态，可以先上报播放状态为AVSessionManager.PlaybackState.PLAYBACK_STATE_BUFFERING，来通知系统不刷新进度。
关于进度条有一些特殊情况需要处理：
1.  歌曲支持试听 （1）应用不需要设置完整的歌曲时长，则只需要设置歌曲的试听时长。当应用仅设置歌曲的试听时长而不是完整时长，用户在播控中心触发进度控制时，应用收到的时长也是VIP试听时长内的相对时间戳位置，而不是完整歌曲的绝对时间戳位置，应用需要重新计算歌曲从零开始的绝对时间戳进行实际响应处理。 （2）如果应用设置完整歌曲时长，但需要系统支持试听片段，也可以在播放时上报起始进度position，当收到的seek指令超过试听片段时，上报试听截止position，系统播控的进度会跟随回弹。
2.  歌曲不支持试听 如果歌曲不支持试听，那么理论上应用内也不支持播放，这时可以把 duration 设置为 -1，以通知系统不显示实际的时长。
3.  广告等内容的时长设置 对于有前贴广告、后贴广告的资源来说，建议这么处理：
注册控制命令
应用接入AVSession，可以通过注册不同的控制命令来实现播控中心界面上的控制操作，即通过on接口注册不同的控制命令参数，即可实现对应的功能。
具体的接口参考接口注册。
创建AVSession后，请先注册应用支持的控制命令，再激活 Session
媒体资源支持的控制命令列表：
| 控制命令 | 功能说明 |
| --- | --- |
| play | 播放命令。 |
| pause | 暂停命令。 |
| stop | 停止命令。 |
| playNext | 播放下一首命令。 |
| playPrevious | 播放上一首命令。 |
| fastForward | 快进命令。 |
| rewind | 快退命令。 |
| playFromAssetId | 根据某个资源id进行播放命令。 |
| seek | 跳转命令。 |
| setSpeed | 设置播放速率命令。 |
| setLoopMode | 设置循环模式命令。 |
| toggleFavorite | 设置是否收藏命令。 |
| skipToQueueItem | 设置播放列表其中某项被选中播放的命令。 |
| handleKeyEvent | 设置按键事件的命令。 |
| commonCommand | 设置自定义控制命令。 |
通话类应用支持的控制：
| 控制命令 | 功能说明 |
| --- | --- |
| answer | 接听电话的命令。 |
| hangUp | 通话挂断的命令。 |
| toggleCallMute | 通话静音或解除静音的命令。 |
不支持命令的处理
系统支持的控制命令对于不支持的控制，比如应用不支持“上一首”的命令处理，只需要使用off 接口注销对应的控制命令，系统的播控中心会相应的对该控制界面进行置灰处理，以明确告知用户此控制命令不支持。
```typescript
import { avSession as AVSessionManager } from '@kit.AVSessionKit';
let context: Context = getContext(this);
async function unregisterSessionListener() {
// 假设已经创建了一个session，如何创建session可以参考之前的案例
let type: AVSessionManager.AVSessionType = 'audio';
let session = await AVSessionManager.createAVSession(context, 'SESSION_NAME', type);
// 取消指定session下的相关监听
session.off('play');
session.off('pause');
session.off('stop');
session.off('playNext');
session.off('playPrevious');
}
```
快进快退
系统支持三种快进快退的时长，应用可以通过接口进行设置；同时注册快进快退的回调命令，以响应控制。
```typescript
import { avSession as AVSessionManager } from '@kit.AVSessionKit';
import { BusinessError } from '@kit.BasicServicesKit';
let context: Context = getContext(this);
async function unregisterSessionListener() {
// 假设已经创建了一个session，如何创建session可以参考之前的案例
let type: AVSessionManager.AVSessionType = 'audio';
let session = await AVSessionManager.createAVSession(context, 'SESSION_NAME', type);
// 设置支持的快进快退的时长设置给AVSession
let metadata: AVSessionManager.AVMetadata = {
assetId: '0', // 由应用指定，用于标识应用媒体库里的媒体
title: 'TITLE',
mediaImage: 'IMAGE',
skipIntervals: AVSessionManager.SkipIntervals.SECONDS_10,
};
session.setAVMetadata(metadata).then(() => {
console.info(`SetAVMetadata successfully`);
}).catch((err: BusinessError) => {
console.error(`Failed to set AVMetadata. Code: ${err.code}, message: ${err.message}`);
});
session.on('fastForward', (time ?: number) => {
console.info(`on fastForward , do fastForward task`);
// do some tasks ···
});
session.on('rewind', (time ?: number) => {
console.info(`on rewind , do rewind task`);
// do some tasks ···
});
}
```
收藏
音乐类应用实现收藏功能，那么需要注册收藏的控制响应on('toggleFavorite')。
```typescript
import { avSession as AVSessionManager } from '@kit.AVSessionKit';
import { BusinessError } from '@kit.BasicServicesKit';
let context: Context = getContext(this);
async function setListener() {
// 假设已经创建了一个session，如何创建session可以参考之前的案例
let type: AVSessionManager.AVSessionType = 'audio';
let session = await AVSessionManager.createAVSession(context, 'SESSION_NAME', type);
session.on('toggleFavorite', (assetId) => {
console.info(`on toggleFavorite `);
// 应用收到收藏命令，进行收藏处理
// 应用内完成或者取消收藏，把新的收藏状态设置给AVSession
let playbackState: AVSessionManager.AVPlaybackState = {
isFavorite:true,
};
session.setAVPlaybackState(playbackState).then(() => {
console.info(`SetAVPlaybackState successfully`);
}).catch((err: BusinessError) => {
console.info(`SetAVPlaybackState BusinessError: code: ${err.code}, message: ${err.message}`);
});
});
}
```
循环模式
针对音乐类应用，系统的播控中心界面会默认展示循环模式的控制操作，目前系统支持四种固定的循环模式控制，参考:LoopMode。
播控中心支持固定的四种循环模式的切换，即： 随机播放、顺序播放、单曲循环、列表循环。应用收到循环模式切换的指令并切换后，需要向系统上报切换后的LoopMode。
若应用内支持的LoopMode不在系统固定的四个循环模式内，需要选择四个固定循环模式其一向系统上报，由应用自定。
实现参考：
```typescript
import { avSession as AVSessionManager } from '@kit.AVSessionKit';
import { BusinessError } from '@kit.BasicServicesKit';
let context: Context = getContext(this);
async function setListener() {
// 假设已经创建了一个session，如何创建session可以参考之前的案例
let type: AVSessionManager.AVSessionType = 'audio';
let session = await AVSessionManager.createAVSession(context, 'SESSION_NAME', type);
// 应用启动时/内部切换循环模式，需要把应用内的当前的循环模式设置给AVSession
let playBackState: AVSessionManager.AVPlaybackState = {
loopMode: AVSessionManager.LoopMode.LOOP_MODE_SINGLE,
};
session.setAVPlaybackState(playBackState).then(() => {
console.info(`set AVPlaybackState successfully`);
}).catch((err: BusinessError) => {
console.error(`Failed to set AVPlaybackState. Code: ${err.code}, message: ${err.message}`);
});
// 应用注册循环模式的控制监听
session.on('setLoopMode', (mode) => {
console.info(`on setLoopMode ${mode}`);
// 应用收到设置循环模式的指令后，应用自定下一个模式，切换完毕后通过AVPlaybackState上报切换后的LoopMode
let playBackState: AVSessionManager.AVPlaybackState = {
loopMode: AVSessionManager.LoopMode.LOOP_MODE_SINGLE,
};
session.setAVPlaybackState(playBackState).then(() => {
console.info(`set AVPlaybackState successfully`);
}).catch((err: BusinessError) => {
console.error(`Failed to set AVPlaybackState. Code: ${err.code}, message: ${err.message}`);
});
});
}
```
进度控制
应用如果支持进度显示，进一步也可以支持进度控制。应用需要响应seek的控制命令，那么当用户在播控中心的界面上进行拖动操作时，应用就会收到对应的回调。参考实现：
```typescript
import { avSession as AVSessionManager } from '@kit.AVSessionKit';
let context: Context = getContext(this);
async function setListener() {
// 假设已经创建了一个session，如何创建session可以参考之前的案例
let type: AVSessionManager.AVSessionType = 'audio';
let session = await AVSessionManager.createAVSession(context, 'SESSION_NAME', type);
session.on('seek', (position: number) => {
console.info(`on seek , the time is ${JSON.stringify(position)}`);
// 由于应用内seek可能会触发较长的缓冲等待，可以先把状态设置为 Buffering
let playbackState: AVSessionManager.AVPlaybackState = {
state: AVSessionManager.PlaybackState.PLAYBACK_STATE_BUFFERING, // 缓冲状态
};
session.setAVPlaybackState(playbackState, (err) => {
if (err) {
console.error(`Failed to set AVPlaybackState. Code: ${err.code}, message: ${err.message}`);
} else {
console.info(`SetAVPlaybackState successfully`);
}
});
// 应用响应seek命令，使用应用内播放器完成seek实现
// 应用内更新新的位置后，也需要同步更新状态给系统
playbackState.state = AVSessionManager.PlaybackState.PLAYBACK_STATE_PLAY; // 播放状态
playbackState.position = {
elapsedTime: position, // 已经播放的位置，以ms为单位
updateTime: new Date().getTime(), // 应用更新当前位置的时间戳，以ms为单位
}
session.setAVPlaybackState(playbackState, (err) => {
if (err) {
console.error(`Failed to set AVPlaybackState. Code: ${err.code}, message: ${err.message}`);
} else {
console.info(`SetAVPlaybackState successfully`);
}
});
});
}
```
适配媒体通知
当前系统不直接向应用提供主动发送媒体控制通知的接口，那么当应用正确接入媒体播控中心并进入播放状态时，系统会自动发送通知，同时在通知和锁屏界面进行展示。
通知中心、锁屏下的播控卡片的展示，由系统进行发送，并控制相应的生命周期。
适配蓝牙按键与有线按键事件
当前系统不直接向应用提供监听多模按键事件的接口，应用如需要监听蓝牙与有线耳机的媒体按键事件，可以通过注册AVSession的控制指令来实现。AVSession提供了如下两种实现方式：
-  方式一（推荐使用）： 按照应用业务需求，正确接入媒体播控中心，注册需要的控制指令并实现对应的功能。AVSession会监听多模按键事件，将其转换为AVSession的控制指令发送回应用。应用无须区分不同的按键事件，按照AVSession的回调处理即可。按照此方式接入播放暂停，也等同于适配了蓝牙耳机的佩戴检测，在双耳佩戴与摘下时也会收到如下播放暂停控制指令。目前支持转换的AVSession控制指令如下：
```typescript
import { avSession as AVSessionManager } from '@kit.AVSessionKit';
let context: Context = getContext(this);
async function setListenerForMesFromController() {
let type: AVSessionManager.AVSessionType = 'audio';
let session = await AVSessionManager.createAVSession(context, 'SESSION_NAME', type);
// 设置必要的媒体信息，务必设置，否则接收不到控制事件
let metadata: AVSessionManager.AVMetadata = {
assetId: '0', // 由应用指定，用于标识应用媒体库里的媒体
title: 'TITLE',
mediaImage: 'IMAGE',
artist: 'ARTIST'
};
session.setAVMetadata(metadata).then(() => {
console.info(`SetAVMetadata successfully`);
}).catch((err: BusinessError) => {
console.error(`Failed to set AVMetadata. Code: ${err.code}, message: ${err.message}`);
});
// 一般在监听器中会对播放器做相应逻辑处理
// 不要忘记处理完后需要通过set接口同步播放相关信息，参考上面的用例
session.on('play', () => {
console.info(`on play , do play task`);
// 如暂不支持该指令，请勿注册；或在注册后但暂不使用时，通过session.off('play')取消监听
// 处理完毕后，请使用SetAVPlayState上报播放状态
});
session.on('pause', () => {
console.info(`on pause , do pause task`);
// 如暂不支持该指令，请勿注册；或在注册后但暂不使用时，通过session.off('pause')取消监听
// 处理完毕后，请使用SetAVPlayState上报播放状态
});
}
```
-  方式二： 通过AVSession注册HandleMediaKeyEvent指令。该回调接口会直接转发媒体按键事件KeyEvent。应用需要自行识别按键事件的类型，并响应事件实现对应的功能。目前支持转发的按键事件类型如下：
```typescript
import { avSession as AVSessionManager } from '@kit.AVSessionKit';
let context: Context = getContext(this);
async function setListenerForMesFromController() {
let type: AVSessionManager.AVSessionType = 'audio';
let session = await AVSessionManager.createAVSession(context, 'SESSION_NAME', type);
// 设置必要的媒体信息，务必设置，否则接收不到按键事件
let metadata: AVSessionManager.AVMetadata = {
assetId: '0', // 由应用指定，用于标识应用媒体库里的媒体
title: 'TITLE',
mediaImage: 'IMAGE',
artist: 'ARTIST'
};
session.setAVMetadata(metadata).then(() => {
console.info(`SetAVMetadata successfully`);
}).catch((err: BusinessError) => {
console.error(`Failed to set AVMetadata. Code: ${err.code}, message: ${err.message}`);
});
session.on('handleKeyEvent', (event) => {
// 解析keycode，应用需要根据keycode对播放器做相应逻辑处理
console.info(`on handleKeyEvent, keyCode=${event.key.code}`);
});
}
```
| 控制命令 | 功能说明 |
| --- | --- |
| play | 播放命令。 |
| pause | 暂停命令。 |
| stop | 停止命令。 |
| playNext | 播放下一首命令。 |
| playPrevious | 播放上一首命令。 |
| fastForward | 快进命令。 |
| rewind | 快退命令。 |
| 按键类型(KeyCode) | 功能说明 |
| --- | --- |
| KEYCODE_MEDIA_PLAY_PAUSE | 多媒体键：播放/暂停 |
| KEYCODE_MEDIA_STOP | 多媒体键：停止 |
| KEYCODE_MEDIA_NEXT | 多媒体键：下一首 |
| KEYCODE_MEDIA_PREVIOUS | 多媒体键：上一首 |
| KEYCODE_MEDIA_REWIND | 多媒体键：快退 |
| KEYCODE_MEDIA_FAST_FORWARD | 多媒体键：快进 |
| KEYCODE_MEDIA_PLAY | 多媒体键：播放 |
| KEYCODE_MEDIA_PAUSE | 多媒体键：暂停 |
示例代码

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/distributed-avsession-V14
爬取时间: 2025-04-28 20:06:33
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/distributed-playback-V14
爬取时间: 2025-04-28 20:06:47
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/distributed-playback-overview-V14
爬取时间: 2025-04-28 20:07:00
来源: Huawei Developer
使用媒体播控，可以简单高效地将音视频投放到其他HarmonyOS设备上播放，如在手机上播放的音视频，可以投到2in1设备上继续播放。
HarmonyOS提供了统一的应用内音视频投播功能设计，通过使用系统提供的投播组件和接口，应用只需要设置对应的资源信息、监听投播中的相关状态，以及应用主动控制的行为（如：播放、暂停）。其他动作包括图标切换、设备的发现、连接、认证等，均由系统完成。
基本概念
-  媒体会话（AVSession） 音视频管控服务，用于对系统中所有音视频行为进行统一的管理。 本地播放时，应用需要向媒体会话提供播放的媒体信息（如正在播放的歌曲、歌曲的播放状态等），并接收和响应播控中心发出的控制命令（如暂停、下一首等）。具体请参考本地媒体会话。 投播时，通过AVSession，应用可以进行投播能力的设置和查询，并创建投播控制器。 应用可以在启动内容显示（比如：视频播放）时，获取支持投播的扩展屏设备并注册监听，当存在扩展屏时，可在扩展屏上全屏绘制要投播的内容。
-  投播组件（AVCastPicker） 系统级的投播组件，可嵌入应用界面的UI组件。当用户点击该组件后，系统将进行设备发现、连接、认证等流程，应用仅需要通过接口获取投播中相关的回调信息。
-  投播控制器（AVCastController） 在投播后，由应用发起的用于控制远端播放的接口，包括播放、暂停、调节音量、设置播放模式、设置播放速度等能力。
-  后台长时任务 应用实现后台播放，需申请后台长时任务，避免应用在投播后被系统后台清理或冻结。具体参考长时任务。
运作机制
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165935.07552138254088579352054054005082:50001231000000:2800:331E0AF597F9998507B46781796F1EEC42003EA55F50E41342AF13DB4A90685D.png)
-  用户在应用界面上点击AVCastPicker组件，触发系统发现可用于投播的设备。用户在设备列表中选择对应设备后，系统连接对应设备。应用无需关注设备的发现连接过程，仅需关注设备在远端是否可用。 应用需要接入AVSession，才可以使用系统提供的统一投播能力，由系统进行设备发现和管理。
-  应用通过AVSession监听设备的连接情况，监听到设备已连接后，可通过AVSession获取一个AVCastController对象用于发送控制命令（如播放、暂停、下一首等）。 应用在进入远端投播时，应停止本地的播放器，避免本端和远端设备同时播放的情况。同时，建议应用重新绘制应用界面，比如界面变更为一个遥控器，来控制远端播放。
-  在本端（包括应用内和播控中心）控制播放时，控制命令将通过AVCastController发送，系统将完成数据传输和信息同步，然后更新远端系统预置播放器的状态。
-  用户同样可以在远端直接控制播放，会直接修改远端播放器的状态。
-  当远端播放器状态变更后，会触发回调，将状态信息返回到本端。应用可以通过AVCastController监听到远端播放器的状态变化。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/distributed-playback-guide-V14
爬取时间: 2025-04-28 20:07:14
来源: Huawei Developer
系统提供了音视频发声设备统一投播组件AVCastPicker，作为播控中心系统级设备切换、投播能力选择入口。应用通过接入统一投播组件，可以实现在应用内及系统播控中心，将音视频资源通过Cast+协议/DLNA协议投播到远端设备。
通过本开发指导，完成一次音视频跨设备投播。
约束与限制
需同时满足以下条件，才能使用该功能：
-  本端设备：HarmonyOS NEXT Developer Preview0及以上版本的手机设备 远端设备：HarmonyOS NEXT Developer Preview0及以上版本的2in1设备、HarmonyOS3.1及以上版本的华为智慧屏、支持标准DLNA协议的设备
接口说明
在开发具体功能前，请先查阅参考文档，获取详细的接口说明。
-  AVCastController由系统获取并返回，在设备连接成功后获取，在设备断开后不能继续使用，否则会抛出异常。 支持在线DRM视频资源投播能力，需注册DRM许可证请求回调函数，获取许可证后，调用处理许可证响应函数。 接口 说明 getAVCastController(callback: AsyncCallback<AVCastController>): void 获取远端投播时的控制接口。 on(type: 'outputDeviceChange', callback: (state: ConnectionState, device: OutputDeviceInfo) => void): void 注册设备变化的回调，同时包含了设备的连接状态。 sendControlCommand(command: AVCastControlCommand, callback: AsyncCallback<void>): void 投播会话的控制接口，用于进行投播中的各种播控指令。 setAVMetadata(data: AVMetadata, callback: AsyncCallback<void>): void 配置媒体信息，包括ID，标题，作者以及DRM类型等。 prepare(item: AVQueueItem, callback: AsyncCallback<void>): void 准备播放，进行资源加载和缓冲，不会触发真正的播放。 start(item: AVQueueItem, callback: AsyncCallback<void>): void 开始播放媒体资源。 processMediaKeyResponse(assetId: string, response: Uint8Array): Promise<void> 提供DRM资源所需的秘钥。 on(type: 'keyRequest', callback: KeyRequestCallback): void 注册DRM秘钥请求的回调。 on(type: 'playbackStateChange', filter: Array<keyof AVPlaybackState> | 'all', callback: (state: AVPlaybackState) => void): void 注册播放状态变化的回调。 on(type: 'mediaItemChange', callback: Callback<AVQueueItem>): void 注册当前播放内容更新的回调，返回当前播放的内容的信息。
| 接口  | 说明  |
| --- | --- |
| getAVCastController(callback: AsyncCallback<AVCastController>): void  | 获取远端投播时的控制接口。  |
| on(type: 'outputDeviceChange', callback: (state: ConnectionState, device: OutputDeviceInfo) => void): void  | 注册设备变化的回调，同时包含了设备的连接状态。  |
| sendControlCommand(command: AVCastControlCommand, callback: AsyncCallback<void>): void  | 投播会话的控制接口，用于进行投播中的各种播控指令。  |
| setAVMetadata(data: AVMetadata, callback: AsyncCallback<void>): void  | 配置媒体信息，包括ID，标题，作者以及DRM类型等。  |
| prepare(item: AVQueueItem, callback: AsyncCallback<void>): void  | 准备播放，进行资源加载和缓冲，不会触发真正的播放。  |
| start(item: AVQueueItem, callback: AsyncCallback<void>): void  | 开始播放媒体资源。  |
| processMediaKeyResponse(assetId: string, response: Uint8Array): Promise<void>  | 提供DRM资源所需的秘钥。  |
| on(type: 'keyRequest', callback: KeyRequestCallback): void  | 注册DRM秘钥请求的回调。  |
| on(type: 'playbackStateChange', filter: Array<keyof AVPlaybackState> | 'all', callback: (state: AVPlaybackState) => void): void  | 注册播放状态变化的回调。  |
| on(type: 'mediaItemChange', callback: Callback<AVQueueItem>): void  | 注册当前播放内容更新的回调，返回当前播放的内容的信息。  |
开发步骤
1.  通过AVSessionManager创建并激活媒体会话。 示例中的context的获取方式请参见获取UIAbility的上下文信息。
```typescript
import  { avSession }  from '@kit.AVSessionKit'; // 导入AVSession模块
// 声明全局的session对象，此写法是加在class类外的声明，如果需要在class类内申明全局变量，需要去掉 export let
export let session: avSession.AVSession;
// 创建session
async createSession(context: Context) {
// 创建的AVSession在基础播控与投播场景可以共用
session = await avSession.createAVSession(context, 'video_test', 'video'); // 'audio'代表音频应用，'video'代表视频应用
await session.activate();
// 请按照如下参数设置，告知系统应用当前支持投播，才能成功投播。
session.setExtras({
requireAbilityList: ['url-cast'],
});
console.info(`Session created. sessionId: ${session.sessionId}`);
}
```
2.  注意，投播后，应用播放器切换上下集时，也可以通过filter参数控制系统播控中心是否显示可投播设备列表，filter参数设置为0，播控会识别为不支持投播，隐藏可投播设备显示。避免用户从播控中心投播，应用资源不可用。
3.  注意，投播后，应用播放器切换上下集时，也可以通过filter参数控制系统播控中心是否显示可投播设备列表，filter参数设置为0，播控会识别为不支持投播，隐藏可投播设备显示。避免用户从播控中心投播，应用资源不可用。
```typescript
import { AVCastPicker, AVCastPickerState } from '@kit.AVSessionKit';
// 应用可以通过onStateChange接口监听组件显示/消失状态，当组件显示时建议不要销毁AVCastPicker的显示；当组件消失时，再根据业务隐藏AVCastPicker。
private onStateChange(state: AVCastPickerState) {
if (state == AVCastPickerState.STATE_APPEARING) {
console.log('The picker starts showing.');
} else if (state == AVCastPickerState.STATE_DISAPPEARING) {
console.log('The picker finishes presenting.');
}
}
// 创建组件，并设置大小
build() {
Row() {
Column() {
AVCastPicker({
normalColor: Color.Red,
onStateChange: this.onStateChange
})
.width('40vp')
.height('40vp')
.border({ width: 1, color: Color.Red })
}.height('50%')
}.width('50%')
}
```
4.  下面代码展示设备连接成功后的相应的处理
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
import { avSession } from '@kit.AVSessionKit';
import { session } from './xxx'; // session声明的文件
castController: avSession.AVCastController | undefined = undefined;
getAVCastController() {
// 如支持投播，可使用下面接口监听设备连接状态的变化
session.on('outputDeviceChange', async (connectState: avSession.ConnectionState,
device: avSession.OutputDeviceInfo) => {
// 可以通过当前设备及设备连接状态来更新应用内播放界面的显示
let currentDevice: avSession.DeviceInfo = device?.devices?.[0];
if (currentDevice.castCategory === avSession.AVCastCategory.CATEGORY_REMOTE && connectState === avSession.ConnectionState.STATE_CONNECTED) { // 设备连接成功
console.info(`Device connected: ${device}`);
this.castController = await session.getAVCastController();
console.info('Succeeded in getting a cast controller');
// 查询当前播放的状态
let avPlaybackState = await this.castController?.getAVPlaybackState();
console.info(`Succeeded in AVPlaybackState resource obtained: ${avPlaybackState}`);
// 监听播放状态的变化
this.castController?.on('playbackStateChange', 'all', (state: avSession.AVPlaybackState) => {
console.info(`Succeeded in Playback state changed: ${state}`);
});
if (currentDevice.supportedProtocols === avSession.ProtocolType.TYPE_CAST_PLUS_STREAM) {
// 此设备支持cast+投播协议
} else if (currentDevice.supportedProtocols === avSession.ProtocolType.TYPE_DLNA) {
// 此设备支持DLNA投播协议
}
// 此设备支持chinaDRM，监听许可证请求事件，也可在发起DRM资源投播前监听。
if (currentDevice.supportedDrmCapabilities?.includes('3d5e6d35-9b9a-41e8-b843-dd3c6e72c42c')) {
this.castController?.on('keyRequest', this.keyRequestCallback);
}
}
})
}
// 处理DRM许可证请求事件
private keyRequestCallback: avSession.KeyRequestCallback = async (assetId: string, requestData: Uint8Array) => {
// 根据assetId获取对应的DRM url
let drmUrl: string = 'http://license.xxx.xxx.com:8080/drmproxy/getLicense';
// 从服务器获取许可证，具体实现可参考附录。
let licenseResponseData = await this.getLicense(drmUrl, requestData);
try {
// 处理DRM许可证响应
await this.castController?.processMediaKeyResponse(assetId, licenseResponseData);
} catch (error) {
console.error(`Failed to process the response corresponding to the media key. Error: ${error}`);
}
}
```
5.  下面代码示例中的url仅作示意使用，开发者需根据实际情况，确认资源有效性并设置：
```typescript
playItem() {
// 设置播放参数，开始播放
let playItem : avSession.AVQueueItem = {
itemId: 0,
description: {
assetId: 'VIDEO-1',
title: 'ExampleTitle',
artist: 'ExampleArtist',
// 网络资源投播，设置mediaUri; 本地资源投播，将本地文件打开后，相关的文件描述符设置到fdSrc
mediaUri: 'https://xxx.xxx.com/example.mp4',
// 该字段大写，音频'AUDIO'，视频'VIDEO'
mediaType: 'VIDEO',
mediaSize: 1000,
//startPosition为投播当前进度，设置该字段可将本机播放进度同步到远端
startPosition: 0,
// 投播资源播放时长，设置该字段可将本机播放时长同步到远端显示
duration: 100000,
albumCoverUri: 'https://www.example.jpeg',
albumTitle: '《ExampleAlbum》',
appName: 'ExampleApp',
// DRM资源，需要配置支持的DRM类型, 以chinaDRM为例。
drmScheme: '3d5e6d35-9b9a-41e8-b843-dd3c6e72c42c',
}
};
// 准备播放，这个不会触发真正的播放，会进行加载和缓冲
this.castController?.prepare(playItem, () => {
console.info('Preparation done');
// 启动播放，真正触发对端播放。请在Prepare成功后再调用start。
this.castController?.start(playItem, () => {
console.info('Playback started');
});
});
}
```
```typescript
playControl() {
// 记录从avsession获取的远端控制器this.castController
// 应用向对端设备下发播放命令
let avCommand: avSession.AVCastControlCommand = {command:'play'};
this.castController?.sendControlCommand(avCommand);
// 应用向对端设备下发暂停命令
avCommand = {command:'pause'};
this.castController?.sendControlCommand(avCommand);
// 应用调节对端设备音量
avCommand = {
command: 'setVolume',
parameter: volume
};
this.castController?.sendControlCommand(avCommand);
// 应用调节对端设备进度
avCommand = {
command: 'seek',
parameter: position
};
this.castController?.sendControlCommand(avCommand);
// 更多控制指令请参考AVCastControlCommand
}
controlListener() {
// 应用监听对端或播控中心上下一首/上下一集切换
this.castController?.on('playPrevious', () => {
console.info('PlayPrevious done');
});
this.castController?.on('playNext', () => {
console.info('PlayNext done');
});
// 应用监听对端或播控中心播放状态、实时进度和音量变化事件
this.castController?.on('playbackStateChange', (playbackState: avSession.AVPlaybackState)=>{
if (playbackState?.state) {
// 播放状态变化
}
if (playbackState?.position) {
// 进度变化，可以根据position来获取对端播放的进度
}
if (playbackState?.volume) {
// 音量变化
}
});
// 应用监听对端投播内容播放完毕事件。应用监听到此回调，可以按照业务在以下三种选其一实现，否则无内容播放，对端出现黑屏。
// 1. 有下一集时，自动切换下一集投播，此时需要调用Prepare和start重新设置新的url，参考步骤5
// 2. 无下一集时，建议循环播放同一集，重新调用prepare和start来设置当前新的url，参考步骤5
// 3. 业务不再支持投播，可以主动断开投播，参考步骤9。
this.castController?.on('endOfStream', () => {
// 按业务处理
});
// 应用监听对端或播控中心的进度调节完成事件，包括快进、快退、进度条拖拽完毕
this.castController?.on('seekDone', (position: number) => {
// seekDone表示用户在对端或是播控内进度调节完毕，可以在收到该回调后，根据'playbackStateChange'回调的position刷新绘制应用内进度条
// 应用主动调用seek调节对端进度后，也请等待seekDone回调再根据'playbackStateChange'中的position来刷新，更精准
});
}
```
6.  在申请长时任务时，需要在module.json5文件中：
```typescript
import { backgroundTaskManager } from '@kit.BackgroundTasksKit';
import { wantAgent } from '@kit.AbilityKit';
import { BusinessError } from '@kit.BasicServicesKit';
let context: Context = getContext(this);
function startContinuousTask() {
let wantAgentInfo: wantAgent.WantAgentInfo = {
// 点击通知后，将要执行的动作列表
wants: [
{
bundleName: "com.example.myapplication",
abilityName: "EntryAbility",
}
],
// 点击通知后，动作类型
operationType: wantAgent.OperationType.START_ABILITY,
// 使用者自定义的一个私有值
requestCode: 0,
// 点击通知后，动作执行属性
wantAgentFlags: [wantAgent.WantAgentFlags.UPDATE_PRESENT_FLAG]
};
// 通过wantAgent模块的getWantAgent方法获取WantAgent对象
try {
wantAgent.getWantAgent(wantAgentInfo).then((wantAgentObj) => {
try {
backgroundTaskManager.startBackgroundRunning(context,
backgroundTaskManager.BackgroundMode.MULTI_DEVICE_CONNECTION, wantAgentObj).then(() => {
console.info('Succeeded in requesting to start running in background');
}).catch((error: BusinessError) => {
console.error(`Failed to request to start running in background. Code: ${error.code}, message: ${error.message}`);
});
} catch (error) {
console.error(`Failed to request to start running in background. Error: ${error}`);
}
});
} catch (error) {
console.error(`Failed to get WantAgent. Error: ${error}`);
}
}
```
7.  方式一：直接调用prepare和start更换投播资源，参考步骤5。对端播放新的视频资源，本地显示切换为“正在投播的界面”，不需要断开投播重新投播。 方式二：不主动切换投播资源，正常绘制应用投播业务的按钮但不要创建AVCastPicker，用户点击后直接调用prepare和start更换投播资源，本地显示切换为“正在投播的界面”，不需要断开投播重新投播。
```typescript
import  { avSession }  from '@kit.AVSessionKit'; // 导入AVSession模块
// 与session声明不在同一文件时，需要import
import { session } from './xxx'; // session声明的文件
this.session?.getOutputDevice().then((outputDeviceInfo: avSession.OutputDeviceInfo) => {
// 当前设备的castCategory为Remote，则表示正在投播中
let isCasting = outputDeviceInfo.devices[0].castCategory === avSession.AVCastCategory.CATEGORY_REMOTE;
}).catch((err: BusinessError) => {
console.error(`GetOutputDevice BusinessError: code: ${err.code}, message: ${err.message}`);
});
```
```typescript
import  { avSession }  from '@kit.AVSessionKit'; // 导入AVSession模块
// 与session声明不在同一文件时，需要import
import { session } from './xxx'; // session声明的文件
this.session?.getOutputDevice().then((outputDeviceInfo: avSession.OutputDeviceInfo) => {
// 当前设备的castCategory为Remote，则表示正在投播中
let isCasting = outputDeviceInfo.devices[0].castCategory === avSession.AVCastCategory.CATEGORY_REMOTE;
}).catch((err: BusinessError) => {
console.error(`GetOutputDevice BusinessError: code: ${err.code}, message: ${err.message}`);
});
```
8.  方式一：直接调用prepare和start更换投播资源，参考步骤5。对端播放新的视频资源，本地显示切换为“正在投播的界面”，不需要断开投播重新投播。 方式二：不主动切换投播资源，正常绘制应用投播业务的按钮但不要创建AVCastPicker，用户点击后直接调用prepare和start更换投播资源，本地显示切换为“正在投播的界面”，不需要断开投播重新投播。
9.  方式一：直接调用prepare和start更换投播资源，参考步骤5。对端播放新的视频资源，本地显示切换为“正在投播的界面”，不需要断开投播重新投播。 方式二：不主动切换投播资源，正常绘制应用投播业务的按钮但不要创建AVCastPicker，用户点击后直接调用prepare和start更换投播资源，本地显示切换为“正在投播的界面”，不需要断开投播重新投播。
10.  在应用进入投播后，当前应用需要取消注册焦点处理事件，以免被其他应用的焦点申请而影响。
```typescript
async release() {
// 一般来说，应用退出时，而不希望继续投播，可以主动结束
await session.stopCasting();
}
```
```typescript
async release() {
// 一般来说，应用退出时，而不希望继续投播，可以主动结束
await session.stopCasting();
}
```
-  注意，投播后，应用播放器切换上下集时，也可以通过filter参数控制系统播控中心是否显示可投播设备列表，filter参数设置为0，播控会识别为不支持投播，隐藏可投播设备显示。避免用户从播控中心投播，应用资源不可用。
```typescript
import  { avSession }  from '@kit.AVSessionKit'; // 导入AVSession模块
// 与session声明不在同一文件时，需要import
import { session } from './xxx'; // session声明的文件
this.session?.getOutputDevice().then((outputDeviceInfo: avSession.OutputDeviceInfo) => {
// 当前设备的castCategory为Remote，则表示正在投播中
let isCasting = outputDeviceInfo.devices[0].castCategory === avSession.AVCastCategory.CATEGORY_REMOTE;
}).catch((err: BusinessError) => {
console.error(`GetOutputDevice BusinessError: code: ${err.code}, message: ${err.message}`);
});
```
-  方式一：直接调用prepare和start更换投播资源，参考步骤5。对端播放新的视频资源，本地显示切换为“正在投播的界面”，不需要断开投播重新投播。 方式二：不主动切换投播资源，正常绘制应用投播业务的按钮但不要创建AVCastPicker，用户点击后直接调用prepare和start更换投播资源，本地显示切换为“正在投播的界面”，不需要断开投播重新投播。
-  方式一：直接调用prepare和start更换投播资源，参考步骤5。对端播放新的视频资源，本地显示切换为“正在投播的界面”，不需要断开投播重新投播。 方式二：不主动切换投播资源，正常绘制应用投播业务的按钮但不要创建AVCastPicker，用户点击后直接调用prepare和start更换投播资源，本地显示切换为“正在投播的界面”，不需要断开投播重新投播。
-  方式一：直接调用prepare和start更换投播资源，参考步骤5。对端播放新的视频资源，本地显示切换为“正在投播的界面”，不需要断开投播重新投播。 方式二：不主动切换投播资源，正常绘制应用投播业务的按钮但不要创建AVCastPicker，用户点击后直接调用prepare和start更换投播资源，本地显示切换为“正在投播的界面”，不需要断开投播重新投播。
```typescript
async release() {
// 一般来说，应用退出时，而不希望继续投播，可以主动结束
await session.stopCasting();
}
```
镜像投屏自动切换资源投播
适用场景：用户通过“无线投屏”功能实现手机等设备和大屏等的镜像投屏，然后打开视频应用进入视频播放，此时会自动切换为资源投播。要达到上述目标体验，还需要做一些额外的适配工作。
```typescript
import  { avSession }  from '@kit.AVSessionKit'; //导入AVSession模块
import { BusinessError } from '@kit.BasicServicesKit';
let session: avSession.AVSession; //声明全局的session对象，此写法是加在class类外的声明，如果需要在class类内申明全局变量，需要去掉 let
async checkOtherCast() {
try {
let currentOutputDevice: avSession.OutputDeviceInfo = currentAVSession.getOutputDeviceSync();
} catch (err) {
let error = err as BusinessError;
console.error(`getOutputDeviceSync error, error code: ${error.code}, error message: ${error.message}`);
}
}
```
附录
从服务器获取许可证
开发者需要根据实际的资源和服务地址获取DRM许可证，以下示例代码仅作为参考。
```typescript
import { http } from '@kit.NetworkKit';
// 获取DRM许可证, 仅做参考，需要结合实际资源和服务地址进行获取。
async getLicense(drmUrl: string, requestData: Uint8Array): Promise<Uint8Array | undefined> {
let licenseRequestStr: string = this.byteToString(requestData);
let licenseResponseStr: string = 'defaultStr';
let httpRequest = http.createHttp();
try {
let response: http.HttpResponse = await httpRequest.request(drmUrl, {
method: http.RequestMethod.POST,
header: {
'Content-Type': 'application/json',
'Accept-Encoding': 'gzip, deflate',
},
extraData: licenseRequestStr,
expectDataType: http.HttpDataType.STRING,
});
if (response?.responseCode == http.ResponseCode.OK) {
if (typeof response.result == 'string') {
licenseResponseStr = response.result;
}
}
httpRequest.destroy();
} catch (error) {
console.error(`Failed to request Http. Error: ${error}`);
return undefined;
}
return this.stringToByte(licenseResponseStr);
}
/**
* Uint8Array to string
* @param arr Uint8Array
* @returns string
*/
byteToString(arr: Uint8Array): string {
let str: string = ''
let _arr: Uint8Array = arr
for (let i = 0; i < _arr.length; i++) {
// 将数值转为二进制字符串
let binaryStr: string = _arr[i].toString(2)
let matchArray = binaryStr.match(new RegExp('/^1+?(?=0)/'))
if (matchArray && binaryStr.length == 8) {
let bytesLength: number = matchArray[0].length
let store: string = _arr[i].toString(2).slice(7 - bytesLength)
for (let j = 1; j < bytesLength; j++) {
store += _arr[j + i].toString(2).slice(2)
}
str += String.fromCharCode(Number.parseInt(store, 2))
i += bytesLength - 1
} else {
str += String.fromCharCode(_arr[i])
}
}
return str
}
/**
* string 转 Uint8Array
* @param str string
* @returns Uint8Array
*/
stringToByte(str: string): Uint8Array {
let bytes: number[] = new Array()
let unicode: number
for (let i = 0; i < str.length; i++) {
unicode = str.charCodeAt(i)
if (unicode >= 0x010000 && unicode <= 0x10FFFF) {
bytes.push(((unicode >> 18) & 0x07) | 0xf0)
bytes.push(((unicode >> 12) & 0x3F) | 0x80)
bytes.push(((unicode >> 6) & 0x3F) | 0x80)
bytes.push((unicode & 0x3F) | 0x80)
} else if (unicode >= 0x000800 && unicode <= 0x00FFF) {
bytes.push(((unicode >> 12) & 0x07) | 0xf0)
bytes.push(((unicode >> 6) & 0x3F) | 0x80)
bytes.push((unicode & 0x3F) | 0x80)
} else if (unicode >= 0x000800 && unicode <= 0x0007FF) {
bytes.push(((unicode >> 6) & 0x3F) | 0x80)
bytes.push((unicode & 0x3F) | 0x80)
} else {
bytes.push(unicode & 0xFF)
}
}
return new Uint8Array(bytes);
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/avsession-extended-screen-V14
爬取时间: 2025-04-28 20:07:27
来源: Huawei Developer
通过本节开发指导，可在系统镜像投屏后，获取投屏设备信息，实现扩展屏模式的投播，实现双屏协作的能力。
运作机制
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165936.50693662651305491007161715043323:50001231000000:2800:3899437EDD0EE438C5528D95D7E824D819F81159ED692DF33DEECDF6B98ACE2D.png)
-  是在系统投屏启动过程中建立的，依据双端协商的投屏视频流的分辨率创建，支持1080P 及以上分辨率。默认镜像主屏内容，当虚拟扩展屏上有UIAbility绘制时，会投屏该屏内容。
-  在本端主屏上显示的内容。假定UIAbility A 与 UIAbility B 属于同一应用，UIAbility A可以控制UIAbility B，实现双屏联动。
-  在虚拟扩展屏上绘制的内容，考虑到远端投屏用户体验，UIAbility B 应铺满全屏。从安全角度考虑，在启动UIAbility B 时，系统会校验主屏前台UIAbility是否归属同一应用，如果校验失败会禁止其在虚拟扩展屏启动。
约束与限制
需同时满足以下条件，才能使用该功能：
-  本端设备：HarmonyOS NEXT Developer Beta1及以上版本的手机设备。 远端设备：支持Cast+或Miracast标准协议的设备，推荐使用华为智慧屏HarmonyOS2.0及以上版本。
-  需要系统发起无线/有线投屏后才可通过接口获取有效的扩展投屏设备。
接口说明
| 接口  | 说明  |
| --- | --- |
| getAllCastDisplays(): Promise<Array<CastDisplayInfo>>;  | 获取当前系统中所有支持扩展屏投播的显示设备。  |
| on(type: 'castDisplayChange', callback: Callback<CastDisplayInfo>): void;  | 设置扩展屏投播显示设备变化的监听事件。  |
| off(type: 'castDisplayChange', callback?: Callback<CastDisplayInfo>): void;  | 取消扩展屏投播显示设备变化事件监听，关闭后，不再进行该事件回调。  |
接口
说明
getAllCastDisplays(): Promise<Array<CastDisplayInfo>>;
获取当前系统中所有支持扩展屏投播的显示设备。
on(type: 'castDisplayChange', callback: Callback<CastDisplayInfo>): void;
设置扩展屏投播显示设备变化的监听事件。
off(type: 'castDisplayChange', callback?: Callback<CastDisplayInfo>): void;
取消扩展屏投播显示设备变化事件监听，关闭后，不再进行该事件回调。
开发步骤
1.  获取的屏幕信息CastDisplayInfo中包含屏幕ID，屏幕名称、状态以及分辨率宽度、高度基础属性，其中屏幕id 值同于Display的id，如需要获取更详细的信息可参考Display获取设备信息说明。
```typescript
import { AbilityConstant, UIAbility, Want } from '@kit.AbilityKit';
import  { avSession }  from '@kit.AVSessionKit'; // 导入AVSession模块
import { BusinessError } from '@kit.BasicServicesKit';
export default class AbilityA extends UIAbility{
private session: avSession.AVSession | undefined = undefined;
private extCastDisplayInfo: avSession.CastDisplayInfo | undefined = undefined;
// 注册监听可投屏设备变化事件
private onCastDisplayChangedCallback = (castDisplayInfo: avSession.CastDisplayInfo) => {
// 新增扩展屏,进入扩展屏显示
if (this.extCastDisplayInfo === undefined && castDisplayInfo.state === avSession.CastDisplayState.STATE_ON) {
console.info('Succeeded in opening the cast display');
this.extCastDisplayInfo = castDisplayInfo;
this.startExternalDisplay();
} else if (this.extCastDisplayInfo?.id == castDisplayInfo.id) {
this.extCastDisplayInfo = castDisplayInfo;
// 扩展屏不可用，退出扩展屏显示
if (castDisplayInfo.state === avSession.CastDisplayState.STATE_OFF){
console.info('Succeeded in closing the cast display');
this.stopExternalDisplay();
this.extCastDisplayInfo = undefined;
}
}
};
// 创建AVSession, 获取可用扩展屏投播设备并注册监听
initAVSession(context: Context) {
avSession.createAVSession(context, 'CastDisplay', 'video').then((session: avSession.AVSession) => {
this.session = session;
this.session?.on('castDisplayChange', this.onCastDisplayChangedCallback);
// 获取当前系统可用的扩展屏显示设备
session.getAllCastDisplays().then((infoArr: avSession.CastDisplayInfo[]) => {
// 有多个扩展屏时可以提供用户选择，也可使用其中任一个作为扩展屏使用。
if (infoArr.length > 0) {
this.extCastDisplayInfo = infoArr[0];
this.startExternalDisplay();
}
}).catch((err: BusinessError<void>) => {
console.error(`Failed to get all CastDisplay. Code: ${err.code}, message: ${err.message}`);
});
});
}
async onCreate(want: Want, launchParam: AbilityConstant.LaunchParam): Promise<void> {
super.onCreate(want, launchParam);
this.initAVSession(this.context);
}
onDestroy() {
this.stopExternalDisplay();
// 去注册监听
this.session?.off('castDisplayChange');
}
}
```
```typescript
// 扩展屏启动UIAbilityB
startExternalDisplay() {
if (this.extCastDisplayInfo !== undefined &&
this.extCastDisplayInfo.id !== 0 &&
this.extCastDisplayInfo.state === avSession.CastDisplayState.STATE_ON) {
let id = this.extCastDisplayInfo?.id;
console.info(`Succeeded in starting ability and the id of display is ${id}`);
this.context.startAbility({
bundleName: 'com.example.myapplication', // 应用自有包名
abilityName: 'AbilityB'
}, {
displayId: id // 扩展屏ID
});
AppStorage.setOrCreate('CastDisplayState', 1);
}
}
// 停止使用扩展屏
stopExternalDisplay() {
AppStorage.setOrCreate('CastDisplayState', 0);
// 更新本页面显示。
}
```
```typescript
import { UIAbility } from '@kit.AbilityKit';
import { window } from '@kit.ArkUI';
import { BusinessError } from '@kit.BasicServicesKit';
export default class AbilityB extends UIAbility {
onWindowStageCreate(windowStage: window.WindowStage): void {
// Main window is created, set main page for this ability
windowStage.getMainWindowSync().setWindowLayoutFullScreen(true); // 设置为全屏
windowStage.loadContent('pages/CastPage', (err: BusinessError) => {
if (err.code) {
console.error(`Failed to load the content. Code: ${err.code}, message: ${err.message}`);
return;
}
console.info('Succeeded in loading the content. ');
});
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/using-switch-call-devices-V14
爬取时间: 2025-04-28 20:07:41
来源: Huawei Developer
基本概念
系统不再提供音频输出设备切换的API，如果需要应用内切换音频输出设备，请实现AVCastPicker组件，相关参数可参考@ohos.multimedia.avCastPicker和@ohos.multimedia.avCastPickerParam。
本文将主要介绍AVCastPicker组件接入，实现通话设备切换。
当前系统支持两种组件样式的显示方式：默认样式显示和自定义样式显示。如果应用选择显示默认样式，当设备切换时，系统将根据当前选择的设备显示系统默认的组件样式；如果应用选择显示自定义样式，那么需要应用根据设备的变化刷新自己定义的样式。
开发步骤
默认样式实现
1.  创建voice_call类型的AVSession，AVSession在构造方法中支持不同的类型参数，由AVSessionType定义，voice_call表示通话类型，如果不创建，将显示空列表。
```typescript
import { avSession } from '@kit.AVSessionKit';
private session: avSession.AVSession | undefined = undefined;
// 通话开始时创建voice_call类型的avsession
this.session = await avSession.createAVSession(getContext(this), 'voiptest', 'voice_call');
```
2.  在需要切换设备的通话界面创建AVCastPicker组件。
```typescript
import { AVCastPicker } from '@kit.AVSessionKit';
// 创建组件，并设置大小
build() {
Row() {
Column() {
AVCastPicker()
.size({ height:45, width:45 })
}
}
}
```
3.  创建VOICE_COMMUNICATION类型的AudioRenderer，并开始播放。具体通话音频播放等实现，请参考AudioKit开发音频通话功能。
```typescript
import { audio } from '@kit.AudioKit';
import { BusinessError } from '@kit.BasicServicesKit';
private audioRenderer: audio.AudioRenderer | undefined = undefined;
private audioStreamInfo: audio.AudioStreamInfo = {
// 请按照实际场景设置，当前参数仅参考
samplingRate: audio.AudioSamplingRate.SAMPLE_RATE_48000, // 采样率
channels: audio.AudioChannel.CHANNEL_2, // 通道
sampleFormat: audio.AudioSampleFormat.SAMPLE_FORMAT_S16LE, // 采样格式
encodingType: audio.AudioEncodingType.ENCODING_TYPE_RAW // 编码格式
}
private audioRendererInfo: audio.AudioRendererInfo = {
// 需使用通话场景相应的参数
usage: audio.StreamUsage.STREAM_USAGE_VIDEO_COMMUNICATION, // 音频流使用类型：VOIP视频通话，默认为扬声器
rendererFlags: 0 // 音频渲染器标志：默认为0即可
}
private audioRendererOptions: audio.AudioRendererOptions = {
streamInfo: this.audioStreamInfo,
rendererInfo: this.audioRendererInfo
}
// 初始化，创建通话audiorenderer实例，设置监听事件
try {
this.audioRenderer = await audio.createAudioRenderer(this.audioRendererOptions);
} catch (err) {
console.error(`audioRender create :  Error: ${JSON.stringify(err)}`);
}
this.audioRenderer?.start((err: BusinessError) => {
if (err) {
console.error(`audioRender start faild :  Error: ${JSON.stringify(err)}`);
} else {
console.error('audioRender start success');
}
});
```
4.  （可选）如果应用想知道设备切换情况，可以监听当前发声设备切换回调。
```typescript
import { audio } from '@kit.AudioKit';
let audioManager = audio.getAudioManager(); // 先创建audiomanager
let audioRoutingManager = audioManager.getRoutingManager(); // 再调用AudioManager的方法创建AudioRoutingManager实例
// 可选监听当前发声设备切换回调
audioRoutingManager.on('preferOutputDeviceChangeForRendererInfo', this.audioRendererInfo, (desc: audio.AudioDeviceDescriptors) => {
console.info(`device change To : ${desc[0].deviceType}`); // 设备类型
});
```
5.  通话结束后，销毁会话。
```typescript
// 通话结束销毁第一步创建的session
this.session?.destroy((err) => {
if (err) {
console.error(`Failed to destroy session. Code: ${err.code}, message: ${err.message}`);
} else {
console.info(`Destroy : SUCCESS `);
}
});
```
自定义样式实现
自定义样式通过设置CustomBuilder类型的参数customPicker实现。
实现自定义样式的步骤与实现默认样式基本相同，开发者可参考默认样式实现，完成创建AVSession、实现音频播放等步骤。
存在差异的步骤如下所示。
1.  创建自定义AVCastPicker，需要新增自定义参数。（对应默认样式实现步骤2）
```typescript
import { AVCastPicker } from '@kit.AVSessionKit';
@State pickerImage:ResourceStr = $r('app.media.earpiece'); // 自定义资源
build() {
Row() {
Column() {
AVCastPicker(
{
customPicker: (): void => this.ImageBuilder() // 新增自定义参数
}
).size({ height: 45, width:45 })
}
}
}
// 自定义内容
@Builder
ImageBuilder(): void {
Image(this.pickerImage)
.size({ width: '100%', height: '100%' })
.backgroundColor('#00000000')
.fillColor(Color.Black)
}
```
2.  如果应用要根据出声设备变化而改变自定义样式，必须监听设备切换，然后实时刷新自定义样式。（对应默认样式实现步骤4）
```typescript
import { audio } from '@kit.AudioKit';
async observerDevices() {
let audioManager = audio.getAudioManager();
let audioRoutingManager = audioManager.getRoutingManager();
// 初次拉起AVCastPicker时需获取当前设备,刷新显示
this.changePickerShow(audioRoutingManager.getPreferredOutputDeviceForRendererInfoSync(this.audioRendererInfo));
// 监听当前发声设备切换，及时根据不同设备类型显示不同的样式
audioRoutingManager.on('preferOutputDeviceChangeForRendererInfo', this.audioRendererInfo, (desc: audio.AudioDeviceDescriptors) => {
this.changePickerShow(audioRoutingManager.getPreferredOutputDeviceForRendererInfoSync(this.audioRendererInfo));
});
}
// 设备更新后刷新自定义资源pickerImage
private changePickerShow(desc: audio.AudioDeviceDescriptors) {
if (desc[0].deviceType === 2) {
this.pickerImage = $r('app.media.sound');
} else if (desc[0].deviceType === 7) {
this.pickerImage = $r('app.media.bluetooth');
} else {
this.pickerImage = $r('app.media.earpiece');
}
}
```
示例代码

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/playback-control-access-selfcheck-V14
爬取时间: 2025-04-28 20:07:54
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/playback-control-access-checklist-V14
爬取时间: 2025-04-28 20:08:08
来源: Huawei Developer
如果应用包含在后台/锁屏状态下播放音频等业务场景，开发者需要根据如下表格规范接入AVSession；如应用仅需在前台播放音视频，开发者可选择不接入AVSession。
如接入AVSession，请按照下表根据应用的业务场景接入，应用上架前请根据此表格自检，以确保应用的基础体验。
判断原则如下：
| 基础特性  | 子特性  | 具体功能  | 音乐类/听书类  | 视频类  | 直播类  | 浏览器类  | 新闻阅读类  | Voip类  |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| 完整体验优化  | 基本接入要求  | 完整体验优化  | 基本接入要求  | 接入要求  |
| 接入AVSession  | √  | √  | √  | √  | √  | √  | √  | √  |
| 基础播控（必需）  | 播放信息  | 媒体封面  | √  | √  | √  | √  | √  | О  | О  | /  |
| 主标题  | √  | √  | √  | √  | √  | √  | √  | /  |
| 进度与时间  | √  | √  | √  | √  | О  | √  | √  | /  |
| 播放控制  | 播放/暂停  | √  | √  | √  | √  | √  | √  | √  | /  |
| 上下一首/集  | √  | √  | √  | √  | /  | /  | /  | /  |
| 按钮置灰  | √  | √  | √  | √  | √  | √  | √  | /  |
| 点击播控卡片跳转应用指定页面  | √  | √  | √  | √  | √  | √  | √  | /  |
| 音视频投播  | 通话设备切换  | 通话设备切换组件  | /  | /  | /  | /  | /  | /  | /  | √  |
| 基础播控（按需选择）  | 播放信息  | 副标题  | √  | О  | О  | О  | О  | О  | О  | /  |
| 滚动歌词  | √  | О  | /  | /  | /  | /  | /  | /  |
| 媒体音源特殊标识  | О  | О  | О  | О  | /  | /  | /  | /  |
| 播放控制  | 收藏  | √  | О  | /  | /  | /  | /  | /  | /  |
| 循环模式  | √  | О  | /  | /  | /  | /  | /  | /  |
| 快进/快退  | /  | /  | √  | О  | /  | О  | /  | /  |
| 播控增强  | 快捷播放  | 播放按钮一键冷启动播放  | √  | √  | /  | /  | /  | /  | /  | /  |
| 历史歌单  | √  | О  | /  | /  | /  | /  | /  | /  |
| 歌单推荐  | √  | О  | /  | /  | /  | /  | /  | /  |
| 特殊组件  | 统一音量组件  | √  | О  | √  | О  | /  | /  | /  | /  |
| 音视频投播  | 基础投播能力  | Cast+协议音视频投播  | √  | О  | √  | О  | О  | О  | О  | /  |
| DRM数字加密视频投播  | /  | /  | О  | О  | /  | /  | /  | /  |
| DLNA协议音视频投播  | √  | О  | √  | О  | О  | О  | О  | /  |
| 投播能力增强  | 镜像投屏自动切换资源投播  | √  | О  | √  | О  | /  | /  | /  | /  |
| 投播组件  | 应用内投播组件（半模态）  | √  | О  | √  | О  | О  | О  | О  | /  |
基础特性
子特性
具体功能
音乐类/听书类
视频类
直播类
浏览器类
新闻阅读类
Voip类
完整体验优化
基本接入要求
完整体验优化
基本接入要求
接入要求
接入AVSession
√
√
√
√
√
√
√
√
基础播控（必需）
播放信息
媒体封面
√
√
√
√
√
О
О
/
主标题
√
√
√
√
√
√
√
/
进度与时间
√
√
√
√
О
√
√
/
播放控制
播放/暂停
√
√
√
√
√
√
√
/
上下一首/集
√
√
√
√
/
/
/
/
按钮置灰
√
√
√
√
√
√
√
/
点击播控卡片跳转应用指定页面
√
√
√
√
√
√
√
/
音视频投播
通话设备切换
通话设备切换组件
/
/
/
/
/
/
/
√
基础播控（按需选择）
播放信息
副标题
√
О
О
О
О
О
О
/
滚动歌词
√
О
/
/
/
/
/
/
媒体音源特殊标识
О
О
О
О
/
/
/
/
播放控制
收藏
√
О
/
/
/
/
/
/
循环模式
√
О
/
/
/
/
/
/
快进/快退
/
/
√
О
/
О
/
/
播控增强
快捷播放
播放按钮一键冷启动播放
√
√
/
/
/
/
/
/
历史歌单
√
О
/
/
/
/
/
/
歌单推荐
√
О
/
/
/
/
/
/
特殊组件
统一音量组件
√
О
√
О
/
/
/
/
音视频投播
基础投播能力
Cast+协议音视频投播
√
О
√
О
О
О
О
/
DRM数字加密视频投播
/
/
О
О
/
/
/
/
DLNA协议音视频投播
√
О
√
О
О
О
О
/
投播能力增强
镜像投屏自动切换资源投播
√
О
√
О
/
/
/
/
投播组件
应用内投播组件（半模态）
√
О
√
О
О
О
О
/
示例代码

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/access-checklist-V14
爬取时间: 2025-04-28 20:08:22
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/basic-playback-control-V14
爬取时间: 2025-04-28 20:08:36
来源: Huawei Developer
媒体封面
自验证关注点：播放过程中查看播控中心是否显示媒体封面，封面图是否清晰。
应用提供媒体内容的封面图片（AVMetadata.mediaImage），如音乐专辑封面、视频海报等。如果应用提供的媒体封面比例或分辨率不满足要求，将会被自动缩放、裁切到合适大小。这可能导致封面图片内的信息损失或模糊，体验下降。mediaImage设置PixelMap性能更优。
音乐类媒体内容应提供比例为 1:1 的方形封面图片，建议分辨率为 800px * 800px（如果应用提供的图片分辨率更大，将被压缩到 800px * 800px 显示），最小分辨率是 300px * 300px。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165936.44924793030521232241179789902140:50001231000000:2800:2E83693020F72FBF0248BA22A1BD140E0909A221E3C1D9B8D2C303B5B83E7518.png)
视频及其他类型的媒体内容除了上述建议分辨率的方形模板外，还支持纵向及横向的矩形封面模板。
纵向矩形模板的宽高比为13:18，如小于此比例，将会被自动缩放、裁切到该比例。
横向矩形模板的宽高比为16:9，如大于此比例，将会被自动缩放、裁切到该比例。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165937.92749424485236437104515556162543:50001231000000:2800:65945ACB4142FD1B47C0C67CBACE15081EC597AFC9C108C76DD3BFE477A1C31C.png)
主标题
自验证关注点：播放过程中查看播控中心是否显示主标题，显示是否正确。
主标题（AVMetadata.title）用于显示歌曲名、影片名等内容名称，直播应用也可设置直播间名等，用于向用户展示当前正在播放的媒体内容，建议采用简短的字符串。字符串超长时会从右向左滚动显示。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165937.63743483267623606047285181940641:50001231000000:2800:2F46FF89FF46AE93767F989837529E36AD5433A624910149F806DF041D9F15BA.png)
进度与时间
自验证关注点：播放过程中查看播控中心进度条是否正常显示，是否支持拖拽，拖拽后是否正确响应，不出现进度条回弹、抖动等。
进度的显示
播控中心支持显示如下三种形式的进度条，为保证用户体验，应用需尽量支持第一种进度条。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165937.08342382804680701949120012709957:50001231000000:2800:7CB2BE1B753A21B3FE19EDFFFEB6E533D79C3E8A7FD9F3A15A014DC43211D9E3.png)
进度的控制
第一种可拖动进度条，表示用户可通过播控中心，调节应用媒体播放进度。为了达到进度调节的一致体验，请参考如下内容开发：
1.  如： （1）应用播放、暂停时需要设置播放状态或暂停状态，及当前播放或暂停时的进度。 （2）用户通过播控中心调节进度条，应用收到调节的回调，或在应用内调节进度条，应用都需要通知播控当前调节完毕的状态与进度。 （3）应用在真实播放开始时，再设置起始进度；若播放存在缓冲状态，可以先上报播放状态为PLAYBACK_STATE_BUFFERING，来通知播控显示为播放但不走进度。
2.  等同于不支持进度条，可按照第三种进度显示接入。
3.  等同于不支持进度条，可按照第三种进度显示接入。
-  等同于不支持进度条，可按照第三种进度显示接入。
副标题
自验证关注点：播放过程中查看播控中心是否显示副标题，显示是否正确。
副标题用于显示媒体内容的辅助信息，如歌曲的歌手名、影片的发布者信息、剧集/综艺节目的选集信息等。可通过AVMetadata.subtitle或者AVMetadata.artist，选其一设置。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165937.92505675009325584528660177786564:50001231000000:2800:B5137B1C50138E496065F881D8E2E5FB7C8C870002B9F4299F29C48D08270449.png)
滚动歌词
自验证关注点：播放过程中查看播控中心是否显示歌词，显示是否正确，是否随进度正确刷新显示。
歌曲类媒体内容如有歌词信息，可以选择在副标题区域显示歌词。将当前播放歌曲的全曲歌词内容，按照标准lyric格式拼接为字符串，如[00:25.44]xxx\r\n[00:26.44]xxx\r\n，通过AVMetadata.lyric设置给播控中心。播控中心会自动按照进度，在副标题位置刷新显示，应用不需要实现其余功能。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165937.08493050984312666167739574811411:50001231000000:2800:E16F180B01360C7FF29FFE7518D320BBFE55928DC880E142C637779C0B4C78AB.png)
媒体音源特殊标识
自验证关注点：播放过程中查看播控中心是否显示“AudioVivid”等标识。
应用可以提供当前播放的媒体内容的资源标签信息（AVMetadata.displayTags）。根据媒体资源的属性，应用可用提供标签信息以体现该媒体内容的特殊性，如：AudioVivid
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165938.70325355755478170894521909647465:50001231000000:2800:CE77B26EE9B4CD656D222CC40BB4FFDFDF95353E0FD04038BF710785C81561DC.png)
播放/暂停
自验证关注点：播放过程中，进入播控中心，点击播放暂停查看是否生效，状态是否与应用内对应。
应用需支持播控中心播放暂停，在接收到播控的播放/暂停回调，或者用户在应用内播放暂停，需上报当前的播放状态与进度。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165938.33933950656513776701736482695967:50001231000000:2800:A8750D057638E5B48EE74168A30B78A1235C7250D4371CAAC1B215D042FDD184.png)
上下一首/集
自验证关注点：播放过程中，进入播控中心，点击上一首、下一首查看是否生效，播放内容是否与应用内对应。
应用按照内部实现，接入上下一首/集，在接收到播控的上下一首/集回调，或者用户在应用切歌切集时，需上报切换后新的媒体信息，播放状态、进度。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165938.96430380226229563231361672724794:50001231000000:2800:6A416C159E4AAC230B18CE9FAE23C87AD803BC4EB9A2530BF75CA64C4FF4E459.png)
按钮置灰
自验证关注点：播放过程中，进入播控中心，查看不支持的功能按钮是否已置灰。请按照自检表按应用类型接入必需的控制指令，以保障用户的体验。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165938.80236348256915815956869356101787:50001231000000:2800:387CFCF2320B67993E854DD3922A57917D8A87F71BE73EC13BEABC03FFE10113.png)
应用按照内部实现，按需注册支持的播放控制指令。对于未注册的播放控制指令，在播控中心会显示为上图置灰样式，明确告知用户当前指令该应用不支持。具体实现可参考应用接入AVSession-不支持命令的处理。
点击播控卡片跳转应用指定页面
自验证关注点：播放过程中，进入播控中心，点击封面大图查看是否跳转至应用当前播放页面。
用户通过点击播控卡片，应跳转到应用的具体业务页，如：音乐/听书/视频的播放详情页，直播间页，新闻阅读播放页，浏览器具体tab页。具体实现可参考媒体会话提供方-开发步骤的第3步。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165938.59661836050556268380587080852108:50001231000000:2800:3AE7DA5A74B9A0C983D39880C846D6D64E575C0CB6C1D9C142075E1C227177C5.png)
收藏
自验证关注点：播放过程中，进入播控中心，点击收藏按钮，查看是否生效，是否与应用内同功能按钮状态一致。
音乐/听书类应用，如应用内支持收藏/喜欢功能，可按需适配播控的收藏功能，用户播放过程中可以通过播控中心点击收藏/取消收藏。应用适配收藏功能，接收到播控的收藏/取消收藏的回调，或者用户在应用内点击收藏/取消收藏，均需上报当前播放内容的收藏状态，保证应用与播控的显示一致。具体实现可参考应用接入AVSession-收藏。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165938.64548195243415256600393096633268:50001231000000:2800:F2D517D02F4AD955EAB1A871A8C5D8D66555B9C020589737D32FECE6F9D1C6BB.png)
循环模式
自验证关注点：播放过程中，进入播控中心，点击切换循环模式，查看是否生效。
音乐/听书类应用，如应用内支持播放模式的切换，可按需适配播控的循环模式切换功能。
播控可支持的播放模式有：顺序播放、列表循环、单曲循环、随机播放。
应用适配循环模式切换功能，收到播控中心循环模式切换的回调后，或用户在应用内切换循环模式时，按照应用内定义的顺序，向播控上报切换后的循环模式。
例：收到播控切换循环模式回调参数为列表循环，表示当前的循环模式，应用内下一个循环模式为随机播放，就切换到随机播放，并设置AVPlaybackState的LoopMode为随机播放。
若应用内支持的循环模式不在系统固定的四个循环模式内，需要选择四个固定循环模式其一向系统上报，由应用自定。
例：收到播控切换循环模式回调参数为列表循环，表示当前的循环模式，应用内下一个循环模式为心动模式，可上报为随机播放。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165939.57833292243364533883113394117480:50001231000000:2800:93C6DA136C7E17179A74C0C43FC896A3425DEDC1312EC23A9F30F1EAB7F211D5.png)
快进/快退
自验证关注点：播放过程中，进入播控中心，点击快进、快退查看是否正常响应，播控中心进度是否显示正确。
对于需要频繁调节播放进度的媒体内容（如播客、听书等长音频媒体，或长视频媒体），应用可以适配快进快退功能。
可选择快进快退的时间长度：10s、15s、30s。如下图显示。用户通过播控中心快进/快退，或在应用内快进快退，应用都需要通知播控当前调节完毕的状态与进度。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165939.53432483865754673902850500742850:50001231000000:2800:E22FD5A43943F2D653F59EDBF42133E670FC02F753A75AF42500CDF8201D5651.png)

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/quick-playback-V14
爬取时间: 2025-04-28 20:08:49
来源: Huawei Developer
针对音乐/听书类应用，播控中心提供一系列快捷播放能力，包括一键启动冷启动续播、以及历史歌单与推荐歌单功能，其中歌单功能中支持显示的音频媒体内容有：音乐歌单、有声书专辑、播客专辑等。视频媒体内容、直播类媒体内容暂不支持歌单。应用选择PlayMusicList意图（音乐类应用）或者PlayAudio意图（听书类应用）其一，注册并适配意图调用，即可实现接入上述三个功能，具体实现参考历史歌单。
播放按钮一键冷启动播放
自验证关注点：用户在应用内播放后，上滑结束应用进程，再进入播控中心，点击播放键查看是否正常拉起应用播放，播控中心是否正确显示当前播放信息及播放状态。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165939.76127637643388478556076543910278:50001231000000:2800:B11CA3834E6499F034EE757DB8A11BDF5B56025A43B6B36C2FFC19AB8E0384A5.png)
历史歌单/歌单推荐
自验证关注点：
-  应用根据用户播放当前音频媒体时选择的入口，向播控提供对应的歌单信息。歌单信息仅包括：歌单封面（图片显示规则等同于媒体封面）、歌单标题（不支持显示副标题）、歌单唯一Id（应用内可识别并播放对应歌单）。
-  歌单分为 “最近播放” 的历史歌单，和 “为你推荐” 的个性化歌单；分别通过用户播放行为记录和应用推荐产生。 用户如未开启 “播控推荐服务”，歌单列表仅展示 “最近播放”。播控会记录用户播放过的音频内容所属的歌单信息。如果多个音频内容同属一个歌单，则只记录为一个歌单信息。最多可展示 4 个最近播放的歌单。 如果用户开启了 “播控推荐服务”，歌单列表展示 “为你推荐”。最多可展示 8 个基于算法推荐的歌单。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165939.07914930317398929611821192707182:50001231000000:2800:5E119F46C2AAC63EACC1335D3748619DED1AD338394D802EB9807F237158A38B.png)

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/avcastpicker-V14
爬取时间: 2025-04-28 20:09:03
来源: Huawei Developer
针对音视频类应用，播控中心提供系统级设备切换、投播能力选择入口，提供音视频发声设备统一投播组件。应用通过接入统一投播组件，可以实现在应用内及系统播控中心，将应用音视频资源通过Cast+协议/DLNA协议投播到远端设备。应用需先按自检要求接入基础播控，才可正常接入音视频投播组件。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165939.65381290193324701815074364592722:50001231000000:2800:5C1B6BAC076630C4CEA0E958A57B71E1AC3C8DAFBAB842BFA451036D29CFC392.png)
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165940.45175763143289428437229225806611:50001231000000:2800:E6FD74BF38ACAF3898F41E9CA92DABCEDC96CD62DE3179190D6150AA5DCBC5A0.png)
基础投播能力
Cast+协议音视频投播/DLNA协议音视频投播
自验证关注点：播放可投播的音视频资源，点击投播至3.1以上的华为智慧屏/DLNA协议的设备，查看投播功能是否正常可用，且在应用内及系统播控中心内能控制远端投播。
1.  如不显示，排查是否按自检要求正确适配了基础播控。
2.  如不显示，排查是否设置了setExtras({requireAbilityList: ['url-cast']})，具体参考投播开发指南。
3.  如对端黑屏/不显示播放内容，应用自排查是否正确设置了资源链接，正确调用prepare及start接口，具体参考投播开发指南。
4.  应用按照实际功能的有无，参照自检表注册必需的控制指令，例如on(type: 'playbackStateChange')来监听播控及远端设备的播放暂停指令、具体控制指令的开发参考投播开发指南。
DRM数字加密视频投播
自验证关注点：播放可投播的DRM数字加密视频资源，点击投播至3.1以上的华为智慧屏，或支持DRM硬件解码的大屏设备，查看投播功能是否正常可用。
投播能力增强
镜像投屏自动切换资源投播
自验证关注点：在控制中心发起无线投屏后，在应用内播放可投播的音视频资源，查看是否自动切换为资源投播模式（Cast+协议音视频投播/DLNA协议音视频投播）。
用户通过“无线投屏”功能实现手机等设备和大屏等的镜像投屏，然后打开视频应用进入视频播放，此时应用需切换到资源投播。具体实现可参考镜像投屏自动切换资源投播。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/avsession-recommendation-V14
爬取时间: 2025-04-28 20:09:17
来源: Huawei Developer
播控特性简介
播控推荐服务致力于为用户提供更便捷的操作路径、更精准的内容推荐服务，帮助用户发现更感兴趣的内容，助力应用从系统级入口直达服务。
播控推荐服务基于用户使用音频类应用的习惯来分配播控推荐服务的资源位，保障用户常用的应用有更多的曝光。
同时，将基于用户的听歌偏好进行内容精准推荐，推荐的内容源需要三方应用通过云侧接口捐赠给播控中心。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165940.01765715276197303181055972481140:50001231000000:2800:BB5E644DE7F1C4584872DCBC4C2D107AE26A5F0BBA18693EDEF0AF53208CBBFA.png)
推荐资源位分配原则
播控中心推荐服务作为公共系统级入口，将依据统一的分配原则，来保证该栏目资源位分配公平，且符合用户的预期。
约束与限制
当前仅支持在中国大陆区域使用。
接入方式

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-kit-V14
爬取时间: 2025-04-28 20:09:30
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-overview-V14
爬取时间: 2025-04-28 20:09:44
来源: Huawei Developer
开发者通过调用Camera Kit(相机服务)提供的接口可以开发相机应用，应用通过访问和操作相机硬件，实现基础操作，如预览、拍照和录像；还可以通过接口组合完成更多操作，如控制闪光灯和曝光时间、对焦或调焦等。
开发场景
当开发者需要开发一个相机应用（或是在应用内开发相机模块）时，可参考以下开发模型了解相机的工作流程，进而开发相机应用，具体可参考相机开发指导。
如果开发者仅是需要拉起系统相机拍摄一张照片、录制一段视频，可直接使用CameraPicker，无需申请相机权限，直接拉起系统相机完成拍摄，具体可参考Camera Picker。
开发模型
相机调用摄像头采集、加工图像视频数据，精确控制对应的硬件，灵活输出图像、视频内容，满足多镜头硬件适配（如广角、长焦、TOF）、多业务场景适配（如不同分辨率、不同格式、不同效果）的要求。
相机的工作流程如图所示，可概括为相机输入设备管理、会话管理和相机输出管理三部分。
-  相机设备调用摄像头采集数据，作为相机输入流。
-  会话管理可配置输入流，即选择哪些镜头进行拍摄。另外还可以配置闪光灯、曝光时间、对焦和调焦等参数，实现不同效果的拍摄，从而适配不同的业务场景。应用可以通过切换会话满足不同场景的拍摄需求。
-  配置相机的输出流，即将内容以预览流、拍照流或视频流输出。
图1相机工作流程
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165940.82274936656468178551853747150736:50001231000000:2800:B326D9F309612D050A5B6D3047BFDD4A9A3233E30DE8271EA7A7203F79A28CEB.png)
了解相机工作流程后，建议开发者了解相机的开发模型，便于更好地开发相机应用。
图2相机开发模型
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165940.78059800747411520664800762449546:50001231000000:2800:16EBA321F1ED24CB71C613100588F55A3061B4791B2D496F3A2B0BAEA45DC9C2.png)
相机应用通过控制相机，实现图像显示（预览）、照片保存（拍照）、视频录制（录像）等基础操作。在实现基本操作过程中，相机服务会控制相机设备采集和输出数据，采集的图像数据在相机底层的设备硬件接口（HDI，Hardware Device Interfaces），直接通过BufferQueue传递到具体的功能模块进行处理。BufferQueue在应用开发中无需关注，用于将底层处理的数据及时送到上层进行图像显示。
以视频录制为例进行说明，相机应用在录制视频过程中，媒体录制服务先创建一个视频Surface用于传递数据，并提供给相机服务，相机服务可控制相机设备采集视频数据，生成视频流。采集的数据通过底层相机HDI处理后，通过Surface将视频流传递给媒体录制服务，媒体录制服务对视频数据进行处理后，保存为视频文件，完成视频录制。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-preparation-V14
爬取时间: 2025-04-28 20:09:58
来源: Huawei Developer
相机应用开发的主要流程包含开发准备、设备输入、会话管理、预览、拍照和录像等。
申请权限
在开发相机应用时，需要先申请相机相关权限，确保应用拥有访问相机硬件及其他功能的权限，需要的权限如下表。在申请权限前，请保证符合权限使用的基本原则。
以上权限均需要通过弹窗向用户申请授权，具体申请方式及校验方式，请参考向用户申请授权。
仅应用需要克隆、备份或同步用户公共目录的图片、视频类文件时，可申请ohos.permission.READ_IMAGEVIDEO、ohos.permission.WRITE_IMAGEVIDEO权限来读写音频文件，申请方式请参考申请受控权限，通过AGC审核后才能使用。为避免应用的上架申请被驳回，开发者应优先使用Picker/控件等替代方案，仅少量符合特殊场景的应用被允许申请受限权限。
开发指导
当前相机提供了ArkTS和C++两种开发语言的开发指导，如下表所示。
| 开发流程 | ArkTS开发指导 | C++开发指导 |
| --- | --- | --- |
| 设备输入 | 设备输入(ArkTS) | 设备输入(C/C++) |
| 会话管理 | 会话管理(ArkTS) | 会话管理(C/C++) |
| 预览 | 预览(ArkTS) | 预览(C/C++) |
| 预览流二次处理 | - | 预览流二次处理(C/C++) |
| 拍照 | 拍照(ArkTS) | 拍照(C/C++) |
| 分段式拍照 | 分段式拍照(ArkTS) | - |
| 动态照片 | 动态照片(ArkTS) | - |
| 录像 | 录像(ArkTS) | 录像(C/C++) |
| 元数据 | 元数据(ArkTS) | 元数据(C/C++) |

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-dev-arkts-V14
爬取时间: 2025-04-28 20:10:11
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-device-management-V14
爬取时间: 2025-04-28 20:10:25
来源: Huawei Developer
在开发一个相机应用前，需要先通过调用相机接口来创建一个独立的相机设备。
开发步骤
详细的API说明请参考Camera API参考。
1.  导入camera接口，接口中提供了相机相关的属性和方法，导入方法如下。
```typescript
import { camera } from '@kit.CameraKit';
import { BusinessError } from '@kit.BasicServicesKit';
import { common } from '@kit.AbilityKit';
```
2.  通过getCameraManager方法，获取cameraManager对象。 Context获取方式请参考：获取UIAbility的上下文信息。 如果获取对象失败，说明相机可能被占用或无法使用。如果被占用，须等到相机被释放后才能重新获取。
```typescript
function getCameraManager(context: common.BaseContext): camera.CameraManager {
let cameraManager: camera.CameraManager = camera.getCameraManager(context);
return cameraManager;
}
```
3.  通过CameraManager类中的getSupportedCameras方法，获取当前设备支持的相机列表，列表中存储了设备支持的所有相机ID。若列表不为空，则说明列表中的每个ID都支持独立创建相机对象；否则，说明当前设备无可用相机，不可继续后续操作。
```typescript
function getCameraDevices(cameraManager: camera.CameraManager): Array<camera.CameraDevice> {
let cameraArray: Array<camera.CameraDevice> = cameraManager.getSupportedCameras();
if (cameraArray != undefined && cameraArray.length > 0) {
for (let index = 0; index < cameraArray.length; index++) {
console.info('cameraId : ' + cameraArray[index].cameraId);  // 获取相机ID
console.info('cameraPosition : ' + cameraArray[index].cameraPosition);  // 获取相机位置
console.info('cameraType : ' + cameraArray[index].cameraType);  // 获取相机类型
console.info('connectionType : ' + cameraArray[index].connectionType);  // 获取相机连接类型
}
return cameraArray;
} else {
console.error("cameraManager.getSupportedCameras error");
return [];
}
}
```
状态监听
在相机应用开发过程中，可以随时监听相机状态，包括新相机的出现、相机的移除、相机的可用状态。在回调函数中，通过相机ID、相机状态这两个参数进行监听，如当有新相机出现时，可以将新相机加入到应用的备用相机中。
通过注册cameraStatus事件，通过回调返回监听结果，callback返回CameraStatusInfo参数，参数的具体内容可参考相机管理器回调接口实例CameraStatusInfo。
```typescript
function onCameraStatusChange(cameraManager: camera.CameraManager): void {
cameraManager.on('cameraStatus', (err: BusinessError, cameraStatusInfo: camera.CameraStatusInfo) => {
if (err !== undefined && err.code !== 0) {
console.error(`Callback Error, errorCode: ${err.code}`);
return;
}
// 如果当通过USB连接相机设备时，回调函数会返回新的相机出现状态
if (cameraStatusInfo.status == camera.CameraStatus.CAMERA_STATUS_APPEAR) {
console.info(`New Camera device appear.`);
}
// 如果当断开相机设备USB连接时，回调函数会返回相机被移除状态
if (cameraStatusInfo.status == camera.CameraStatus.CAMERA_STATUS_DISAPPEAR) {
console.info(`Camera device has been removed.`);
}
// 相机被关闭时，回调函数会返回相机可用状态
if (cameraStatusInfo.status == camera.CameraStatus.CAMERA_STATUS_AVAILABLE) {
console.info(`Current Camera is available.`);
}
// 相机被打开/占用时，回调函数会返回相机不可用状态
if (cameraStatusInfo.status == camera.CameraStatus.CAMERA_STATUS_UNAVAILABLE) {
console.info(`Current Camera has been occupied.`);
}
console.info(`camera: ${cameraStatusInfo.camera.cameraId}`);
console.info(`status: ${cameraStatusInfo.status}`);
});
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-device-input-V14
爬取时间: 2025-04-28 20:10:39
来源: Huawei Developer
在开发相机应用时，需要先参考开发准备申请相关权限。
相机应用可通过调用和控制相机设备，完成预览、拍照和录像等基础操作。
开发步骤
详细的API说明请参考Camera API参考。
1.  导入camera接口，接口中提供了相机相关的属性和方法，导入方法如下。 在相机设备输入之前需要先完成相机管理，详细开发步骤请参考相机管理。
```typescript
import { camera } from '@kit.CameraKit';
import { BusinessError } from '@kit.BasicServicesKit';
```
2.  通过cameraManager类中的createCameraInput方法创建相机输入流。
```typescript
async function createInput(cameraDevice: camera.CameraDevice, cameraManager: camera.CameraManager): Promise<camera.CameraInput | undefined> {
// 创建相机输入流
let cameraInput: camera.CameraInput | undefined = undefined;
try {
cameraInput = cameraManager.createCameraInput(cameraDevice);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to createCameraInput errorCode = ' + err.code);
}
if (cameraInput === undefined) {
return undefined;
}
// 监听cameraInput错误信息
cameraInput.on('error', cameraDevice, (error: BusinessError) => {
console.error(`Camera input error code: ${error.code}`);
});
// 打开相机
await cameraInput.open();
return cameraInput;
}
```
3.  通过getSupportedSceneModes方法，获取当前相机设备支持的模式列表，列表中存储了相机设备支持的所有模式SceneMode。
```typescript
function getSupportedSceneMode(cameraDevice: camera.CameraDevice, cameraManager: camera.CameraManager): Array<camera.SceneMode> {
// 获取相机设备支持的模式列表
let sceneModeArray: Array<camera.SceneMode> = cameraManager.getSupportedSceneModes(cameraDevice);
if (sceneModeArray != undefined && sceneModeArray.length > 0) {
for (let index = 0; index < sceneModeArray.length; index++) {
console.info('Camera SceneMode : ' + sceneModeArray[index]);
}
return sceneModeArray;
} else {
console.error("cameraManager.getSupportedSceneModes error");
return [];
}
}
```
4.  通过getSupportedOutputCapability方法，获取当前相机设备支持的所有输出流，如预览流、拍照流、录像流等。输出流在CameraOutputCapability中的各个profile字段中，根据相机设备指定模式SceneMode的不同，需要添加不同类型的输出流。
```typescript
async function getSupportedOutputCapability(cameraDevice: camera.CameraDevice, cameraManager: camera.CameraManager, sceneMode: camera.SceneMode): Promise<camera.CameraOutputCapability | undefined> {
// 获取相机设备支持的输出流能力
let cameraOutputCapability: camera.CameraOutputCapability = cameraManager.getSupportedOutputCapability(cameraDevice, sceneMode);
if (!cameraOutputCapability) {
console.error("cameraManager.getSupportedOutputCapability error");
return undefined;
}
console.info("outputCapability: " + JSON.stringify(cameraOutputCapability));
// 以NORMAL_PHOTO模式为例，需要添加预览流、拍照流
// previewProfiles属性为获取当前设备支持的预览输出流
let previewProfilesArray: Array<camera.Profile> = cameraOutputCapability.previewProfiles;
if (!previewProfilesArray) {
console.error("createOutput previewProfilesArray == null || undefined");
}
//photoProfiles属性为获取当前设备支持的拍照输出流
let photoProfilesArray: Array<camera.Profile> = cameraOutputCapability.photoProfiles;
if (!photoProfilesArray) {
console.error("createOutput photoProfilesArray == null || undefined");
}
return cameraOutputCapability;
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-session-management-V14
爬取时间: 2025-04-28 20:10:52
来源: Huawei Developer
相机使用预览、拍照、录像、元数据功能前，均需要创建相机会话。
在会话中，可以完成以下功能：
-  配置相机的输入流和输出流。相机在拍摄前，必须完成输入输出流的配置。 配置输入流即添加设备输入，对用户而言，相当于选择设备的某一摄像头拍摄；配置输出流，即选择数据将以什么形式输出。当应用需要实现拍照时，输出流应配置为预览流和拍照流，预览流的数据将显示在XComponent组件上，拍照流的数据将通过ImageReceiver接口的能力保存到相册中。
-  添加闪光灯、调整焦距等配置。具体支持的配置及接口说明请参考Camera API参考。
-  会话切换控制。应用可以通过移除和添加输出流的方式，切换相机模式。如当前会话的输出流为拍照流，应用可以将拍照流移除，然后添加视频流作为输出流，即完成了拍照到录像的切换。
完成会话配置后，应用提交和开启会话，可以开始调用相机相关功能。
开发步骤
1.  导入相关接口，导入方法如下。
```typescript
import { camera } from '@kit.CameraKit';
import { BusinessError } from '@kit.BasicServicesKit';
```
2.  调用cameraManager类中的createSession方法创建一个会话。
```typescript
function getSession(cameraManager: camera.CameraManager): camera.Session | undefined {
let session: camera.Session | undefined = undefined;
try {
session = cameraManager.createSession(camera.SceneMode.NORMAL_PHOTO) as camera.PhotoSession;
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to create the session instance. error: ${JSON.stringify(err)}`);
}
return session;
}
```
3.  调用PhotoSession类中的beginConfig方法配置会话。
```typescript
function beginConfig(photoSession: camera.PhotoSession): void {
try {
photoSession.beginConfig();
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to beginConfig. error: ${JSON.stringify(err)}`);
}
}
```
4.  使能。向会话中添加相机的输入流和输出流，调用addInput添加相机的输入流；调用addOutput添加相机的输出流。以下示例代码以添加预览流previewOutput和拍照流photoOutput为例，即当前模式支持拍照和预览。 调用PhotoSession类中的commitConfig和start方法提交相关配置，并启动会话。
```typescript
async function startSession(photoSession: camera.PhotoSession, cameraInput: camera.CameraInput, previewOutput: camera.PreviewOutput, photoOutput: camera.PhotoOutput): Promise<void> {
try {
photoSession.addInput(cameraInput);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to addInput. error: ${JSON.stringify(err)}`);
}
try {
photoSession.addOutput(previewOutput);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to add previewOutput. error: ${JSON.stringify(err)}`);
}
try {
photoSession.addOutput(photoOutput);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to add photoOutput. error: ${JSON.stringify(err)}`);
}
try {
await photoSession.commitConfig();
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to commitConfig. error: ${JSON.stringify(err)}`);
}
try {
await photoSession.start();
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to start. error: ${JSON.stringify(err)}`);
}
}
```
5.  会话控制。调用PhotoSession类中的stop方法可以停止当前会话。调用removeOutput和addOutput方法可以完成会话切换控制。以下示例代码以移除拍照流photoOutput，添加视频流videoOutput为例，完成了拍照到录像的切换。
```typescript
async function switchOutput(photoSession: camera.PhotoSession, videoOutput: camera.VideoOutput, photoOutput: camera.PhotoOutput): Promise<void> {
try {
await photoSession.stop();
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to stop. error: ${JSON.stringify(err)}`);
}
try {
photoSession.beginConfig();
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to beginConfig. error: ${JSON.stringify(err)}`);
}
// 从会话中移除拍照输出流
try {
photoSession.removeOutput(photoOutput);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to remove photoOutput. error: ${JSON.stringify(err)}`);
}
// 向会话中添加视频输出流
try {
photoSession.addOutput(videoOutput);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to add videoOutput. error: ${JSON.stringify(err)}`);
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-preview-V14
爬取时间: 2025-04-28 20:11:06
来源: Huawei Developer
在开发相机应用时，需要先参考开发准备申请相关权限。
预览是启动相机后看见的画面，通常在拍照和录像前执行。
开发步骤
详细的API说明请参考Camera API参考。
1.  导入camera接口，接口中提供了相机相关的属性和方法，导入方法如下。
```typescript
import { camera } from '@kit.CameraKit';
import { BusinessError } from '@kit.BasicServicesKit';
```
2.  创建Surface。 XComponent组件为预览流提供的Surface（获取surfaceId请参考getXcomponentSurfaceId方法），而XComponent的能力由UI提供，相关介绍可参考XComponent组件参考。 预览流与录像输出流的分辨率的宽高比要保持一致，如果设置XComponent组件中的Surface显示区域宽高比为1920:1080 = 16:9，则需要预览流中的分辨率的宽高比也为16:9，如分辨率选择640:360，或960:540，或1920:1080，以此类推。
3.  通过CameraOutputCapability类中的previewProfiles属性获取当前设备支持的预览能力，返回previewProfilesArray数组 。通过createPreviewOutput方法创建预览输出流，其中，createPreviewOutput方法中的两个参数分别是previewProfilesArray数组中的第一项和步骤二中获取的surfaceId。
```typescript
function getPreviewOutput(cameraManager: camera.CameraManager, cameraOutputCapability: camera.CameraOutputCapability, surfaceId: string): camera.PreviewOutput | undefined {
let previewProfilesArray: Array<camera.Profile> = cameraOutputCapability.previewProfiles;
let previewOutput: camera.PreviewOutput | undefined = undefined;
try {
previewOutput = cameraManager.createPreviewOutput(previewProfilesArray[0], surfaceId);
} catch (error) {
let err = error as BusinessError;
console.error("Failed to create the PreviewOutput instance. error code: " + err.code);
}
return previewOutput;
}
```
4.  使能。通过Session.start方法输出预览流，接口调用失败会返回相应错误码，错误码类型参见Camera错误码。
```typescript
async function startPreviewOutput(cameraManager: camera.CameraManager, previewOutput: camera.PreviewOutput): Promise<void> {
let cameraArray: Array<camera.CameraDevice> = [];
cameraArray = cameraManager.getSupportedCameras();
if (cameraArray.length == 0) {
console.error('no camera.');
return;
}
// 获取支持的模式类型
let sceneModes: Array<camera.SceneMode> = cameraManager.getSupportedSceneModes(cameraArray[0]);
let isSupportPhotoMode: boolean = sceneModes.indexOf(camera.SceneMode.NORMAL_PHOTO) >= 0;
if (!isSupportPhotoMode) {
console.error('photo mode not support');
return;
}
let cameraInput: camera.CameraInput | undefined = undefined;
cameraInput = cameraManager.createCameraInput(cameraArray[0]);
if (cameraInput === undefined) {
console.error('cameraInput is undefined');
return;
}
// 打开相机
await cameraInput.open();
let session: camera.PhotoSession = cameraManager.createSession(camera.SceneMode.NORMAL_PHOTO) as camera.PhotoSession;
session.beginConfig();
session.addInput(cameraInput);
session.addOutput(previewOutput);
await session.commitConfig();
await session.start();
}
```
状态监听
在相机应用开发过程中，可以随时监听预览输出流状态，包括预览流启动、预览流结束、预览流输出错误。
-  通过注册固定的frameStart回调函数获取监听预览启动结果，previewOutput创建成功时即可监听，预览第一次曝光时触发，有该事件返回结果则认为预览流已启动。
```typescript
function onPreviewOutputFrameStart(previewOutput: camera.PreviewOutput): void {
previewOutput.on('frameStart', (err: BusinessError) => {
if (err !== undefined && err.code !== 0) {
return;
}
console.info('Preview frame started');
});
}
```
-  通过注册固定的frameEnd回调函数获取监听预览结束结果，previewOutput创建成功时即可监听，预览完成最后一帧时触发，有该事件返回结果则认为预览流已结束。
```typescript
function onPreviewOutputFrameEnd(previewOutput: camera.PreviewOutput): void {
previewOutput.on('frameEnd', (err: BusinessError) => {
if (err !== undefined && err.code !== 0) {
return;
}
console.info('Preview frame ended');
});
}
```
-  通过注册固定的error回调函数获取监听预览输出错误结果，回调返回预览输出接口使用错误时对应的错误码，错误码类型参见Camera错误码。
```typescript
function onPreviewOutputError(previewOutput: camera.PreviewOutput): void {
previewOutput.on('error', (previewOutputError: BusinessError) => {
console.error(`Preview output error code: ${previewOutputError.code}`);
});
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-shooting-V14
爬取时间: 2025-04-28 20:11:20
来源: Huawei Developer
拍照是相机的最重要功能之一，拍照模块基于相机复杂的逻辑，为了保证用户拍出的照片质量，在中间步骤可以设置分辨率、闪光灯、焦距、照片质量及旋转角度等信息。
开发步骤
详细的API说明请参考Camera API参考。
1.  导入image接口。创建拍照输出流的SurfaceId以及拍照输出的数据，都需要用到系统提供的image接口能力，导入image接口的方法如下。
```typescript
import { image } from '@kit.ImageKit';
import { camera } from '@kit.CameraKit';
import { fileIo as fs } from '@kit.CoreFileKit';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
import { BusinessError } from '@kit.BasicServicesKit';
```
2.  创建拍照输出流。 通过CameraOutputCapability类中的photoProfiles属性，可获取当前设备支持的拍照输出流，通过createPhotoOutput方法传入支持的某一个输出流及步骤一获取的SurfaceId创建拍照输出流。
```typescript
function getPhotoOutput(cameraManager: camera.CameraManager, cameraOutputCapability: camera.CameraOutputCapability): camera.PhotoOutput | undefined {
let photoProfilesArray: Array<camera.Profile> = cameraOutputCapability.photoProfiles;
if (!photoProfilesArray) {
console.error("createOutput photoProfilesArray == null || undefined");
}
let photoOutput: camera.PhotoOutput | undefined = undefined;
try {
photoOutput = cameraManager.createPhotoOutput(photoProfilesArray[0]);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to createPhotoOutput. error: ${JSON.stringify(err)}`);
}
return photoOutput;
}
```
3.  设置拍照photoAvailable的回调，并将拍照的buffer保存为图片。 Context获取方式请参考：获取UIAbility的上下文信息。 如需要在图库中看到所保存的图片、视频资源，需要将其保存到媒体库，保存方式请参考：保存媒体库资源。 需要在photoOutput.on('photoAvailable')接口获取到buffer时，将buffer在安全控件中保存到媒体库。
```typescript
let context = getContext(this);
function setPhotoOutputCb(photoOutput: camera.PhotoOutput) {
//设置回调之后，调用photoOutput的capture方法，就会将拍照的buffer回传到回调中
photoOutput.on('photoAvailable', (errCode: BusinessError, photo: camera.Photo): void => {
console.info('getPhoto start');
console.info(`err: ${JSON.stringify(errCode)}`);
if (errCode || photo === undefined) {
console.error('getPhoto failed');
return;
}
let imageObj: image.Image = photo.main;
imageObj.getComponent(image.ComponentType.JPEG, (errCode: BusinessError, component: image.Component): void => {
console.info('getComponent start');
if (errCode || component === undefined) {
console.error('getComponent failed');
return;
}
let buffer: ArrayBuffer;
if (component.byteBuffer) {
buffer = component.byteBuffer;
} else {
console.error('byteBuffer is null');
return;
}
// 如需要在图库中看到所保存的图片、视频资源，请使用用户无感的安全控件创建媒体资源。
// buffer处理结束后需要释放该资源，如果未正确释放资源会导致后续拍照获取不到buffer
imageObj.release();
});
});
}
```
4.  参数配置。 配置相机的参数可以调整拍照的一些功能，包括闪光灯、变焦、焦距等。
```typescript
function configuringSession(photoSession: camera.PhotoSession): void {
// 判断设备是否支持闪光灯
let flashStatus: boolean = false;
try {
flashStatus = photoSession.hasFlash();
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to hasFlash. error: ${JSON.stringify(err)}`);
}
console.info(`Returned with the flash light support status: ${flashStatus}`);
if (flashStatus) {
// 判断是否支持自动闪光灯模式
let flashModeStatus: boolean = false;
try {
let status: boolean = photoSession.isFlashModeSupported(camera.FlashMode.FLASH_MODE_AUTO);
flashModeStatus = status;
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to check whether the flash mode is supported. error: ${JSON.stringify(err)}`);
}
if (flashModeStatus) {
// 设置自动闪光灯模式
try {
photoSession.setFlashMode(camera.FlashMode.FLASH_MODE_AUTO);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to set the flash mode. error: ${JSON.stringify(err)}`);
}
}
}
// 判断是否支持连续自动变焦模式
let focusModeStatus: boolean = false;
try {
let status: boolean = photoSession.isFocusModeSupported(camera.FocusMode.FOCUS_MODE_CONTINUOUS_AUTO);
focusModeStatus = status;
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to check whether the focus mode is supported. error: ${JSON.stringify(err)}`);
}
if (focusModeStatus) {
// 设置连续自动变焦模式
try {
photoSession.setFocusMode(camera.FocusMode.FOCUS_MODE_CONTINUOUS_AUTO);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to set the focus mode. error: ${JSON.stringify(err)}`);
}
}
// 获取相机支持的可变焦距比范围
let zoomRatioRange: Array<number> = [];
try {
zoomRatioRange = photoSession.getZoomRatioRange();
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to get the zoom ratio range. error: ${JSON.stringify(err)}`);
}
if (zoomRatioRange.length <= 0 ) {
return;
}
// 设置可变焦距比
try {
photoSession.setZoomRatio(zoomRatioRange[0]);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to set the zoom ratio value. error: ${JSON.stringify(err)}`);
}
}
```
5.  触发拍照。 通过photoOutput类的capture方法，执行拍照任务。该方法有两个参数，第一个参数为拍照设置参数的setting，setting中可以设置照片的质量和旋转角度，第二参数为回调函数。 获取拍照旋转角度的方法为，通过通过PhotoOutput类中的getPhotoRotation方法获取rotation实际的值
```typescript
function capture(captureLocation: camera.Location, photoOutput: camera.PhotoOutput): void {
let settings: camera.PhotoCaptureSetting = {
quality: camera.QualityLevel.QUALITY_LEVEL_HIGH,  // 设置图片质量高
rotation: camera.ImageRotation.ROTATION_0,  // 设置图片旋转角度的camera.ImageRotation.ROTATION_0是通过说明中获取拍照角度的getPhotoRotation方法获取的值进行设置
location: captureLocation,  // 设置图片地理位置
mirror: false  // 设置镜像使能开关(默认关)
};
photoOutput.capture(settings, (err: BusinessError) => {
if (err) {
console.error(`Failed to capture the photo. error: ${JSON.stringify(err)}`);
return;
}
console.info('Callback invoked to indicate the photo capture request success.');
});
}
```
状态监听
在相机应用开发过程中，可以随时监听拍照输出流状态，包括拍照流开始、拍照帧的开始与结束、拍照输出流的错误。
-  通过注册固定的captureStart回调函数获取监听拍照开始结果，photoOutput创建成功时即可监听，相机设备已经准备开始这次拍照时触发，该事件返回此次拍照的captureId。
```typescript
function onPhotoOutputCaptureStart(photoOutput: camera.PhotoOutput): void {
photoOutput.on('captureStartWithInfo', (err: BusinessError, captureStartInfo: camera.CaptureStartInfo) => {
if (err !== undefined && err.code !== 0) {
return;
}
console.info(`photo capture started, captureId : ${captureStartInfo.captureId}`);
});
}
```
-  通过注册固定的captureEnd回调函数获取监听拍照结束结果，photoOutput创建成功时即可监听，该事件返回结果为拍照完全结束后的相关信息CaptureEndInfo。
```typescript
function onPhotoOutputCaptureEnd(photoOutput: camera.PhotoOutput): void {
photoOutput.on('captureEnd', (err: BusinessError, captureEndInfo: camera.CaptureEndInfo) => {
if (err !== undefined && err.code !== 0) {
return;
}
console.info(`photo capture end, captureId : ${captureEndInfo.captureId}`);
console.info(`frameCount : ${captureEndInfo.frameCount}`);
});
}
```
-  通过注册固定的captureReady回调函数获取监听可拍下一张结果，photoOutput创建成功时即可监听，当下一张可拍时触发，该事件返回结果为下一张可拍的相关信息。
```typescript
function onPhotoOutputCaptureReady(photoOutput: camera.PhotoOutput): void {
photoOutput.on('captureReady', (err: BusinessError) => {
if (err !== undefined && err.code !== 0) {
return;
}
console.info(`photo capture ready`);
});
}
```
-  通过注册固定的error回调函数获取监听拍照输出流的错误结果。回调返回拍照输出接口使用错误时的对应错误码，错误码类型参见Camera错误码。
```typescript
function onPhotoOutputError(photoOutput: camera.PhotoOutput): void {
photoOutput.on('error', (error: BusinessError) => {
console.error(`Photo output error code: ${error.code}`);
});
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-recording-V14
爬取时间: 2025-04-28 20:11:33
来源: Huawei Developer
录像也是相机应用的最重要功能之一，录像是循环帧的捕获。对于录像的流畅度，开发者可以参考拍照中的步骤4，设置分辨率、闪光灯、焦距、照片质量及旋转角度等信息。
开发步骤
详细的API说明请参考Camera API参考。
1.  导入media模块。 创建录像输出流的SurfaceId以及录像输出的数据，都需要用到系统提供的media接口能力，导入media接口的方法如下。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
import { camera } from '@kit.CameraKit';
import { media } from '@kit.MediaKit';
```
2.  创建Surface。 系统提供的media接口可以创建一个录像AVRecorder实例，通过该实例的getInputSurface方法获取SurfaceId，与录像输出流做关联，处理录像输出流输出的数据。
```typescript
async function getVideoSurfaceId(aVRecorderConfig: media.AVRecorderConfig): Promise<string | undefined> {  // aVRecorderConfig可参考下一章节
let avRecorder: media.AVRecorder | undefined = undefined;
try {
avRecorder = await media.createAVRecorder();
} catch (error) {
let err = error as BusinessError;
console.error(`createAVRecorder call failed. error code: ${err.code}`);
}
if (avRecorder === undefined) {
return undefined;
}
avRecorder.prepare(aVRecorderConfig, (err: BusinessError) => {
if (err == null) {
console.info('prepare success');
} else {
console.error('prepare failed and error is ' + err.message);
}
});
let videoSurfaceId = await avRecorder.getInputSurface();
return videoSurfaceId;
}
```
3.  创建录像输出流。 通过CameraOutputCapability类中的videoProfiles属性，可获取当前设备支持的录像输出流。然后，定义创建录像的参数，通过createVideoOutput方法创建录像输出流。 预览流与录像输出流的分辨率的宽高比要保持一致，如示例代码中宽高比为640:480 = 4:3，则需要预览流中的分辨率的宽高比也为4:3，如分辨率选择640:480，或960:720，或1440:1080，以此类推 获取录像旋转角度的方法：通过VideoOutput类中的getVideoRotation方法获取rotation实际的值
```typescript
async function getVideoOutput(cameraManager: camera.CameraManager, videoSurfaceId: string, cameraOutputCapability: camera.CameraOutputCapability): Promise<camera.VideoOutput | undefined> {
let videoProfilesArray: Array<camera.VideoProfile> = cameraOutputCapability.videoProfiles;
if (!videoProfilesArray) {
console.error("createOutput videoProfilesArray == null || undefined");
return undefined;
}
// AVRecorderProfile
let aVRecorderProfile: media.AVRecorderProfile = {
fileFormat : media.ContainerFormatType.CFT_MPEG_4, // 视频文件封装格式，只支持MP4
videoBitrate : 100000, // 视频比特率
videoCodec : media.CodecMimeType.VIDEO_AVC, // 视频文件编码格式，支持avc格式
videoFrameWidth : 640,  // 视频分辨率的宽
videoFrameHeight : 480, // 视频分辨率的高
videoFrameRate : 30 // 视频帧率
};
// 创建视频录制的参数，预览流与录像输出流的分辨率的宽(videoFrameWidth)高(videoFrameHeight)比要保持一致
let aVRecorderConfig: media.AVRecorderConfig = {
videoSourceType: media.VideoSourceType.VIDEO_SOURCE_TYPE_SURFACE_YUV,
profile: aVRecorderProfile,
url: 'fd://35',
rotation: 90 // rotation的值90，是通过getPhotoRotation接口获取到的值，具体请参考说明中获取录像旋转角度的方法
};
// 创建avRecorder
let avRecorder: media.AVRecorder | undefined = undefined;
try {
avRecorder = await media.createAVRecorder();
} catch (error) {
let err = error as BusinessError;
console.error(`createAVRecorder call failed. error code: ${err.code}`);
}
if (avRecorder === undefined) {
return undefined;
}
// 设置视频录制的参数
avRecorder.prepare(aVRecorderConfig);
// 创建VideoOutput对象
let videoOutput: camera.VideoOutput | undefined = undefined;
// createVideoOutput传入的videoProfile对象的宽高需要和aVRecorderProfile保持一致。
let videoProfile: undefined | camera.VideoProfile = videoProfilesArray.find((profile: camera.VideoProfile) => {
return profile.size.width === aVRecorderProfile.videoFrameWidth && profile.size.height === aVRecorderProfile.videoFrameHeight;
});
if (!videoProfile) {
console.error('videoProfile is not found');
return;
}
try {
videoOutput = cameraManager.createVideoOutput(videoProfile, videoSurfaceId);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to create the videoOutput instance. errorCode = ' + err.code);
}
return videoOutput;
}
```
4.  开始录像。 先通过videoOutput的start方法启动录像输出流，再通过avRecorder的start方法开始录像。
```typescript
async function startVideo(videoOutput: camera.VideoOutput, avRecorder: media.AVRecorder): Promise<void> {
videoOutput.start(async (err: BusinessError) => {
if (err) {
console.error(`Failed to start the video output ${err.message}`);
return;
}
console.info('Callback invoked to indicate the video output start success.');
});
try {
await avRecorder.start();
} catch (error) {
let err = error as BusinessError;
console.error(`avRecorder start error: ${JSON.stringify(err)}`);
}
}
```
5.  停止录像。 先通过avRecorder的stop方法停止录像，再通过videoOutput的stop方法停止录像输出流。
```typescript
async function stopVideo(videoOutput: camera.VideoOutput, avRecorder: media.AVRecorder): Promise<void> {
try {
await avRecorder.stop();
} catch (error) {
let err = error as BusinessError;
console.error(`avRecorder stop error: ${JSON.stringify(err)}`);
}
videoOutput.stop((err: BusinessError) => {
if (err) {
console.error(`Failed to stop the video output ${err.message}`);
return;
}
console.info('Callback invoked to indicate the video output stop success.');
});
}
```
状态监听
在相机应用开发过程中，可以随时监听录像输出流状态，包括录像开始、录像结束、录像流输出的错误。
-  通过注册固定的frameStart回调函数获取监听录像开始结果，videoOutput创建成功时即可监听，录像第一次曝光时触发，有该事件返回结果则认为录像开始。
```typescript
function onVideoOutputFrameStart(videoOutput: camera.VideoOutput): void {
videoOutput.on('frameStart', (err: BusinessError) => {
if (err !== undefined && err.code !== 0) {
return;
}
console.info('Video frame started');
});
}
```
-  通过注册固定的frameEnd回调函数获取监听录像结束结果，videoOutput创建成功时即可监听，录像完成最后一帧时触发，有该事件返回结果则认为录像流已结束。
```typescript
function onVideoOutputFrameEnd(videoOutput: camera.VideoOutput): void {
videoOutput.on('frameEnd', (err: BusinessError) => {
if (err !== undefined && err.code !== 0) {
return;
}
console.info('Video frame ended');
});
}
```
-  通过注册固定的error回调函数获取监听录像输出错误结果，callback返回预览输出接口使用错误时对应的错误码，错误码类型参见Camera错误码。
```typescript
function onVideoOutputError(videoOutput: camera.VideoOutput): void {
videoOutput.on('error', (error: BusinessError) => {
console.error(`Video output error code: ${error.code}`);
});
}
```
示例代码

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-metadata-V14
爬取时间: 2025-04-28 20:11:47
来源: Huawei Developer
在开发相机应用时，需要先参考开发准备申请相关权限。
元数据（Metadata）是对相机返回的图像信息数据的描述和上下文，针对图像信息，提供的更详细的数据，如照片或视频中，识别人像的取景框坐标等信息。
Metadata主要是通过一个TAG（Key），去找对应的Data，用于传递参数和配置信息，减少内存拷贝操作。
开发步骤
详细的API说明请参考Camera API参考。
1.  导入相关接口，导入方法如下。
```typescript
import { camera } from '@kit.CameraKit';
import { BusinessError } from '@kit.BasicServicesKit';
```
2.  调用CameraOutputCapability类中的supportedMetadataObjectTypes属性，获取当前设备支持的元数据类型，并通过createMetadataOutput方法创建元数据输出流。
```typescript
function getMetadataOutput(cameraManager: camera.CameraManager, cameraOutputCapability: camera.CameraOutputCapability): camera.MetadataOutput | undefined {
let metadataObjectTypes: Array<camera.MetadataObjectType> = cameraOutputCapability.supportedMetadataObjectTypes;
let metadataOutput: camera.MetadataOutput | undefined = undefined;
try {
metadataOutput = cameraManager.createMetadataOutput(metadataObjectTypes);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to createMetadataOutput, error code: ${err.code}`);
}
return metadataOutput;
}
```
3.  调用Session.start方法开启metadata数据输出，再通过监听事件metadataObjectsAvailable回调拿到数据，接口调用失败时，会返回相应错误码，错误码类型参见Camera错误码。 previewOutput获取方式请参考相机预览开发步骤。
```typescript
async function startMetadataOutput(previewOutput: camera.PreviewOutput, metadataOutput: camera.MetadataOutput, cameraManager: camera.CameraManager): Promise<void> {
let cameraArray: Array<camera.CameraDevice> = [];
cameraArray = cameraManager.getSupportedCameras();
if (cameraArray.length == 0) {
console.error('no camera.');
return;
}
// 获取支持的模式类型
let sceneModes: Array<camera.SceneMode> = cameraManager.getSupportedSceneModes(cameraArray[0]);
let isSupportPhotoMode: boolean = sceneModes.indexOf(camera.SceneMode.NORMAL_PHOTO) >= 0;
if (!isSupportPhotoMode) {
console.error('photo mode not support');
return;
}
let cameraInput: camera.CameraInput | undefined = undefined;
cameraInput = cameraManager.createCameraInput(cameraArray[0]);
if (cameraInput === undefined) {
console.error('cameraInput is undefined');
return;
}
// 打开相机
await cameraInput.open();
let session: camera.PhotoSession = cameraManager.createSession(camera.SceneMode.NORMAL_PHOTO) as camera.PhotoSession;
session.beginConfig();
session.addInput(cameraInput);
session.addOutput(previewOutput);
session.addOutput(metadataOutput);
await session.commitConfig();
await session.start();
}
```
4.  调用Session.stop方法停止输出metadata数据，接口调用失败会返回相应错误码，错误码类型参见Camera错误码。
```typescript
function stopMetadataOutput(session: camera.Session): void {
session.stop().then(() => {
console.info('Callback returned with session stopped.');
}).catch((err: BusinessError) => {
console.error(`Failed to session stop, error code: ${err.code}`);
});
}
```
状态监听
在相机应用开发过程中，可以随时监听metadata数据以及输出流的状态。
-  通过注册监听获取metadata对象，监听事件固定为metadataObjectsAvailable。检测到有效metadata数据时，callback返回相应的metadata数据信息，metadataOutput创建成功时可监听。 当前的元数据类型仅支持人脸检测（FACE_DETECTION）功能。元数据信息对象为识别到的人脸区域的矩形信息（Rect），包含矩形区域的左上角x坐标、y坐标和矩形的宽高数据。
```typescript
function onMetadataObjectsAvailable(metadataOutput: camera.MetadataOutput): void {
metadataOutput.on('metadataObjectsAvailable', (err: BusinessError, metadataObjectArr: Array<camera.MetadataObject>) => {
if (err !== undefined && err.code !== 0) {
return;
}
console.info('metadata output metadataObjectsAvailable');
});
}
```
-  通过注册回调函数，获取监听metadata流的错误结果，callback返回metadata输出接口使用错误时返回的错误码，错误码类型参见Camera错误码。
```typescript
function onMetadataError(metadataOutput: camera.MetadataOutput): void {
metadataOutput.on('error', (metadataOutputError: BusinessError) => {
console.error(`Metadata output error code: ${metadataOutputError.code}`);
});
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-focus-V14
爬取时间: 2025-04-28 20:12:01
来源: Huawei Developer
相机框架提供对设备对焦的能力，业务应用可以根据使用场景进行对焦模式和对焦点的设置。
开发步骤
详细的API说明请参考Camera API参考。
1.  导入相关接口，导入方法如下。
```typescript
import { camera } from '@kit.CameraKit';
import { BusinessError } from '@kit.BasicServicesKit';
```
2.  在设置对焦模式前，需要先调用isFocusModeSupported检查设备是否支持指定的焦距模式。 需要在Session调用commitConfig完成配流之后调用。
```typescript
function isFocusModeSupported(photoSession: camera.PhotoSession): boolean {
let status: boolean = false;
try {
// 以检查是否支持连续自动对焦模式为例
status = photoSession.isFocusModeSupported(camera.FocusMode.FOCUS_MODE_CONTINUOUS_AUTO);
} catch (error) {
// 失败返回错误码error.code并处理
let err = error as BusinessError;
console.error(`The isFocusModeSupported call failed. error code: ${err.code}`);
}
return status;
}
```
3.  调用setFocusMode设置对焦模式。 需要在Session调用commitConfig完成配流之后调用。
```typescript
function setFocusMode(photoSession: camera.PhotoSession): void {
const focusPoint: camera.Point = {x: 1, y: 1};
try {
// 设置自动对焦模式
photoSession.setFocusMode(camera.FocusMode.FOCUS_MODE_AUTO);
// 设置对焦点
photoSession.setFocusPoint(focusPoint);
} catch (error) {
// 失败返回错误码error.code并处理
let err = error as BusinessError;
console.error(`The setFocusMode and setFocusPoint call failed. error code: ${err.code}`);
}
}
```
状态监听
在相机应用开发过程中，可以随时监听相机聚焦的状态变化。
通过注册focusStateChange的回调函数获取监听结果，仅当自动对焦模式时，且相机对焦状态发生改变时触发该事件。
```typescript
function onFocusStateChange(photoSession: camera.PhotoSession): void {
photoSession.on('focusStateChange', (err: BusinessError, focusState: camera.FocusState) => {
if (err !== undefined && err.code !== 0) {
console.error(`focusStateChange error code: ${err.code}`);
return;
}
console.info(`focusStateChange focusState: ${focusState}`);
// 为保证对焦功能的用户体验，在自动对焦成功后，可将对焦模式设置为连续自动对焦
if (focusState === camera.FocusState.FOCUS_STATE_FOCUSED) {
photoSession.setFocusMode(camera.FocusMode.FOCUS_MODE_CONTINUOUS_AUTO);
}
});
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-torch-use-V14
爬取时间: 2025-04-28 20:12:14
来源: Huawei Developer
手电筒模式的使用是通过操作手机启用手电筒功能，使设备的手电筒功能持续保持常亮状态。
在使用相机应用并操作手电筒功能时，存在以下几种情况说明：
开发步骤
详细的API说明请参考Camera API参考。
1.  导入camera接口，接口中提供了相机相关的属性和方法，导入方法如下。
```typescript
import { camera } from '@kit.CameraKit';
import { BusinessError } from '@kit.BasicServicesKit';
```
2.  通过CameraManager类中的isTorchSupported方法，检测当前设备是否支持手电筒功能。
```typescript
function isTorchSupported(cameraManager: camera.CameraManager) : boolean {
let torchSupport: boolean = false;
try {
torchSupport = cameraManager.isTorchSupported();
} catch (error) {
let err = error as BusinessError;
console.error('Failed to torch. errorCode = ' + err.code);
}
console.info('Returned with the torch support status:' + torchSupport);
return torchSupport;
}
```
3.  通过CameraManager类中的isTorchModeSupported方法，检测是否支持指定的手电筒模式TorchMode。
```typescript
function isTorchModeSupported(cameraManager: camera.CameraManager, torchMode: camera.TorchMode) : boolean {
let isTorchModeSupport: boolean = false;
try {
isTorchModeSupport = cameraManager.isTorchModeSupported(torchMode);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to set the torch mode. errorCode = ' + err.code);
}
return isTorchModeSupport;
}
```
4.  通过CameraManager类中的setTorchMode方法，设置当前设备的手电筒模式。以及通过CameraManager类中的getTorchMode方法，获取当前设备的手电筒模式。 在使用getTorchMode方法前，需要先注册监听手电筒的状态变化，请参考状态监听。
```typescript
function setTorchModeSupported(cameraManager: camera.CameraManager, torchMode: camera.TorchMode) : void {
cameraManager.setTorchMode(torchMode);
let isTorchMode = cameraManager.getTorchMode();
console.info(`Returned with the torch mode supportd mode: ${isTorchMode}`);
}
```
状态监听
在相机应用开发过程中，可以随时监听手电筒状态，包括手电筒打开、手电筒关闭、手电筒不可用、手电筒恢复可用。手电筒状态发生变化，可通过回调函数获取手电筒模式的变化。
通过注册torchStatusChange事件，通过回调返回监听结果，callback返回TorchStatusInfo参数，参数的具体内容可参考相机管理器回调接口实例TorchStatusInfo。
```typescript
function onTorchStatusChange(cameraManager: camera.CameraManager): void {
cameraManager.on('torchStatusChange', (err: BusinessError, torchStatusInfo: camera.TorchStatusInfo) => {
if (err !== undefined && err.code !== 0) {
console.error(`Callback Error, errorCode: ${err.code}`);
return;
}
console.info(`onTorchStatusChange, isTorchAvailable: ${torchStatusInfo.isTorchAvailable}, isTorchActive: ${torchStatusInfo.
isTorchActive}, level: ${torchStatusInfo.torchLevel}`);
});
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-foldable-display-V14
爬取时间: 2025-04-28 20:12:28
来源: Huawei Developer
在开发相机应用时，需要先参考开发准备申请相关权限。
一台可折叠设备在不同折叠状态下，可使用不同的摄像头，应用可调用CameraManager.on('foldStatusChange')或display.on('foldStatusChange')监听设备的折叠状态变化，并调用CameraManager.getSupportedCameras获取当前状态下可用摄像头，完成相应适配，确保应用在折叠状态变更时的用户体验。
详细的API说明请参考Camera API参考。
创建XComponent
使用两个XComponent分别展示折叠态和展开态，防止切换折叠屏状态亮屏的时候上一个摄像头还未关闭，残留上一个摄像头的画面。
```typescript
@Entry
@Component
struct Index {
@State reloadXComponentFlag: boolean = false;
@StorageLink('foldStatus') @Watch('reloadXComponent') foldStatus: number = 0;
private mXComponentController: XComponentController = new XComponentController();
private mXComponentOptions: XComponentOptions = {
type: XComponentType.SURFACE,
controller: this.mXComponentController
}
reloadXComponent() {
this.reloadXComponentFlag = !this.reloadXComponentFlag;
}
async loadXComponent() {
//初始化XComponent
}
build() {
Stack() {
if (this.reloadXComponentFlag) {
XComponent(this.mXComponentOptions)
.onLoad(async () => {
await this.loadXComponent();
})
.width(px2vp(1080))
.height(px2vp(1920))
} else {
XComponent(this.mXComponentOptions)
.onLoad(async () => {
await this.loadXComponent();
})
.width(px2vp(1080))
.height(px2vp(1920))
}
}
.size({ width: '100%', height: '100%' })
.backgroundColor(Color.Black)
}
}
```
获取设备折叠状态
此处提供两种方案供开发者选择。
```typescript
import { camera } from '@kit.CameraKit';
import { BusinessError } from '@kit.BasicServicesKit';
let cameraManager = camera.getCameraManager(getContext())
function registerFoldStatusChanged(err: BusinessError, foldStatusInfo: camera.FoldStatusInfo) {
// foldStatus 变量用来控制显示XComponent组件
AppStorage.setOrCreate<number>('foldStatus', foldStatusInfo.foldStatus);
}
cameraManager.on('foldStatusChange', registerFoldStatusChanged);
//cameraManager.off('foldStatusChange', registerFoldStatusChanged);
```
```typescript
import { display } from '@kit.ArkUI';
let preFoldStatus: display.FoldStatus = display.getFoldStatus();
display.on('foldStatusChange', (foldStatus: display.FoldStatus) => {
// 从半折叠态（FOLD_STATUS_HALF_FOLDED）和展开态（FOLD_STATUS_EXPANDED），相机框架返回所支持的摄像头是一致的，所以从半折叠态到展开态不需要重新配流，从展开态到半折叠态也是一样的
if ((preFoldStatus === display.FoldStatus.FOLD_STATUS_HALF_FOLDED &&
foldStatus === display.FoldStatus.FOLD_STATUS_EXPANDED) ||
(preFoldStatus === display.FoldStatus.FOLD_STATUS_EXPANDED &&
foldStatus === display.FoldStatus.FOLD_STATUS_HALF_FOLDED)) {
preFoldStatus = foldStatus;
return;
}
preFoldStatus = foldStatus;
// foldStatus 变量用来控制显示XComponent组件
AppStorage.setOrCreate<number>('foldStatus', foldStatus);
})
```
完整示例
```typescript
import { camera } from '@kit.CameraKit';
import { BusinessError } from '@kit.BasicServicesKit';
import { abilityAccessCtrl } from '@kit.AbilityKit';
import { display } from '@kit.ArkUI';
let context = getContext(this);
const TAG = 'FoldScreenCameraAdaptationDemo ';
@Entry
@Component
struct Index {
@State isShow: boolean = false;
@State reloadXComponentFlag: boolean = false;
@StorageLink('foldStatus') @Watch('reloadXComponent') foldStatus: number = 0;
private mXComponentController: XComponentController = new XComponentController();
private mXComponentOptions: XComponentOptions = {
type: XComponentType.SURFACE,
controller: this.mXComponentController
}
private mSurfaceId: string = '';
private mCameraPosition: camera.CameraPosition = camera.CameraPosition.CAMERA_POSITION_BACK;
private mCameraManager: camera.CameraManager = camera.getCameraManager(context);
// surface宽高根据需要自行选择
private surfaceRect: SurfaceRect = {
surfaceWidth: 1080,
surfaceHeight: 1920
};
private curCameraDevice: camera.CameraDevice | undefined = undefined;
private mCameraInput: camera.CameraInput | undefined = undefined;
private mPreviewOutput: camera.PreviewOutput | undefined = undefined;
private mPhotoSession: camera.PhotoSession | undefined = undefined;
// One of the recommended preview resolutions
private previewProfileObj: camera.Profile = {
format: 1003,
size: {
width: 1920,
height: 1080
}
};
private preFoldStatus: display.FoldStatus = display.getFoldStatus();
// 监听折叠屏状态，可以使用cameraManager.on(type: 'foldStatusChange', callback: AsyncCallback<FoldStatusInfo>): void;
// 也可以使用display.on(type: 'foldStatusChange', callback: Callback<FoldStatus>): void;
private foldStatusCallback =
(err: BusinessError, info: camera.FoldStatusInfo): void => this.registerFoldStatusChanged(err, info);
private displayFoldStatusCallback =
(foldStatus: display.FoldStatus): void => this.onDisplayFoldStatusChange(foldStatus);
registerFoldStatusChanged(err: BusinessError, foldStatusInfo: camera.FoldStatusInfo) {
console.info(TAG + 'foldStatusChanged foldStatus: ' + foldStatusInfo.foldStatus);
for (let i = 0; i < foldStatusInfo.supportedCameras.length; i++) {
console.info(TAG +
`foldStatusChanged camera[${i}]: ${foldStatusInfo.supportedCameras[i].cameraId},cameraPosition: ${foldStatusInfo.supportedCameras[i].cameraPosition}`);
}
AppStorage.setOrCreate<number>('foldStatus', foldStatusInfo.foldStatus);
}
onDisplayFoldStatusChange(foldStatus: display.FoldStatus): void {
console.error(TAG + `onDisplayFoldStatusChange foldStatus: ${foldStatus}`);
if ((this.preFoldStatus === display.FoldStatus.FOLD_STATUS_HALF_FOLDED &&
foldStatus === display.FoldStatus.FOLD_STATUS_EXPANDED) ||
(this.preFoldStatus === display.FoldStatus.FOLD_STATUS_EXPANDED &&
foldStatus === display.FoldStatus.FOLD_STATUS_HALF_FOLDED)) {
this.preFoldStatus = foldStatus;
return;
}
this.preFoldStatus = foldStatus;
// 获取当前打开的相机摄像头，如果是后置，折叠状态不影响当前摄像头的使用
if (!this.curCameraDevice) {
return;
}
// foldStatus 变量用来控制显示XComponent组件
AppStorage.setOrCreate<number>('foldStatus', foldStatus);
}
requestPermissionsFn(): void {
let atManager = abilityAccessCtrl.createAtManager();
atManager.requestPermissionsFromUser(context, [
'ohos.permission.CAMERA'
]).then((): void => {
this.isShow = true;
}).catch((error: BusinessError): void => {
console.error(TAG + 'ohos.permission.CAMERA no permission.');
});
}
aboutToAppear(): void {
console.log(TAG + 'aboutToAppear is called');
this.requestPermissionsFn();
this.onFoldStatusChange();
}
async aboutToDisappear(): Promise<void> {
await this.releaseCamera();
// 解注册
this.offFoldStatusChange();
}
async onPageShow(): Promise<void> {
await this.initCamera(this.mSurfaceId, this.mCameraPosition);
}
async releaseCamera(): Promise<void> {
// 停止当前会话
try {
await this.mPhotoSession?.stop();
} catch (error) {
let err = error as BusinessError;
console.error(TAG + 'Failed to stop session, errorCode = ' + err.code);
}
// 释放相机输入流
try {
await this.mCameraInput?.close();
} catch (error) {
let err = error as BusinessError;
console.error(TAG + 'Failed to close device, errorCode = ' + err.code);
}
// 释放预览输出流
try {
await this.mPreviewOutput?.release();
} catch (error) {
let err = error as BusinessError;
console.error(TAG + 'Failed to release previewOutput, errorCode = ' + err.code);
}
this.mPreviewOutput = undefined;
// 释放会话
try {
await this.mPhotoSession?.release();
} catch (error) {
let err = error as BusinessError;
console.error(TAG + 'Failed to release photoSession, errorCode = ' + err.code);
}
// 会话置空
this.mPhotoSession = undefined;
}
onFoldStatusChange(): void {
this.mCameraManager.on('foldStatusChange', this.foldStatusCallback);
// display.on('foldStatusChange', this.displayFoldStatusCallback);
}
offFoldStatusChange(): void {
this.mCameraManager.off('foldStatusChange', this.foldStatusCallback);
// display.off('foldStatusChange', this.displayFoldStatusCallback);
}
reloadXComponent(): void {
this.reloadXComponentFlag = !this.reloadXComponentFlag;
}
async loadXComponent(): Promise<void> {
this.mSurfaceId = this.mXComponentController.getXComponentSurfaceId();
this.mXComponentController.setXComponentSurfaceRect(this.surfaceRect);
console.info(TAG + `mCameraPosition: ${this.mCameraPosition}`)
await this.initCamera(this.mSurfaceId, this.mCameraPosition);
}
getPreviewProfile(cameraOutputCapability: camera.CameraOutputCapability): camera.Profile | undefined {
let previewProfiles = cameraOutputCapability.previewProfiles;
if (previewProfiles.length < 1) {
return undefined;
}
let index = previewProfiles.findIndex((previewProfile: camera.Profile) => {
return previewProfile.size.width === this.previewProfileObj.size.width &&
previewProfile.size.height === this.previewProfileObj.size.height &&
previewProfile.format === this.previewProfileObj.format;
})
if (index === -1) {
return undefined;
}
return previewProfiles[index];
}
async initCamera(surfaceId: string, cameraPosition: camera.CameraPosition): Promise<void> {
await this.releaseCamera();
// 创建CameraManager对象
if (!this.mCameraManager) {
console.error(TAG + 'camera.getCameraManager error');
return;
}
// 获取相机列表
let cameraArray: Array<camera.CameraDevice> = this.mCameraManager.getSupportedCameras();
if (cameraArray.length <= 0) {
console.error(TAG + 'cameraManager.getSupportedCameras error');
return;
}
for (let index = 0; index < cameraArray.length; index++) {
console.info(TAG + 'cameraId : ' + cameraArray[index].cameraId); // 获取相机ID
console.info(TAG + 'cameraPosition : ' + cameraArray[index].cameraPosition); // 获取相机位置
console.info(TAG + 'cameraType : ' + cameraArray[index].cameraType); // 获取相机类型
console.info(TAG + 'connectionType : ' + cameraArray[index].connectionType); // 获取相机连接类型
}
let deviceIndex = cameraArray.findIndex((cameraDevice: camera.CameraDevice) => {
return cameraDevice.cameraPosition === cameraPosition;
})
if (deviceIndex === -1) {
deviceIndex = 0;
console.error(TAG + 'not found camera');
}
this.curCameraDevice = cameraArray[deviceIndex];
// 创建相机输入流
try {
this.mCameraInput = this.mCameraManager.createCameraInput(this.curCameraDevice);
} catch (error) {
let err = error as BusinessError;
console.error(TAG + 'Failed to createCameraInput errorCode = ' + err.code);
}
if (this.mCameraInput === undefined) {
return;
}
// 打开相机
try {
await this.mCameraInput.open();
} catch (error) {
let err = error as BusinessError;
console.error(TAG + 'Failed to open device, errorCode = ' + err.code);
}
// 获取支持的模式类型
let sceneModes: Array<camera.SceneMode> = this.mCameraManager.getSupportedSceneModes(this.curCameraDevice);
let isSupportPhotoMode: boolean = sceneModes.indexOf(camera.SceneMode.NORMAL_PHOTO) >= 0;
if (!isSupportPhotoMode) {
console.error(TAG + 'photo mode not support');
return;
}
// 获取相机设备支持的输出流能力
let cameraOutputCapability: camera.CameraOutputCapability =
this.mCameraManager.getSupportedOutputCapability(this.curCameraDevice, camera.SceneMode.NORMAL_PHOTO);
if (!cameraOutputCapability) {
console.error(TAG + 'cameraManager.getSupportedOutputCapability error');
return;
}
console.info(TAG + 'outputCapability: ' + JSON.stringify(cameraOutputCapability));
let previewProfile = this.getPreviewProfile(cameraOutputCapability);
if (previewProfile === undefined) {
console.error(TAG + 'The resolution of the current preview stream is not supported.');
return;
}
this.previewProfileObj = previewProfile;
// 创建预览输出流,其中参数 surfaceId 参考上文 XComponent 组件，预览流为XComponent组件提供的surface
try {
this.mPreviewOutput = this.mCameraManager.createPreviewOutput(this.previewProfileObj, surfaceId);
} catch (error) {
let err = error as BusinessError;
console.error(TAG + `Failed to create the PreviewOutput instance. error code: ${err.code}`);
}
if (this.mPreviewOutput === undefined) {
return;
}
//创建会话
try {
this.mPhotoSession = this.mCameraManager.createSession(camera.SceneMode.NORMAL_PHOTO) as camera.PhotoSession;
} catch (error) {
let err = error as BusinessError;
console.error(TAG + 'Failed to create the session instance. errorCode = ' + err.code);
}
if (this.mPhotoSession === undefined) {
return;
}
// 开始配置会话
try {
this.mPhotoSession.beginConfig();
} catch (error) {
let err = error as BusinessError;
console.error(TAG + 'Failed to beginConfig. errorCode = ' + err.code);
}
// 向会话中添加相机输入流
try {
this.mPhotoSession.addInput(this.mCameraInput);
} catch (error) {
let err = error as BusinessError;
console.error(TAG + 'Failed to addInput. errorCode = ' + err.code);
}
// 向会话中添加预览输出流
try {
this.mPhotoSession.addOutput(this.mPreviewOutput);
} catch (error) {
let err = error as BusinessError;
console.error(TAG + 'Failed to addOutput(previewOutput). errorCode = ' + err.code);
}
// 提交会话配置
try {
await this.mPhotoSession.commitConfig();
} catch (error) {
let err = error as BusinessError;
console.error(TAG + 'Failed to commit session configuration, errorCode = ' + err.code);
}
// 启动会话
try {
await this.mPhotoSession.start()
} catch (error) {
let err = error as BusinessError;
console.error(TAG + 'Failed to start session. errorCode = ' + err.code);
}
}
build() {
if (this.isShow) {
Stack() {
if (this.reloadXComponentFlag) {
XComponent(this.mXComponentOptions)
.onLoad(async () => {
await this.loadXComponent();
})
.width(px2vp(1080))
.height(px2vp(1920))
} else {
XComponent(this.mXComponentOptions)
.onLoad(async () => {
await this.loadXComponent();
})
.width(px2vp(1080))
.height(px2vp(1920))
}
Text('切换摄像头')
.size({ width: 80, height: 48 })
.position({ x: 1, y: 1 })
.backgroundColor(Color.White)
.textAlign(TextAlign.Center)
.borderRadius(24)
.onClick(async () => {
this.mCameraPosition = this.mCameraPosition === camera.CameraPosition.CAMERA_POSITION_BACK ?
camera.CameraPosition.CAMERA_POSITION_FRONT : camera.CameraPosition.CAMERA_POSITION_BACK;
this.reloadXComponentFlag = !this.reloadXComponentFlag;
})
}
.size({ width: '100%', height: '100%' })
.backgroundColor(Color.Black)
}
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-deferred-capture-V14
爬取时间: 2025-04-28 20:12:42
来源: Huawei Developer
分段式拍照是相机的重要功能之一，即应用下发拍照任务后，系统将分多阶段上报不同质量的图片。
通过分段式拍照，优化了系统的拍照响应时延，从而提升用户体验。
应用开发分段式拍照主要分为以下步骤：
开发步骤
详细的API说明请参考Camera API参考。
1.  导入依赖，需要导入相机框架、媒体库、图片相关领域依赖。
```typescript
import { camera } from '@kit.CameraKit';
import { BusinessError } from '@kit.BasicServicesKit';
import { common } from '@kit.AbilityKit';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
```
2.  确定拍照输出流。 通过CameraOutputCapability类中的photoProfiles属性，可获取当前设备支持的拍照输出流，通过createPhotoOutput方法创建拍照输出流。
```typescript
function getPhotoOutput(cameraManager: camera.CameraManager,
cameraOutputCapability: camera.CameraOutputCapability): camera.PhotoOutput | undefined {
let photoProfilesArray: Array<camera.Profile> = cameraOutputCapability.photoProfiles;
if (!photoProfilesArray) {
console.error("createOutput photoProfilesArray == null || undefined");
}
let photoOutput: camera.PhotoOutput | undefined = undefined;
try {
photoOutput = cameraManager.createPhotoOutput(photoProfilesArray[0]);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to createPhotoOutput. error: ${JSON.stringify(err)}`);
}
return photoOutput;
}
```
3.  设置拍照photoAssetAvailable的回调。 如果已经注册了photoAssetAvailable回调，并且在Session开始之后又注册了photoAvailable回调，photoAssetAvailable和photoAvailable同时注册，会导致流被重启，仅photoAssetAvailable生效。 不建议开发者同时注册photoAvailable和photoAssetAvailable。 落盘图片参考媒体库接口：saveCameraPhoto 请求图片参考媒体库接口：requestImageData和onDataPrepared
```typescript
function photoAssetAvailableCallback(err: BusinessError, photoAsset: photoAccessHelper.PhotoAsset): void {
if (err) {
console.error(`photoAssetAvailable error: ${JSON.stringify(err)}.`);
return;
}
console.info('photoOutPutCallBack photoAssetAvailable');
// 开发者可通过photoAsset调用媒体库相关接口，自定义处理图片
// 处理方式一：调用媒体库落盘接口保存一阶段图，二阶段图就绪后媒体库会主动帮应用替换落盘图片
mediaLibSavePhoto(photoAsset);
// 处理方式二：调用媒体库接口请求图片并注册一阶段图或二阶段图buffer回调，自定义使用
mediaLibRequestBuffer(photoAsset);
}
function onPhotoOutputPhotoAssetAvailable(photoOutput: camera.PhotoOutput): void {
photoOutput.on('photoAssetAvailable', photoAssetAvailableCallback);
}
let context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
async function mediaLibSavePhoto(photoAsset: photoAccessHelper.PhotoAsset): Promise<void> {
try {
let assetChangeRequest: photoAccessHelper.MediaAssetChangeRequest = new photoAccessHelper.MediaAssetChangeRequest(photoAsset);
assetChangeRequest.saveCameraPhoto();
await phAccessHelper.applyChanges(assetChangeRequest);
console.info('apply saveCameraPhoto successfully');
} catch (err) {
console.error(`apply saveCameraPhoto failed with error: ${err.code}, ${err.message}`);
}
}
class MediaDataHandler implements photoAccessHelper.MediaAssetDataHandler<ArrayBuffer> {
onDataPrepared(data: ArrayBuffer) {
if (data === undefined) {
console.error('Error occurred when preparing data');
return;
}
// 应用获取到图片buffer后可自定义处理
console.info('on image data prepared');
}
}
async function mediaLibRequestBuffer(photoAsset: photoAccessHelper.PhotoAsset) {
let requestOptions: photoAccessHelper.RequestOptions = {
// 按照业务需求配置回图模式
// FAST_MODE：仅接收一阶段低质量图回调
// HIGH_QUALITY_MODE：仅接收二阶段全质量图回调
// BALANCE_MODE：接收一阶段及二阶段图片回调
deliveryMode: photoAccessHelper.DeliveryMode.FAST_MODE,
}
const handler = new MediaDataHandler();
await photoAccessHelper.MediaAssetManager.requestImageData(context, photoAsset, requestOptions, handler);
console.info('requestImageData successfully');
}
```
4.  拍照时的会话配置及触发拍照的方式，与普通拍照相同，请参考拍照的步骤4-5。
状态监听
在相机应用开发过程中，可以随时监听拍照输出流状态，包括拍照流开始、拍照帧的开始与结束、拍照输出流的错误。
-  通过注册固定的captureStart回调函数获取监听拍照开始结果，photoOutput创建成功时即可监听，相机设备已经准备开始这次拍照时触发，该事件返回此次拍照的captureId。
```typescript
function onPhotoOutputCaptureStart(photoOutput: camera.PhotoOutput): void {
photoOutput.on('captureStartWithInfo', (err: BusinessError, captureStartInfo: camera.CaptureStartInfo) => {
if (err !== undefined && err.code !== 0) {
return;
}
console.info(`photo capture started, captureId : ${captureStartInfo.captureId}`);
});
}
```
-  通过注册固定的captureEnd回调函数获取监听拍照结束结果，photoOutput创建成功时即可监听，该事件返回结果为拍照完全结束后的相关信息CaptureEndInfo。
```typescript
function onPhotoOutputCaptureEnd(photoOutput: camera.PhotoOutput): void {
photoOutput.on('captureEnd', (err: BusinessError, captureEndInfo: camera.CaptureEndInfo) => {
if (err !== undefined && err.code !== 0) {
return;
}
console.info(`photo capture end, captureId : ${captureEndInfo.captureId}`);
console.info(`frameCount : ${captureEndInfo.frameCount}`);
});
}
```
-  通过注册固定的captureReady回调函数获取监听可拍下一张结果，photoOutput创建成功时即可监听，当下一张可拍时触发，该事件返回结果为下一张可拍的相关信息。
```typescript
function onPhotoOutputCaptureReady(photoOutput: camera.PhotoOutput): void {
photoOutput.on('captureReady', (err: BusinessError) => {
if (err !== undefined && err.code !== 0) {
return;
}
console.info(`photo capture ready`);
});
}
```
-  通过注册固定的error回调函数获取监听拍照输出流的错误结果。callback返回拍照输出接口使用错误时的对应错误码，错误码类型参见Camera错误码。
```typescript
function onPhotoOutputError(photoOutput: camera.PhotoOutput): void {
photoOutput.on('error', (error: BusinessError) => {
console.error(`Photo output error code: ${error.code}`);
});
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-moving-photo-V14
爬取时间: 2025-04-28 20:12:55
来源: Huawei Developer
相机框架提供动态照片拍摄能力，业务应用可以类似拍摄普通照片一样，一键式拍摄得到动态照片。
应用开发动态照片主要分为以下步骤：
开发步骤
详细的API说明请参考Camera API参考。
1.  导入依赖，需要导入相机框架、媒体库、图片相关领域依赖。
```typescript
import { camera } from '@kit.CameraKit';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
import { BusinessError } from '@kit.BasicServicesKit';
```
2.  确定拍照输出流。 通过CameraOutputCapability类中的photoProfiles属性，可获取当前设备支持的拍照输出流，通过createPhotoOutput方法创建拍照输出流。
```typescript
function getPhotoOutput(cameraManager: camera.CameraManager,
cameraOutputCapability: camera.CameraOutputCapability): camera.PhotoOutput | undefined {
let photoProfilesArray: Array<camera.Profile> = cameraOutputCapability.photoProfiles;
if (!photoProfilesArray) {
console.error("createOutput photoProfilesArray == null || undefined");
}
let photoOutput: camera.PhotoOutput | undefined = undefined;
try {
photoOutput = cameraManager.createPhotoOutput(photoProfilesArray[0]);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to createPhotoOutput. error: ${JSON.stringify(err)}`);
}
return photoOutput;
}
```
3.  查询当前设备当前模式是否支持动态照片能力。 查询是否支持动态照片前需要先完成相机会话配置、提交和启动会话，详细开发步骤请参考会话管理。
```typescript
function isMovingPhotoSupported(photoOutput: camera.PhotoOutput): boolean {
let isSupported: boolean = false;
try {
isSupported = photoOutput.isMovingPhotoSupported();
} catch (error) {
// 失败返回错误码error.code并处理
let err = error as BusinessError;
console.error(`The isMovingPhotoSupported call failed. error code: ${err.code}`);
}
return isSupported;
}
```
4.  使能动态照片拍照能力。
```typescript
function enableMovingPhoto(photoOutput: camera.PhotoOutput): void {
try {
photoOutput.enableMovingPhoto(true);
} catch (error) {
// 失败返回错误码error.code并处理
let err = error as BusinessError;
console.error(`The enableMovingPhoto call failed. error code: ${err.code}`);
}
}
```
5.  触发拍照，与普通拍照方式相同，请参考拍照。
状态监听
在相机应用开发过程中，可以随时监听动态照片拍照输出流状态。通过注册photoAsset的回调函数获取监听结果，photoOutput创建成功时即可监听。
```typescript
let context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
async function mediaLibSavePhoto(photoAsset: photoAccessHelper.PhotoAsset): Promise<void> {
try {
let assetChangeRequest: photoAccessHelper.MediaAssetChangeRequest = new photoAccessHelper.MediaAssetChangeRequest(photoAsset);
assetChangeRequest.saveCameraPhoto();
await phAccessHelper.applyChanges(assetChangeRequest);
console.info('apply saveCameraPhoto successfully');
} catch (err) {
console.error(`apply saveCameraPhoto failed with error: ${err.code}, ${err.message}`);
}
}
function onPhotoOutputPhotoAssetAvailable(photoOutput: camera.PhotoOutput): void {
photoOutput.on('photoAssetAvailable', (err: BusinessError, photoAsset: photoAccessHelper.PhotoAsset): void => {
if (err) {
console.info(`photoAssetAvailable error: ${JSON.stringify(err)}.`);
return;
}
console.info('photoOutPutCallBack photoAssetAvailable');
// 调用媒体库落盘接口保存一阶段图和动态照片视频
mediaLibSavePhoto(photoAsset);
});
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-animation-V14
爬取时间: 2025-04-28 20:13:13
来源: Huawei Developer
在使用相机过程中，如相机模式切换，前后置镜头切换等场景，不可避免出现预览流替换，为优化用户体验，可合理使用动效过渡。本文主要介绍如何使用预览流截图，并通过ArkUI提供的显示动画能力实现下方三种核心场景动效：
-  模式切换动效，使用预览流截图做模糊动效过渡。 图片为从录像模式切换为拍照模式的效果。
-  前后置切换动效，使用预览流截图做翻转模糊动效过渡。 图片为从前置摄像头切换为后置摄像头的效果。
-  拍照闪黑动效，使用闪黑组件覆盖预览流实现闪黑动效过渡。 图片为点击完成拍摄的效果。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165942.77180983478151581230724503982791:50001231000000:2800:601567A6F8FD24EFDA4DEBABFCD2812B0EC9414547289DF1DF3A425888AC1029.gif)
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165942.35602231616395554748582650397910:50001231000000:2800:5DF2E88018ED993466C8EE770F16F2CB93BFAB08499406813887FBF6F2B0C00E.gif)
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165942.98526505259050897615034429887235:50001231000000:2800:C5812CDAE81C46AAB54318D52A9BA86A17BFD62ED25C1CC221C0B0E3852D776E.gif)
闪黑动效
使用组件覆盖的形式实现闪黑效果。
1.  导入依赖，需要导入相机框架、图片、ArkUI相关领域依赖。
```typescript
import { curves } from '@kit.ArkUI';
```
2.  构建闪黑组件。 此处定义一个闪黑组件，在拍照闪黑及前后置切换时显示，用来遮挡XComponent组件。 属性定义： 闪黑组件的实现逻辑参考：
```typescript
@State isShowBlack: boolean = false; // 是否显示闪黑组件
@StorageLink('captureClick') @Watch('onCaptureClick') captureClickFlag: number = 0; // 拍照闪黑动效入口
@State flashBlackOpacity: number = 1; // 闪黑组件透明度
```
3.  实现闪黑动效。
```typescript
function flashBlackAnim() {
console.info('flashBlackAnim E');
this.flashBlackOpacity = 1; // 闪黑组件不透明
this.isShowBlack = true; // 显示闪黑组件
animateToImmediately({
curve: curves.interpolatingSpring(1, 1, 410, 38),
delay: 50, // 延时50ms，实现黑屏
onFinish: () => {
this.isShowBlack = false; // 闪黑组件下树
this.flashBlackOpacity = 1;
console.info('flashBlackAnim X');
}
}, () => {
this.flashBlackOpacity = 0; // 闪黑组件从不透明到透明
})
}
```
4.  触发闪黑动效。 点击或触控拍照按钮，更新StorageLink绑定CaptureClick的值，触发onCaptureClick方法，动效开始播放。
```typescript
onCaptureClick(): void {
console.info('onCaptureClick');
console.info('onCaptureClick');
this.flashBlackAnim();
}
```
模糊动效
通过预览流截图，实现模糊动效，从而完成模式切换，或是前后置切换的动效。
1.  导入依赖，需要导入相机框架、图片、ArkUI相关领域依赖。
```typescript
import { camera } from '@kit.CameraKit';
import { image } from '@kit.ImageKit';
import { curves } from '@kit.ArkUI';
```
2.  获取预览流截图。 预览流截图通过图形提供的image.createPixelMapFromSurface接口实现，surfaceId为当前预览流的surfaceId，size为当前预览流profile的宽高。创建截图工具类(ts文件)，导入依赖，导出获取截图方法供页面使用，截图工具类实现参考：
```typescript
import { image } from '@kit.ImageKit';
export class BlurAnimateUtil {
public static surfaceShot: image.PixelMap;
/**
* 获取surface截图
* @param surfaceId
* @returns
*/
public static async doSurfaceShot(surfaceId: string) {
console.info(`doSurfaceShot surfaceId:${surfaceId}.`);
if (surfaceId === '') {
console.error('surface not ready!');
return;
}
try {
if (this.surfaceShot) {
await this.surfaceShot.release();
}
this.surfaceShot = await image.createPixelMapFromSurface(surfaceId, {
size: { width: 1920, height: 1080 }, // 取预览流profile的宽高
x: 0,
y: 0
});
let imageInfo: image.ImageInfo = await this.surfaceShot.getImageInfo();
console.info('doSurfaceShot surfaceShot:' + JSON.stringify(imageInfo.size));
} catch (err) {
console.error(JSON.stringify(err));
}
}
/**
* 获取doSurfaceShot得到的截图
* @returns
*/
public static getSurfaceShot(): image.PixelMap {
return this.surfaceShot;
}
}
```
3.  构建截图组件。 此处定义一个截图组件，置于预览流XComponent组件之上，用来遮挡XComponent组件。 属性定义： 截图组件的实现参考：
```typescript
@State isShowBlur: boolean = false; // 是否显示截图组件
@StorageLink('modeChange') @Watch('onModeChange') modeChangeFlag: number = 0; // 模式切换动效触发入口
@StorageLink('switchCamera') @Watch('onSwitchCamera') switchCameraFlag: number = 0;// 前后置切换动效触发入口
@StorageLink('frameStart') @Watch('onFrameStart') frameStartFlag: number = 0; // 动效消失入口
@State screenshotPixelMap: image.PixelMap | undefined = undefined; // 截图组件PixelMap
@State surfaceId: string = ''; // 当前预览流XComponent的surfaceId
@StorageLink('curPosition') curPosition: number = 0; // 当前镜头前后置状态
@State shotImgBlur: number = 0; // 截图组件模糊度
@State shotImgOpacity: number = 1; // 截图组件透明度
@State shotImgScale: ScaleOptions = { x: 1, y: 1 }; // 截图组件比例
@State shotImgRotation: RotateOptions = { y: 0.5, angle: 0 } // 截图组件旋转角度
```
4.  （按实际情况选择）实现模糊出现动效。 模式切换动效分两段实现，模糊出现动效和模糊消失动效。 模糊出现动效：用户点击或触控事件触发预览流截图，显示截图组件，截图清晰到模糊，覆盖旧预览流。 注意：由于图形提供的image.createPixelMapFromSurface接口是截取surface内容获取PixelMap，其内容和XComponent组件绘制逻辑不同，需要根据前后置镜头做不同的图片内容旋转补偿和组件旋转补偿。
```typescript
async function showBlurAnim() {
console.info('showBlurAnim E');
// 获取已完成的surface截图
let shotPixel = BlurAnimateUtil.getSurfaceShot();
// 后置
if (this.curPosition === 0) {
console.info('showBlurAnim BACK');
// 直板机后置截图初始内容旋转补偿90°
await shotPixel.rotate(90); //ImageKit提供，用于图片内容旋转
// 直板机后置截图初始组件旋转补偿0°
this.shotImgRotation = { y: 0.5, angle: 0 };
} else {
console.info('showBlurAnim FRONT');
// 直板机前置截图内容旋转补偿270°
await shotPixel.rotate(270);
// 直板机前置截图组件旋转补偿180°
this.shotImgRotation = { y: 0.5, angle: 180 };
}
this.screenshotPixelMap = shotPixel;
// 初始化动效参数
this.shotImgBlur = 0; // 无模糊
this.shotImgOpacity = 1; // 不透明
this.isShowBlur = true;  // 显示截图组件
animateToImmediately(
{
duration: 200,
curve: Curve.Friction,
onFinish: async () => {
console.info('showBlurAnim X');
}
},
() => {
this.shotImgBlur = 48; // 截图组件模糊度变化动效
}
);
}
```
5.  实现模糊消失动效。 模糊消失动效：由新模式预览流首帧回调on('frameStart')触发，截图组件模糊到清晰，显示新预览流。
```typescript
function hideBlurAnim(): void {
this.isShowBlack = false;
console.info('hideBlurAnim E');
animateToImmediately({
duration: 200,
curve: Curve.FastOutSlowIn,
onFinish: () => {
this.isShowBlur = false; // 模糊组件下树
this.shotImgBlur = 0;
this.shotImgOpacity = 1;
console.info('hideBlurAnim X');
}
}, () => {
// 截图透明度变化动效
this.shotImgOpacity = 0; // 截图组件透明度变化动效
});
}
```
6.  （按实际情况选择）实现模糊翻转动效。 模糊翻转动效分两段实现，模糊翻转动效和模糊消失动效，其中模糊消失动效同第5步。 模糊翻转动效：分两段组件翻转实现，先向外翻转90°再向内翻转90°，同时还执行了模糊度、透明度、比例缩放等动效。 为保证预览流在翻转时不露出，需要构建一个闪黑组件用于遮挡XComponent组件，构建方式参考闪黑动效-步骤2。
```typescript
/**
* 先向外翻转90°，前后置切换触发
*/
async function rotateFirstAnim() {
console.info('rotateFirstAnim E');
// 获取已完成的surface截图
let shotPixel = BlurAnimateUtil.getSurfaceShot();
// 后置切前置
if (this.curPosition === 1) {
console.info('rotateFirstAnim BACK');
// 直板机后置切前置截图初始内容旋转补偿90°
await shotPixel.rotate(90); //ImageKit提供，用于图片内容旋转
// 直板机后置切前置截图初始组件旋转补偿0°
this.shotImgRotation = { y: 0.5, angle: 0 };
} else {
console.info('rotateFirstAnim FRONT');
// 直板机前置切后置截图初始内容旋转补偿270°
await shotPixel.rotate(270);
// 直板机前置切后置截图初始组件旋转补偿180°
this.shotImgRotation = { y: 0.5, angle: 180 };
}
this.screenshotPixelMap = shotPixel;
this.isShowBlack = true; // 显示闪黑组件，覆盖预览流保证视觉效果
this.isShowBlur = true; // 显示截图组件
animateToImmediately(
{
duration: 200,
delay: 50, // 时延保证组件缩放模糊动效先行，再翻转,视觉效果更好
curve: curves.cubicBezierCurve(0.20, 0.00, 0.83, 1.00),
onFinish: () => {
console.info('rotateFirstAnim X');
// 在onFinish后触发二段翻转
this.rotateSecondAnim();
}
},
() => {
// 截图向外翻转动效
if (this.curPosition === 1) {
this.shotImgRotation = { y: 0.5, angle: 90 };
} else {
this.shotImgRotation = { y: 0.5, angle: 270 };
}
}
)
}
/**
* 再向内翻转90°
*/
async function rotateSecondAnim() {
console.info('rotateSecondAnim E');
// 获取已完成的surface截图
let shotPixel = BlurAnimateUtil.getSurfaceShot();
// 后置
if (this.curPosition === 1) {
// 直板机后置镜头内容旋转补偿90°
await shotPixel.rotate(90);
// 组件旋转调整为-90°，保证二段翻转后，图片不是镜像的
this.shotImgRotation = { y: 0.5, angle: 90 };
} else { // 前置
// 直板机前置截图内容旋转补偿270°
await shotPixel.rotate(270);
// 直板机前置截图组件旋转补偿180°
this.shotImgRotation = { y: 0.5, angle: 180 };
}
this.screenshotPixelMap = shotPixel;
animateToImmediately(
{
duration: 200,
curve: curves.cubicBezierCurve(0.17, 0.00, 0.20, 1.00),
onFinish: () => {
console.info('rotateSecondAnim X');
}
},
() => {
// 截图向内翻转动效，翻转至初始状态
if (this.curPosition === 1) {
this.shotImgRotation = { y: 0.5, angle: 0 };
} else {
this.shotImgRotation = { y: 0.5, angle: 180 };
}
}
)
}
/**
* 向外翻转90°同时
*/
function blurFirstAnim() {
console.info('blurFirstAnim E');
// 初始化动效参数
this.shotImgBlur = 0; //无模糊
this.shotImgOpacity = 1; //不透明
this.shotImgScale = { x: 1, y: 1 };
animateToImmediately(
{
duration: 200,
curve: Curve.Sharp,
onFinish: () => {
console.info('blurFirstAnim X');
this.blurSecondAnim();
}
},
() => {
// 截图模糊动效
this.shotImgBlur = 48;
// 截图比例缩小动效
this.shotImgScale = { x: 0.75, y: 0.75 };
}
);
}
/**
* 向内翻转90°同时
*/
function blurSecondAnim() {
console.info('blurSecondAnim E');
animateToImmediately(
{
duration: 200,
curve: Curve.Sharp,
onFinish: () => {
console.info('blurSecondAnim X');
}
},
() => {
// 截图比例恢复动效
this.shotImgScale = { x: 1, y: 1 };
}
)
}
```
7.  按需触发动效。 模式切换动效触发：点击或触控模式按钮立即执行doSurfaceShot截图方法，更新StorageLink绑定modeChange的值，触发onModeChange方法，开始动效。 前后置切换动效触发：点击或触控前后置切换按钮立即执行doSurfaceShot截图方法，更新StorageLink绑定switchCamera的值，触发onSwitchCamera方法，开始动效。 模糊消失动效触发：监听预览流首帧回调on('frameStart')，更新StorageLink绑定frameStart的值，触发onFrameStart方法，开始动效。
```typescript
onModeChange(): void {
console.info('onModeChange');
this.showBlurAnim();
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-worker-V14
爬取时间: 2025-04-28 20:13:27
来源: Huawei Developer
Worker主要作用是为应用程序提供一个多线程的运行环境，可满足应用程序在执行过程中与主线程分离，在后台线程中运行一个脚本进行耗时操作，极大避免类似于计算密集型或高延迟的任务阻塞主线程的运行。
通常开发者使用相机功能需要创建相机会话，并持续接收处理预览流、拍照流、录像流等从而实现相关相机功能，这些密集型操作如果都放在主线程即UI线程，可能会阻塞UI绘制，推荐开发者在worker线程中实现相机功能。
开发步骤
1.  创建worker线程文件，配置worker。 DevEco Studio支持一键生成Worker，在对应的{moduleName}目录下任意位置，点击鼠标右键 > New > Worker，即可自动生成Worker的模板文件及配置信息，无需再手动在build-profile.json5中进行相关配置 。 CameraWorker.ets实现参考：
```typescript
import { ErrorEvent, MessageEvents, ThreadWorkerGlobalScope, worker } from '@kit.ArkTS';
import CameraService from '../CameraService';
const workerPort: ThreadWorkerGlobalScope = worker.workerPort;
// 自定义消息格式
interface MessageInfo {
hasResolve: boolean;
type: string;
context: Context; // 注意worker线程中无法使用getContext()直接获取宿主线程context，需要通过消息从宿主线程通信到worker线程使用。
surfaceId: string;
}
workerPort.onmessage = async (e: MessageEvents) => {
const messageInfo: MessageInfo = e.data;
console.info(`worker onmessage type:${messageInfo.type}`)
if ('initCamera' === messageInfo.type) {
// 在worker线程中收到宿主线程初始化相机的消息
console.info(`worker initCamera surfaceId:${messageInfo.surfaceId}`)
// 在worker线程中初始化相机
await CameraService.initCamera(messageInfo.context, messageInfo.surfaceId);
} else if ('releaseCamera' === messageInfo.type) {
// 在worker线程中收到宿主线程释放相机的消息
console.info('worker releaseCamera.');
// 在worker线程中释放相机
await CameraService.releaseCamera();
}
}
workerPort.onmessageerror = (e: MessageEvents) => {
}
workerPort.onerror = (e: ErrorEvent) => {
}
```
2.  创建相机服务代理类，调用CameraKit方法都放在这个类里执行。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
import { camera } from '@kit.CameraKit';
class CameraService {
private imageWidth: number = 1920;
private imageHeight: number = 1080;
private cameraManager: camera.CameraManager | undefined = undefined;
private cameras: Array<camera.CameraDevice> | Array<camera.CameraDevice> = [];
private cameraInput: camera.CameraInput | undefined = undefined;
private previewOutput: camera.PreviewOutput | undefined = undefined;
private photoOutput: camera.PhotoOutput | undefined = undefined;
private session: camera.PhotoSession | camera.VideoSession | undefined = undefined;
// 初始化相机
async initCamera(context: Context, surfaceId: string): Promise<void> {
console.info(`initCamera surfaceId: ${surfaceId}`);
try {
await this.releaseCamera();
// 获取相机管理器实例
this.cameraManager = camera.getCameraManager(context);
if (this.cameraManager === undefined) {
console.error('cameraManager is undefined');
return;
}
this.cameras = this.cameraManager.getSupportedCameras();
// 创建cameraInput输出对象
this.cameraInput = this.cameraManager.createCameraInput(this.cameras[0]);
if (this.cameraInput === undefined) {
console.error('Failed to create the camera input.');
return;
}
// 打开相机
await this.cameraInput.open();
let previewProfile: camera.Profile = {
format: camera.CameraFormat.CAMERA_FORMAT_YUV_420_SP,
size: {
width: this.imageWidth,
height: this.imageHeight
}
};
// 创建预览流输出
this.previewOutput = this.cameraManager.createPreviewOutput(previewProfile, surfaceId);
if (this.previewOutput === undefined) {
console.error('Failed to create the preview stream.');
return;
}
let photoProfile: camera.Profile = {
format: camera.CameraFormat.CAMERA_FORMAT_JPEG,
size: {
width: this.imageWidth,
height: this.imageHeight
}
};
// 创建拍照流输出
this.photoOutput = this.cameraManager.createPhotoOutput(photoProfile);
if (this.photoOutput === undefined) {
console.error('Failed to create the photoOutput.');
return;
}
// 创建相机会话，启动会话
this.session = this.cameraManager.createSession(camera.SceneMode.NORMAL_PHOTO) as camera.PhotoSession;
this.session.beginConfig();
this.session.addInput(this.cameraInput);
this.session.addOutput(this.previewOutput);
this.session.addOutput(this.photoOutput);
await this.session.commitConfig();
await this.session.start();
} catch (error) {
let err = error as BusinessError;
console.error(`initCamera fail: ${JSON.stringify(err)}`);
}
}
// 释放相机资源
async releaseCamera(): Promise<void> {
console.info('releaseCamera is called');
try {
await this.previewOutput?.release();
await this.photoOutput?.release();
await this.session?.release();
await this.cameraInput?.close();
} catch (error) {
let err = error as BusinessError;
console.error(`releaseCamera fail: error: ${JSON.stringify(err)}`);
} finally {
this.previewOutput = undefined;
this.photoOutput = undefined;
this.cameraManager = undefined;
this.session = undefined;
this.cameraInput = undefined;
}
console.info('releaseCamera success');
}
}
export default new CameraService();
```
3.  创建组件，用于显示预览流，在页面相关生命周期中构造ThreadWorker实例，在worker线程中完成相机初始化和释放。
```typescript
import { worker } from '@kit.ArkTS';
@Entry
@Component
struct Index {
private mXComponentController: XComponentController = new XComponentController();
private surfaceId: string = '';
@State imageWidth: number = 1920;
@State imageHeight: number = 1080;
// 创建ThreadWorker对象获取worker实例
private workerInstance: worker.ThreadWorker = new worker.ThreadWorker('entry/ets/workers/CameraWorker.ets');
onPageShow(): void {
if ('' !== this.surfaceId) {
// 通过worker实例向worker线程发送消息初始化相机
this.workerInstance.postMessage({
type: 'initCamera',
context: getContext(this),
surfaceId: this.surfaceId,
})
}
}
onPageHide(): void {
// 通过worker实例向worker线程发送消息销毁相机
this.workerInstance.postMessage({
type: 'releaseCamera',
})
}
build() {
Column() {
Column() {
XComponent({
id: 'componentId',
type: XComponentType.SURFACE,
controller: this.mXComponentController
})
.onLoad(async () => {
console.info('onLoad is called');
// 初始化XComponent获取预览流surfaceId
this.surfaceId = this.mXComponentController.getXComponentSurfaceId();
let surfaceRect: SurfaceRect = {
surfaceWidth: this.imageHeight,
surfaceHeight: this.imageWidth
};
this.mXComponentController.setXComponentSurfaceRect(surfaceRect);
console.info(`onLoad surfaceId: ${this.surfaceId}`);
if (!this.workerInstance) {
console.error('create stage worker failed');
return;
}
// 宿主线程向worker线程发送初始化相机消息
this.workerInstance.postMessage({
type: 'initCamera',
context: getContext(this), // 将宿主线程的context传给worker线程使用
surfaceId: this.surfaceId, // 将surfaceId传给worker线程使用
})
})// The width and height of the surface are opposite to those of the XComponent.
.width(px2vp(this.imageHeight))
.height(px2vp(this.imageWidth))
}.justifyContent(FlexAlign.Center)
.height('90%')
Text('WorkerDemo')
.fontSize(36)
}
.justifyContent(FlexAlign.End)
.height('100%')
.width('100%')
}
}
```
trace对比
不使用worker：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165942.29369524170721765627350267995376:50001231000000:2800:0FE5C0B9DCD1592F180B76F33271B06CB546446EC89E13A327FE778121C96E9E.png)
使用woker：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165942.36820126328792853459833632253771:50001231000000:2800:5A3AC2561093D7AD94846BE0CC316D6D02163372FDAFAEDD3FD50BF3382C99FE.png)

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-rotation-V14
爬取时间: 2025-04-28 20:13:41
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-rotation-angle-adaptation-V14
爬取时间: 2025-04-28 20:13:55
来源: Huawei Developer
屏幕处于不同的屏幕状态时，原始图像需旋转不同的角度，以确保图像在合适的方向显示，效果如图所示。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250318161615.20182234763930793771207635956606:50001231000000:2800:3B3C1346B963CB860BD66F4B79CCC98EC302B8443E7DAEB7FED225C6F7FAC5B5.png)
本开发指导将指导开发者在预览、拍照、录像等不同场景下，如何适配相机的旋转角度。
-  拍照开发指导：创建会话>计算设备旋转角度>拍照 录像开发指导：创建会话>计算设备旋转角度>录像
详细的API参考说明，请参考Camera API文档。
创建会话
1.  相机使用预览等功能前，均需创建相机会话，调用CameraManager类中的createSession方法创建一个会话，创建会话时需指定创建SceneMode为NORMAL_PHOTO或NORMAL_VIDEO，创建的session处于拍照或者录像模式。
预览
完成会话创建后，开发者可根据实际需求，配置输出流。
1.  displayRotation：显示设备的屏幕旋转角度，可通过display.getDefaultDisplaySync获取Display对象并读取其rotation属性值，并将对应角度填入。 例：Display.rotation = 1，表示显示设备屏幕顺时针旋转为90°，此处displayRotation填入90。
2.  该接口需要在session调用commitConfig完成配流后调用。
拍照
完成会话创建后，开发者可根据实际需求，配置输出流。拍照的旋转角度与重力方向（即设备旋转角度）相关。
1.  该接口需要在session调用commitConfig完成配流后调用。 deviceDegree：设备旋转角度。获取方式请见计算设备旋转角度。
录像
完成会话创建后，开发者可根据实际需求，配置输出流。录像的旋转角度与重力方向（即设备旋转角度）相关。
1.  该接口需要在session调用commitConfig完成配流后调用。 deviceDegree：设备旋转角度。获取方式请见计算设备旋转角度。
计算设备旋转角度
当前可通过调用once(type: SensorId.GRAVITY, callback: Callback<GravityResponse>)获取一次重力传感器在x、y、z三个方向上的数据，计算得出设备旋转角度deviceDegree，示例如下所示。
如果无法获得重力传感器数据，需要申请重力传感器权限ohos.permission.ACCELEROMETER。权限申请请参考声明权限，如何获取传感器数据请参考传感器开发指导。
```typescript
import { Decimal } from '@kit.ArkTS';
import { sensor } from '@kit.SensorServiceKit';
import { BusinessError } from '@ohos.base';
getRealData(data: sensor.GravityResponse): number {
let getDeviceDegree: number = 0;
hilog.info('Succeeded in invoki e. X-coordinate component: ' + data.x);
hilog.info('Succeeded in invoking once. Y-coordinate component: ' + data.y);
hilog.info('Succeeded in invoking once. Z-coordinate component: ' + data.z);
let x = data.x;
let y = data.y;
let z = data.z;
if ((x * x + y * y) * 3 < z * z) {
return getDeviceDegree;
} else {
let sd: Decimal = Decimal.atan2(y, -x);
let sc: Decimal = Decimal.round(Number(sd) / 3.141592653589 * 180)
getDeviceDegree = 90 - Number(sc);
getDeviceDegree = getDeviceDegree >= 0 ? getDeviceDegree % 360 : getDeviceDegree % 360 + 360;
}
return getDeviceDegree;
}
getGravity() : Promise<number> {
sensor.getSensorList((error: BusinessError, data: Array<sensor.Sensor>) => {
for (let i = 0; i < data.length; i++) {
if (data[i].sensorId === sensor.SensorId.GRAVITY) {
this.isSupported = true;
break;
}
}});
if (this.isSupported === true) {
const promise: Promise<number> = new Promise((resolve, reject) => {
sensor.once(sensor.SensorId.GRAVITY, (data: sensor.GravityResponse) => {
resolve(this.getRealData(data));
});
})
return promise;
} else {
const promise: Promise<number> = new Promise((resolve, reject) => {
sensor.once(sensor.SensorId.ACCELEROMETER, (data: sensor.AccelerometerResponse) => {
resolve(this.getRealData(data as sensor.GravityResponse));
});
})
return promise;
}
}
```
视频通话送远端场景
两个设备之间进行视频通话，存在设备间持握方向不一致问题，建议在本端将画面转正，再通过网络发送到对端，画面转正参考自绘制场景预览角度的归一化处理。
常见问题
指定XComponent的大小，防止旋转后图像拉伸变形
图像显示出现拉伸或压缩等变形，是因为图像分辨率与XComponent的宽高比不匹配。以应用层下发的1920*1080(16:9)竖屏和横屏为例，器件出图均是按照4:3比例出一张RAW图，在此基础上，根据应用层下发的16:9比例进行裁切，提供数据给应用层。因此，无论手机持握方向如何变化，应用层接收的数据始终是16:9比例的图片。具体图示如下：
| 设备和镜头方向  | 处理过程示意图  | XComponent布局  |
| --- | --- | --- |
| 设备条件： 手机竖屏、充电口向下。 使用后置相机拍摄。 可得： 后置相机镜头角度 = 90°屏幕旋转角度 = 0°，Display.rotation = 0图像预览旋转角度 = 0°+90° = 90°  |   | 出图与最终成像有90度夹角，布局宽高与图像宽高交换。  |
| 设备条件： 手机横屏、充电口向右。 使用后置相机拍摄。 可得： 后置相机镜头角度 = 90°屏幕旋转角度 = 270°，Display.rotation = 3图像预览旋转角度 = 270°+90° = 360° = 0°  |   | 出图与最终成像有0度夹角，布局与图像宽高比一致。  |
设备和镜头方向
处理过程示意图
XComponent布局
设备条件：
手机竖屏、充电口向下。
使用后置相机拍摄。
可得：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250318161615.18504153199909113748123947762229:50001231000000:2800:A2ECBAEB77569D590B99255C77950BF7A3E2E2B147EDE0D242FC5F78A7C6532B.png)
出图与最终成像有90度夹角，布局宽高与图像宽高交换。
设备条件：
手机横屏、充电口向右。
使用后置相机拍摄。
可得：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250318161615.29587689744930378261614136684900:50001231000000:2800:DCC82A8DFABADD37754B0A7099F2D69C6172679CDD62678C80EDE26495DE3A0E.png)
出图与最终成像有0度夹角，布局与图像宽高比一致。
从上图可以看出，当手机从竖屏转换为横屏时，图像始终保持16:9的输出比例，但镜头与屏幕显示方向之间的夹角从90度变为0度。如果布局保持9:16不变，那么16:9的图像数据放置在9:16的空间内显示，会导致图像形变。因此，为确保图像显示正常，横屏时需要将布局的宽高比调整为16:9。
首先，将XComponent的宽度和高度作为状态变量进行监听，通过window.on('windowSizeChange')监听窗口的变化，根据屏幕旋转角度（display.rotation）与相机镜头角度（CameraDevice.cameraOrientation）之间的角度来确定布局的宽高比，以确保布局能跟随窗口实时调整。
除了指定XComponent的宽高外，还可以通过设置XComponent的renderFit来实现图片的自适应大小显示、居中裁剪显示等效果。具体详情请参考RenderFit介绍。
自绘制场景预览角度的归一化处理
在自绘制场景中，对于后置摄像头，可以通过调用getPreviewRotation获取旋转角度，将图像转正；对于前置摄像头，由于存在水平镜像和垂直镜像的差异，为了简化操作，需先对前置摄像头的图像角度进行归一化处理后，再将图像转正，并根据业务需求决定是否进行镜像处理。
pixelMap处理方式：
```typescript
async saveToFile(pixelMap: image.PixelMap): Promise<void> {
let rotation = display.getDefaultDisplaySync().rotation * camera.ImageRotation.ROTATION_90;
let angle = this.previewOutputReceiver?.getPreviewRotation(rotation);
if (angle === undefined) {
return;
}
this.previewOutputReceiver?.setPreviewRotation(angle);
if (GlobalContext.get().getT<number>('curCameraPosition') === camera.CameraPosition.CAMERA_POSITION_FRONT) {
if (rotation ===90 || rotation === 270) {
angle = (angle + 180 ) % 360;
}
await pixelMap.rotate(angle);
await pixelMap.flip(true, false);
} else {
await pixelMap.rotate(angle);
}
}
```
适配一多设备
为了适配一多设备，主要分为以下几步：
拍照无法镜像
通过设置PhotoCaptureSetting中的mirror属性改变拍照镜像。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-rotation-term-V14
爬取时间: 2025-04-28 20:14:09
来源: Huawei Developer
在适配相机旋转角度中涉及设备方向、镜头角度、屏幕显示角度等多个术语，开发者可以了解相关概念，帮助理解框架的运作机制。
设备自然方向
设备自然方向指设备默认的使用方向，以手机为例，如图所示，手机的自然方向为竖屏且充电口向下。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250318161615.06499168204264192123905147298696:50001231000000:2800:306279C21A16C5270D00657D2DB217AB54B98BCCB491149B4C7621B7D46E88D5.png)
屏幕显示方向
屏幕显示方向指当前用户视角下，设备正确的显示方向。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250318161615.98580014365963604215418221672139:50001231000000:2800:ED47B953E9AF1E496CE7A730C19D3CA6E9B1E1BBAF6373355B6739A84B63C513.png)
屏幕旋转角度
显示设备的屏幕顺时针旋转角度，简称为屏幕旋转角度，即设备从自然方向到当前方向的顺时针夹角。
如图所示，图示夹角即为屏幕旋转角度，可通过Display.rotation获取。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250318161615.38858202677299460333319810839022:50001231000000:2800:3AEE5C06860DBAE6597349A12845C5F29C17E40338B8EF994415C289E5F8AD71.png)
相机镜头安装角度
相机镜头安装角度指相机采集图像方向到设备自然方向在顺时针方向的夹角。
以手机为例，手机后置相机传感器是横屏安装的，当手机在竖屏方向使用后置相机镜头拍摄时，相机采集到的原始图像方向如图所示。
此时图像需要顺时针旋转90度，才能与设备自然方向保持一致，所以后置相机的镜头角度为90度。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250318161616.83031614564307670137029625267593:50001231000000:2800:3B5C40F7D080B2202463708418EC4E0079920EE33860C6F54F29E99DA220FF76.png)
而手机前置镜头，是朝向使用者的，当手机在竖屏方向使用前置相机镜头拍摄时，出图方向与后置出图方向互为镜像，如下图所示，前置相机的镜头角度为270度。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250318161616.28950613615561039539332056289882:50001231000000:2800:7DE5805DFAF5FF8C5EC772C91A852D0A5339281C05629D3F5D562CA83F01796A.png)
预览旋转角度
开发者可参考以下章节，了解框架实现的机制，在实际开发过程中，推荐通过接口获取预览旋转角度。
在预览时，图像旋转角度与屏幕显示旋转角度相关。系统将以原始图像方向为基线，根据相机镜头角度和屏幕显示补偿角度，旋转图像。
计算公式：图像旋转角度=镜头安装角度+屏幕显示补偿角度，屏幕显示补偿角度的值与屏幕旋转角度相等。
以手机设备为例展示相机在预览下如何处理图像，计算的角度设置给系统侧，作用于直接送显场景，应用自绘制参考应用自绘制预览角度处理。
| 设备和镜头方向  | 处理过程示意图  |
| --- | --- |
| 设备条件： 手机竖屏、充电口向下。 使用后置相机拍摄。 可得： 后置相机镜头角度 = 90°屏幕旋转角度= 0°，Display.rotation = 0图像预览旋转角度 = 90°+0° = 90°  |   |
| 设备条件： 手机横屏、充电口向左。 使用后置相机拍摄。 可得： 后置相机镜头角度 = 90°屏幕旋转角度 = 90°，Display.rotation = 1图像预览旋转角度 = 90°+90° = 180°  |   |
| 设备条件： 手机竖屏、充电口向上。 使用后置相机拍摄。 可得： 后置相机镜头角度 = 90°屏幕旋转角度 = 180°，Display.rotation = 2图像预览旋转角度 = 90°+180° = 270°  |   |
| 设备条件： 手机横屏、充电口向右。 使用后置相机拍摄。 可得： 后置相机镜头角度 = 90°屏幕旋转角度 = 270°，Display.rotation = 3图像预览旋转角度 = 90°+270° = 0°  |   |
| 设备条件： 手机竖屏、充电口向下。 使用前置相机拍摄。 可得： 前置相机镜头角度 = 270°前置相机镜像出图屏幕旋转角度= 0°，Display.rotation = 0图像预览旋转角度 = 270°+0° = 270°  |   |
| 设备条件： 手机横屏、充电口向左。 使用前置相机拍摄。 可得： 前置相机镜头角度 = 270°前置相机镜像出图屏幕旋转角度 = 90°，Display.rotation = 1图像预览旋转角度 = 270°+90° =0°  |   |
| 设备条件： 手机竖屏、充电口向上。 使用前置相机拍摄。 可得： 前置相机镜头角度 = 270°前置相机镜像出图 屏幕旋转角度 = 180°，Display.rotation = 2图像预览旋转角度 = 270°+180° = 90°  |   |
| 设备条件： 手机横屏、充电口向右。 使用前置相机拍摄。 可得： 前置相机镜头角度 = 270°前置相机镜像出图 屏幕旋转角度 = 270°，Display.rotation = 3图像预览旋转角度 = 270°+270° = 180°  |   |
设备和镜头方向
处理过程示意图
设备条件：
手机竖屏、充电口向下。
使用后置相机拍摄。
可得：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250318161616.85950628596502720467959819723738:50001231000000:2800:28980D45AFD022C7677BC7BE9A13C4DBFD8EA3908CAA30165DA1B48B0F32DAA7.png)
设备条件：
手机横屏、充电口向左。
使用后置相机拍摄。
可得：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250318161616.70823187564341374326440299909290:50001231000000:2800:4DAFA90B90649D3654EE2C4F9C2BCB8B1E5DA81526E0D51059A12082CB3F0CBC.png)
设备条件：
手机竖屏、充电口向上。
使用后置相机拍摄。
可得：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250318161616.47122486170510372182608528526850:50001231000000:2800:AE07CF6386545D665FE8AF6C9CD564E158516E2E2C4A7FA0549473FFCE89EF46.png)
设备条件：
手机横屏、充电口向右。
使用后置相机拍摄。
可得：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250318161616.67280584421139612908284615388678:50001231000000:2800:263111143185F22600523E0515E5473A70F46E6A2E9B935D7FEB46D821DDBCFD.png)
设备条件：
手机竖屏、充电口向下。
使用前置相机拍摄。
可得：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250318161616.67639047244843150796763295963776:50001231000000:2800:7FF7C8FCE2F67B0977F19B8179650EC0F02796322F768C8D3D236E7EB7C7D819.png)
设备条件：
手机横屏、充电口向左。
使用前置相机拍摄。
可得：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250318161616.17731029876561953164246128745973:50001231000000:2800:8EF47FA1224A7DE83FCE396595DE1E09269199BB6B0579B15C3D8DBF21C0BFDD.png)
设备条件：
手机竖屏、充电口向上。
使用前置相机拍摄。
可得：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250318161616.65583420474868547717426037710502:50001231000000:2800:C26885217CCF46DB38C8318E8B67F3E15FD188F467B6494BB3C77C0A8399CF26.png)
设备条件：
手机横屏、充电口向右。
使用前置相机拍摄。
可得：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250318161616.38478862345218157024313856333386:50001231000000:2800:D455395D4ACEE01AA9D185BB83FF7E703B6C73DF6AA603ABB6DD78A26A85FC20.png)
应用自绘制预览角度处理
应用自绘制场景是指应用获取图片后，通过libyuv、GL等图形处理库进行二次处理，生成新的图像数据并送到显示设备进行渲染绘制。
常见的实现方式是通过使用imageReceiver创建的回调流，应用层作为消费端，自行处理图片旋转等操作，以适应自绘制场景的预览角度需求。自绘制场景预览角度与预览旋转角度中描述的场景存在细微差异。
主要差异体现在使用前置镜头拍摄预览的场景：
| 设备和镜头方向  | 处理过程示意图  |
| --- | --- |
| 设备条件： 手机竖屏、充电口向下。 使用前置相机拍摄。 可得： 前置相机镜头角度 = 270°前置相机镜像出图屏幕旋转角度= 0°，Display.rotation = 0图像预览旋转角度 = 270°+0° = 270°  |   |
| 设备条件： 手机横屏、充电口向左。 使用前置相机拍摄。 可得： 前置相机镜头角度 = 270°前置相机镜像出图屏幕旋转角度 = 90°，Display.rotation = 1图像预览旋转角度 = 270°+90° = 0°  |   |
设备和镜头方向
处理过程示意图
设备条件：
手机竖屏、充电口向下。
使用前置相机拍摄。
可得：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250318161616.95542040419778528788098459123784:50001231000000:2800:A6FFED9CB20F441864B125D6267D35A67AABF02E71498F4344033AE15BFC0810.png)
设备条件：
手机横屏、充电口向左。
使用前置相机拍摄。
可得：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250318161616.57684280382049385219038933773441:50001231000000:2800:B2A54F1C845FBFCBFA4BEDB2864B8C1E16EE5919D96B85391DA53231E3AEF9FC.png)
拍照/录像角度
在拍照、录像时，图像旋转角度与设备重力方向（即设备旋转角度）相关。
应用需要监听SensorId.GRAVITY事件，获取重力传感器在x、y、z三个方向上的数据，计算得出设备旋转角度，请参考计算设备旋转角度。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-secure-photo-V14
爬取时间: 2025-04-28 20:14:22
来源: Huawei Developer
安全相机主要为银行等有活体检测等安全诉求的应用提供，安全相机的使用需要加密算法框架及可信应用服务。
应用具体使用步骤如下：
当前文档主要说明通过Camera Kit完成的步骤，证明会话相关步骤需通过DeviceSecurity Kit完成，具体可参考可信应用服务-安全摄像头。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165944.79377421146944078740592934328927:50001231000000:2800:6754DAEA45C59667AECD25313C464D32CBD74D270A02048574020F438E7765FB.png)
开发步骤
详细的API说明请参考Camera API参考。
1.  导入依赖，需要导入相机框架相关领域依赖。
```typescript
import { camera } from '@kit.CameraKit';
import { image } from '@kit.ImageKit';
```
2.  选择支持安全相机的设备。 通过CameraManager类中的getSupportedSceneModes方法，可以获取当前设备支持的所有模式，如果当前设备支持安全相机模式，即可使用该设备做后续安全相机操作。 当前安全相机仅支持手机前置镜头。
```typescript
function isSecureCamera(cameraManager: camera.CameraManager, cameraDevice: camera.CameraDevice): boolean {
let sceneModes: Array<camera.SceneMode> = cameraManager.getSupportedSceneModes(cameraDevice);
const secureMode = sceneModes.find(mode => mode === camera.SceneMode.SECURE_PHOTO);
if (secureMode) {
console.info('current device support secure camera!');
return true;
} else {
console.info('current device not support secure camera!');
return false;
}
}
let secureCamera: camera.CameraDevice;
function getSecureCamera(cameraManager: camera.CameraManager): void {
let cameraArray: Array<camera.CameraDevice> = cameraManager.getSupportedCameras();
for (let index = 0; index < cameraArray.length; index++) {
if (isSecureCamera(cameraManager, cameraArray[index])) {
secureCamera = cameraArray[index];
}
}
}
```
3.  查询相机设备在安全模式下支持的输出能力。 通过CameraManager类getSupportedOutputCapability方法，可获取设备在安全模式下支持的输出能力。 当前安全相机仅支持输出预览流，推荐预览流使用640 * 480分辨率。
```typescript
function getSupportedOutputCapability(cameraManager: camera.CameraManager, secureCamera: camera.CameraDevice): void {
let outputCap: camera.CameraOutputCapability =
cameraManager.getSupportedOutputCapability(secureCamera, camera.SceneMode.SECURE_PHOTO);
let previewProfilesArray: Array<camera.Profile> = outputCap.previewProfiles;
}
```
4.  创建设备输入输出。 安全相机需要创建两路输出流： 安全数据流没有单独的数据类型，同属于预览流，输出能力与预览流保持一致，创建ImageReceiver仅支持JPEG格式。
```typescript
async function createInputAndOutputs(cameraManager: camera.CameraManager,
secureCamera: camera.CameraDevice,
previewProfile: camera.Profile,
previewSurfaceId: string): Promise<void> {
// 创建输入流
let cameraInput: camera.CameraInput = cameraManager.createCameraInput(secureCamera);
// 创建普通预览输出流
let previewOutput: camera.PreviewOutput = cameraManager.createPreviewOutput(previewProfile, previewSurfaceId);
// 创建安全数据输出流
const receiver: image.ImageReceiver =
image.createImageReceiver({ width: previewProfile.size.width, height: previewProfile.size.height },
image.ImageFormat.JPEG, 8);
const secureSurfaceId: string = await receiver.getReceivingSurfaceId();
let secureOutput: camera.PreviewOutput = cameraManager.createPreviewOutput(previewProfile, secureSurfaceId);
}
```
5.  安全数据流没有单独的数据类型，同属于预览流，输出能力与预览流保持一致，创建ImageReceiver仅支持JPEG格式。
6.  打开安全设备。 CameraInput提供了open(isSecureEnabled)方法用于打开安全相机并返回安全摄像头序列号，该序列号是安全模块创建证明会话的必须参数。 仅当isSecureEnabled为true时，才会打开安全相机，并有安全序列号返回。
```typescript
async function openCamera(cameraInput: camera.CameraInput) {
const seqId: bigint = await cameraInput.open(true);
}
```
7.  创建安全相机会话，配流启流。 创建安全相机模式的会话，将输入流、输出流加入会话，需要将安全数据流通过SecureSession的addSecureOutput方法标记成安全输出。
```typescript
async function openSession(cameraManager: camera.CameraManager,
cameraInput: camera.CameraInput,
previewOutput: camera.PreviewOutput,
secureOutput: camera.PreviewOutput): Promise<void> {
try {
let secureSession: camera.SecureSession = cameraManager.createSession(camera.SceneMode.SECURE_PHOTO);
if (secureSession === undefined) {
console.error('create secureSession failed!');
}
secureSession.beginConfig();
secureSession.addInput(cameraInput);
secureSession.addOutput(previewOutput);
secureSession.addOutput(secureOutput);
secureSession.addSecureOutput(secureOutput); // 把secureOutput标记成安全输出
await secureSession.commitConfig();
await secureSession.start();
} catch (err) {
console.error('openSession failed!');
}
}
```
8.  解析安全数据流每帧安全图像，在服务器侧完成安全图像的签名验证。 如果有在端侧验证图像数据或地理位置数据签名的需求，可参考验证签名中与安全图像相关的部分。
```typescript
function onBuffer(receiver: image.ImageReceiver): void {
receiver.on('imageArrival', () => {
// 从ImageReceiver读取下一张图片
receiver.readNextImage().then((img: image.Image) => {
// 从图像中获取组件缓存
img.getComponent(image.ComponentType.JPEG).then((component: image.Component) => {
// 安全数据流内容，应用通过解析该buffer内容完成签名认证
const buffer = component.byteBuffer;
console.info('Succeeded in getting component byteBuffer.');
})
})
})
}
```
-  安全数据流没有单独的数据类型，同属于预览流，输出能力与预览流保持一致，创建ImageReceiver仅支持JPEG格式。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-framerate-V14
爬取时间: 2025-04-28 20:14:36
来源: Huawei Developer
动态调整帧率是直播、视频等场景下控制预览效果的重要能力之一。应用可通过此能力，显性地控制流输出帧率，以适应不同帧率下的业务目标。
某些场景下降低帧率可在相机设备启用时降低功耗。
约束与限制
支持的帧率范围及帧率的设置依赖于硬件能力的实现，不同的硬件平台可能拥有不同的默认帧率。
开发流程
相机使用预览功能前，均需要创建相机会话。完成会话配置后，应用提交和开启会话，才可以开始调用相机相关功能。
流程图如下所示：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165944.83327828257114728557154653423313:50001231000000:2800:9BE21F54F68F5FCCB6DF144FF6322C8FD0BB3E096A9D49D50CD2805F6CBE9C54.png)
与普通的预览流程相比，动态调整预览帧率的注意点如图上标识：
1.  仅当Session处于NORMAL_PHOTO或NORMAL_VIDEO模式时，支持调整预览流帧率。调整帧率的创建会话方式见创建Session会话并指定模式。
如何配置会话（Session）、释放资源，请参考会话管理>预览，或是完整流程示例。
创建Session会话并指定模式
相机使用预览等功能前，均需创建相机会话，调用CameraManager的createSession创建一个会话。
创建会话时需指定SceneMode为NORMAL_PHOTO或NORMAL_VIDEO，创建出的Session处于拍照或录像模式。
以创建Session会话并指定为NORMAL_PHOTO模式为例：
调整帧率
1.  需要在Session调用commitConfig完成配流之后调用。
```typescript
function getSupportedFrameRange(previewOutput: camera.PreviewOutput): Array<camera.FrameRateRange> {
// 获取支持的帧率范围，不同的硬件平台可能提供不同的帧率范围
return previewOutput.getSupportedFrameRates();
}
```
2.  根据实际开发需求，调用PreviewOutput类提供的setFrameRate接口对帧率进行动态调整。
```typescript
function setFrameRate(previewOutput: camera.PreviewOutput, minFps: number, maxFps: number): void {
try {
previewOutput.setFrameRate(minFps, maxFps);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to setFrameRate for previewOutput. error: ${JSON.stringify(err)}`);
}
}
```
3.  （可选）通过PreviewOutput类提供的getActiveFrameRate接口查询已设置过并生效的帧率。
```typescript
function getActiveFrameRange(previewOutput: camera.PreviewOutput): camera.FrameRateRange {
return previewOutput.getActiveFrameRate();
}
```
完整流程
根据上述开发流程，完整的session配流及previewOutput在session.start前后调整帧率示例代码如下。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-preconfig-V14
爬取时间: 2025-04-28 20:14:50
来源: Huawei Developer
相机预配置（Preconfig），对常用的场景和分辨率进行了预配置集成，可简化开发相机应用流程，提高应用的开发效率。
开发者在开发相机应用时，在获取到CameraDevice之后，如果遵循通用流程开发，步骤较为繁琐。需要先调用CameraManager的getSupportedOutputCapability来查询当前相机在指定模式下所支持的各类输出的配置信息，拿到CameraOutputCapability之后，应用开发者还需要对里面的各类数据进行解析，筛选，找到自己需要的配置数据Profile以及VideoProfile。最后使用对应的Profile以及VideoProfile创建对应的PreviewOutput、PhotoOutput以及VideoOutput。
为了解决上述问题，优化应用开发流程，系统针对拍照（PhotoSession）、录像（VideoSession）两类场景，提供了preconfig接口帮助开发者快速完成相机参数配置。推荐仅需要自定义拍照界面的无需开发专业相机应用的开发者，使用相机预配置功能快速开发应用。
以拍照（PhotoSession）为例，与遵循通用流程开发，有以下差异：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165944.86535995298388865938734172549541:50001231000000:2800:B29C52E9565DDF8F685A11CCE33A69671DA552211CE178DAC619C34498CD7164.png)
其他相关能力：
规格说明
系统提供了4种预配置类型（PreconfigType），分别为PRECONFIG_720P、PRECONFIG_1080P、PRECONFIG_4K、PRECONFIG_HIGH_QUALITY。以及3种画幅比例规格（PreconfigRatio），1:1画幅（PRECONFIG_RATIO_1_1）、4:3画幅（PRECONFIG_RATIO_4_3）、16:9画幅（PRECONFIG_RATIO_16_9）。
由于不同的设备所支持的能力不同。使用相机预配置（preconfig）功能时，需要先调用canPreconfig检查对应的PreconfigType和PreconfigRatio的组合在当前设备上是否支持。
在不同的画幅比例下，其分辨率规格不同，详见下表。
-  预配置类型PreconfigType PRECONFIG_RATIO_1_1 PRECONFIG_RATIO_4_3 PRECONFIG_RATIO_16_9 PRECONFIG_720P 720x720 960x720 1280x720 PRECONFIG_1080P 1080x1080 1440x1080 1920x1080 PRECONFIG_4K 1080x1080 1440x1080 1920x1080 PRECONFIG_HIGH_QUALITY 1440x1440 1920x1440 2560x1440
-  预配置类型PreconfigType PRECONFIG_RATIO_1_1 PRECONFIG_RATIO_4_3 PRECONFIG_RATIO_16_9 PRECONFIG_720P 720x720 960x720 1280x720 PRECONFIG_1080P 1080x1080 1440x1080 1920x1080 PRECONFIG_4K 2160x2160 2880x2160 3840x2160 PRECONFIG_HIGH_QUALITY 跟随Sensor（镜头）最大能力 跟随Sensor（镜头）最大能力 跟随Sensor（镜头）最大能力
-  预配置类型PreconfigType PRECONFIG_RATIO_1_1 PRECONFIG_RATIO_4_3 PRECONFIG_RATIO_16_9 PRECONFIG_720P 720x720 960x720 1280x720 PRECONFIG_1080P 1080x1080 1440x1080 1920x1080 PRECONFIG_4K 1080x1080 1440x1080 1920x1080 PRECONFIG_HIGH_QUALITY 1080x1080 1440x1080 1920x1080
-  预配置类型PreconfigType PRECONFIG_RATIO_1_1 PRECONFIG_RATIO_4_3 PRECONFIG_RATIO_16_9 PRECONFIG_720P 720x720 960x720 1280x720 PRECONFIG_1080P 1080x1080 1440x1080 1920x1080 PRECONFIG_4K 2160x2160 2880x2160 3840x2160 PRECONFIG_HIGH_QUALITY 2160x2160 2880x2160 3840x2160
-  预配置类型PreconfigType PRECONFIG_RATIO_1_1 PRECONFIG_RATIO_4_3 PRECONFIG_RATIO_16_9 PRECONFIG_720P 跟随Sensor（镜头）最大能力 跟随Sensor（镜头）最大能力 跟随Sensor（镜头）最大能力 PRECONFIG_1080P 跟随Sensor（镜头）最大能力 跟随Sensor（镜头）最大能力 跟随Sensor（镜头）最大能力 PRECONFIG_4K 跟随Sensor（镜头）最大能力 跟随Sensor（镜头）最大能力 跟随Sensor（镜头）最大能力 PRECONFIG_HIGH_QUALITY 跟随Sensor（镜头）最大能力 跟随Sensor（镜头）最大能力 跟随Sensor（镜头）最大能力
| 预配置类型PreconfigType  | PRECONFIG_RATIO_1_1  | PRECONFIG_RATIO_4_3  | PRECONFIG_RATIO_16_9  |
| --- | --- | --- | --- |
| PRECONFIG_720P  | 720x720  | 960x720  | 1280x720  |
| PRECONFIG_1080P  | 1080x1080  | 1440x1080  | 1920x1080  |
| PRECONFIG_4K  | 1080x1080  | 1440x1080  | 1920x1080  |
| PRECONFIG_HIGH_QUALITY  | 1440x1440  | 1920x1440  | 2560x1440  |
| 预配置类型PreconfigType  | PRECONFIG_RATIO_1_1  | PRECONFIG_RATIO_4_3  | PRECONFIG_RATIO_16_9  |
| --- | --- | --- | --- |
| PRECONFIG_720P  | 720x720  | 960x720  | 1280x720  |
| PRECONFIG_1080P  | 1080x1080  | 1440x1080  | 1920x1080  |
| PRECONFIG_4K  | 2160x2160  | 2880x2160  | 3840x2160  |
| PRECONFIG_HIGH_QUALITY  | 跟随Sensor（镜头）最大能力  | 跟随Sensor（镜头）最大能力  | 跟随Sensor（镜头）最大能力  |
| 预配置类型PreconfigType  | PRECONFIG_RATIO_1_1  | PRECONFIG_RATIO_4_3  | PRECONFIG_RATIO_16_9  |
| --- | --- | --- | --- |
| PRECONFIG_720P  | 720x720  | 960x720  | 1280x720  |
| PRECONFIG_1080P  | 1080x1080  | 1440x1080  | 1920x1080  |
| PRECONFIG_4K  | 1080x1080  | 1440x1080  | 1920x1080  |
| PRECONFIG_HIGH_QUALITY  | 1080x1080  | 1440x1080  | 1920x1080  |
| 预配置类型PreconfigType  | PRECONFIG_RATIO_1_1  | PRECONFIG_RATIO_4_3  | PRECONFIG_RATIO_16_9  |
| --- | --- | --- | --- |
| PRECONFIG_720P  | 720x720  | 960x720  | 1280x720  |
| PRECONFIG_1080P  | 1080x1080  | 1440x1080  | 1920x1080  |
| PRECONFIG_4K  | 2160x2160  | 2880x2160  | 3840x2160  |
| PRECONFIG_HIGH_QUALITY  | 2160x2160  | 2880x2160  | 3840x2160  |
| 预配置类型PreconfigType  | PRECONFIG_RATIO_1_1  | PRECONFIG_RATIO_4_3  | PRECONFIG_RATIO_16_9  |
| --- | --- | --- | --- |
| PRECONFIG_720P  | 跟随Sensor（镜头）最大能力  | 跟随Sensor（镜头）最大能力  | 跟随Sensor（镜头）最大能力  |
| PRECONFIG_1080P  | 跟随Sensor（镜头）最大能力  | 跟随Sensor（镜头）最大能力  | 跟随Sensor（镜头）最大能力  |
| PRECONFIG_4K  | 跟随Sensor（镜头）最大能力  | 跟随Sensor（镜头）最大能力  | 跟随Sensor（镜头）最大能力  |
| PRECONFIG_HIGH_QUALITY  | 跟随Sensor（镜头）最大能力  | 跟随Sensor（镜头）最大能力  | 跟随Sensor（镜头）最大能力  |
开发步骤
详细的API说明请参考Camera API参考。
1.  导入相关接口。
```typescript
import { camera } from '@kit.CameraKit';
import { BusinessError } from '@kit.BasicServicesKit';
```
2.  创建输出流Output。 此处以创建预览流和拍照流为例。
```typescript
// 创建预览输出流
let previewOutput: camera.PreviewOutput | undefined = undefined;
try {
previewOutput = cameraManager.createPreviewOutput(surfaceId);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to create the PreviewOutput instance. error code: ${err.code}`);
}
if (previewOutput === undefined) {
return;
}
// 创建拍照输出流
let photoOutput: camera.PhotoOutput | undefined = undefined;
try {
photoOutput = cameraManager.createPhotoOutput();
} catch (error) {
let err = error as BusinessError;
console.error('Failed to createPhotoOutput errorCode = ' + err.code);
}
if (photoOutput === undefined) {
return;
}
```
3.  调用CameraManager类中的createCameraInput方法，创建输入流Input。
```typescript
let cameraInput: camera.CameraInput | undefined = undefined;
try {
cameraInput = cameraManager.createCameraInput(cameraArray[0]);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to createCameraInput errorCode = ' + err.code);
}
if (cameraInput === undefined) {
return;
}
// 打开相机
await cameraInput.open();
```
4.  调用createSession创建会话（Session）。 SceneMode需要指定为NORMAL_PHOTO或NORMAL_VIDEO，对应拍照场景PhotoSession和录像场景VideoSession。
```typescript
//创建会话
let photoSession: camera.PhotoSession | undefined = undefined;
try {
photoSession = cameraManager.createSession(camera.SceneMode.NORMAL_PHOTO) as camera.PhotoSession;
} catch (error) {
let err = error as BusinessError;
console.error('Failed to create the session instance. errorCode = ' + err.code);
}
if (photoSession === undefined) {
return;
}
```
```typescript
// 查询Preconfig能力
try {
let isPreconfigSupport = photoSession.canPreconfig(camera.PreconfigType.PRECONFIG_1080P);
if (!isPreconfigSupport) {
console.error('PhotoSession canPreconfig check fail.');
return;
}
} catch (error) {
let err = error as BusinessError;
console.error('Failed to call canPreconfig. errorCode = ' + err.code);
return;
}
// 配置Preconfig
try {
photoSession.preconfig(camera.PreconfigType.PRECONFIG_1080P);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to call preconfig. errorCode = ' + err.code);
return;
}
```
5.  Session添加Input和Output。 Session调用preconfig接口成功之后，Session内部会将预置数据准备好，如果向Session中进行添加未配置Profile的Output，Session则会对相应的Output进行配置对应Profile。如果向Session中添加已配置Profile的Output，则Session的预配置数据不生效。
```typescript
// 开始配置会话
try {
photoSession.beginConfig();
} catch (error) {
let err = error as BusinessError;
console.error('Failed to beginConfig. errorCode = ' + err.code);
}
// 向会话中添加相机输入流
try {
photoSession.addInput(cameraInput);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to addInput. errorCode = ' + err.code);
}
// 向会话中添加预览输出流
try {
photoSession.addOutput(previewOutput);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to addOutput(previewOutput). errorCode = ' + err.code);
}
// 向会话中添加拍照输出流
try {
photoSession.addOutput(photoOutput);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to addOutput(photoOutput). errorCode = ' + err.code);
}
// 提交会话配置
await photoSession.commitConfig();
```
6.  启动Session。
```typescript
// 启动会话
await photoSession.start().then(() => {
console.info('Promise returned to indicate the session start success.');
});
```
完整示例
Context获取方式请参考：获取UIAbility的上下文信息。
```typescript
import { camera } from '@kit.CameraKit';
import { image } from '@kit.ImageKit';
import { BusinessError } from '@kit.BasicServicesKit';
import { common } from '@kit.AbilityKit';
import { fileIo as fs } from '@kit.CoreFileKit';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
let context = getContext(this);
async function savePicture(buffer: ArrayBuffer, img: image.Image): Promise<void> {
let accessHelper: photoAccessHelper.PhotoAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
let options: photoAccessHelper.CreateOptions = {
title: Date.now().toString()
};
let photoUri: string = await accessHelper.createAsset(photoAccessHelper.PhotoType.IMAGE, 'jpg', options);
let file: fs.File = fs.openSync(photoUri, fs.OpenMode.READ_WRITE | fs.OpenMode.CREATE);
await fs.write(file.fd, buffer);
fs.closeSync(file);
img.release();
}
function setPhotoOutputCb(photoOutput: camera.PhotoOutput): void {
//设置回调之后，调用photoOutput的capture方法，就会将拍照的buffer回传到回调中
photoOutput.on('photoAvailable', (errCode: BusinessError, photo: camera.Photo): void => {
console.info('getPhoto start');
console.info(`err: ${JSON.stringify(errCode)}`);
if (errCode || photo === undefined) {
console.error('getPhoto failed');
return;
}
let imageObj = photo.main;
imageObj.getComponent(image.ComponentType.JPEG, (errCode: BusinessError, component: image.Component): void => {
console.info('getComponent start');
if (errCode || component === undefined) {
console.error('getComponent failed');
return;
}
let buffer: ArrayBuffer;
if (component.byteBuffer) {
buffer = component.byteBuffer;
} else {
console.error('byteBuffer is null');
return;
}
savePicture(buffer, imageObj);
});
});
}
async function cameraShootingCase(baseContext: common.BaseContext, surfaceId: string): Promise<void> {
// 创建CameraManager对象
let cameraManager: camera.CameraManager = camera.getCameraManager(baseContext);
if (!cameraManager) {
console.error("camera.getCameraManager error");
return;
}
// 监听相机状态变化
cameraManager.on('cameraStatus', (err: BusinessError, cameraStatusInfo: camera.CameraStatusInfo) => {
if (err !== undefined && err.code !== 0) {
console.error('cameraStatus with errorCode = ' + err.code);
return;
}
console.info(`camera : ${cameraStatusInfo.camera.cameraId}`);
console.info(`status: ${cameraStatusInfo.status}`);
});
// 获取相机列表
let cameraArray: Array<camera.CameraDevice> = cameraManager.getSupportedCameras();
if (cameraArray.length <= 0) {
console.error("cameraManager.getSupportedCameras error");
return;
}
for (let index = 0; index < cameraArray.length; index++) {
console.info('cameraId : ' + cameraArray[index].cameraId); // 获取相机ID
console.info('cameraPosition : ' + cameraArray[index].cameraPosition); // 获取相机位置
console.info('cameraType : ' + cameraArray[index].cameraType); // 获取相机类型
console.info('connectionType : ' + cameraArray[index].connectionType); // 获取相机连接类型
}
// 创建相机输入流
let cameraInput: camera.CameraInput | undefined = undefined;
try {
cameraInput = cameraManager.createCameraInput(cameraArray[0]);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to createCameraInput errorCode = ' + err.code);
}
if (cameraInput === undefined) {
return;
}
// 监听cameraInput错误信息
let cameraDevice: camera.CameraDevice = cameraArray[0];
cameraInput.on('error', cameraDevice, (error: BusinessError) => {
console.error(`Camera input error code: ${error.code}`);
})
// 打开相机
await cameraInput.open();
// 获取支持的模式类型
let sceneModes: Array<camera.SceneMode> = cameraManager.getSupportedSceneModes(cameraArray[0]);
let isSupportPhotoMode: boolean = sceneModes.indexOf(camera.SceneMode.NORMAL_PHOTO) >= 0;
if (!isSupportPhotoMode) {
console.error('photo mode not support');
return;
}
//创建会话
let photoSession: camera.PhotoSession | undefined = undefined;
try {
photoSession = cameraManager.createSession(camera.SceneMode.NORMAL_PHOTO) as camera.PhotoSession;
} catch (error) {
let err = error as BusinessError;
console.error('Failed to create the session instance. errorCode = ' + err.code);
}
if (photoSession === undefined) {
return;
}
// 监听session错误信息
photoSession.on('error', (error: BusinessError) => {
console.error(`Capture session error code: ${error.code}`);
});
// 查询Preconfig能力
try {
let isPreconfigSupport = photoSession.canPreconfig(camera.PreconfigType.PRECONFIG_1080P);
if (!isPreconfigSupport) {
console.error('PhotoSession canPreconfig check fail.');
return;
}
} catch (error) {
let err = error as BusinessError;
console.error('Failed to call canPreconfig. errorCode = ' + err.code);
return;
}
// 配置Preconfig
try {
photoSession.preconfig(camera.PreconfigType.PRECONFIG_1080P);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to call preconfig. errorCode = ' + err.code);
return;
}
// 创建预览输出流,其中参数 surfaceId 参考上文 XComponent 组件，预览流为XComponent组件提供的surface
let previewOutput: camera.PreviewOutput | undefined = undefined;
try {
previewOutput = cameraManager.createPreviewOutput(surfaceId);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to create the PreviewOutput instance. error code: ${err.code}`);
}
if (previewOutput === undefined) {
return;
}
// 监听预览输出错误信息
previewOutput.on('error', (error: BusinessError) => {
console.error(`Preview output error code: ${error.code}`);
});
// 创建拍照输出流
let photoOutput: camera.PhotoOutput | undefined = undefined;
try {
photoOutput = cameraManager.createPhotoOutput();
} catch (error) {
let err = error as BusinessError;
console.error('Failed to createPhotoOutput errorCode = ' + err.code);
}
if (photoOutput === undefined) {
return;
}
//调用上面的回调函数来保存图片
setPhotoOutputCb(photoOutput);
// 开始配置会话
try {
photoSession.beginConfig();
} catch (error) {
let err = error as BusinessError;
console.error('Failed to beginConfig. errorCode = ' + err.code);
}
// 向会话中添加相机输入流
try {
photoSession.addInput(cameraInput);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to addInput. errorCode = ' + err.code);
}
// 向会话中添加预览输出流
try {
photoSession.addOutput(previewOutput);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to addOutput(previewOutput). errorCode = ' + err.code);
}
// 向会话中添加拍照输出流
try {
photoSession.addOutput(photoOutput);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to addOutput(photoOutput). errorCode = ' + err.code);
}
// 提交会话配置
await photoSession.commitConfig();
// 启动会话
await photoSession.start().then(() => {
console.info('Promise returned to indicate the session start success.');
});
// 判断设备是否支持闪光灯
let flashStatus: boolean = false;
try {
flashStatus = photoSession.hasFlash();
} catch (error) {
let err = error as BusinessError;
console.error('Failed to hasFlash. errorCode = ' + err.code);
}
console.info('Returned with the flash light support status:' + flashStatus);
if (flashStatus) {
// 判断是否支持自动闪光灯模式
let flashModeStatus: boolean = false;
try {
let status: boolean = photoSession.isFlashModeSupported(camera.FlashMode.FLASH_MODE_AUTO);
flashModeStatus = status;
} catch (error) {
let err = error as BusinessError;
console.error('Failed to check whether the flash mode is supported. errorCode = ' + err.code);
}
if(flashModeStatus) {
// 设置自动闪光灯模式
try {
photoSession.setFlashMode(camera.FlashMode.FLASH_MODE_AUTO);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to set the flash mode. errorCode = ' + err.code);
}
}
}
// 判断是否支持连续自动变焦模式
let focusModeStatus: boolean = false;
try {
let status: boolean = photoSession.isFocusModeSupported(camera.FocusMode.FOCUS_MODE_CONTINUOUS_AUTO);
focusModeStatus = status;
} catch (error) {
let err = error as BusinessError;
console.error('Failed to check whether the focus mode is supported. errorCode = ' + err.code);
}
if (focusModeStatus) {
// 设置连续自动变焦模式
try {
photoSession.setFocusMode(camera.FocusMode.FOCUS_MODE_CONTINUOUS_AUTO);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to set the focus mode. errorCode = ' + err.code);
}
}
// 获取相机支持的可变焦距比范围
let zoomRatioRange: Array<number> = [];
try {
zoomRatioRange = photoSession.getZoomRatioRange();
} catch (error) {
let err = error as BusinessError;
console.error('Failed to get the zoom ratio range. errorCode = ' + err.code);
}
if (zoomRatioRange.length <= 0) {
return;
}
// 设置可变焦距比
try {
photoSession.setZoomRatio(zoomRatioRange[0]);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to set the zoom ratio value. errorCode = ' + err.code);
}
let photoCaptureSetting: camera.PhotoCaptureSetting = {
quality: camera.QualityLevel.QUALITY_LEVEL_HIGH, // 设置图片质量高
rotation: camera.ImageRotation.ROTATION_0 // 设置图片旋转角度0
}
// 使用当前拍照设置进行拍照
photoOutput.capture(photoCaptureSetting, (err: BusinessError) => {
if (err) {
console.error(`Failed to capture the photo ${err.message}`);
return;
}
console.info('Callback invoked to indicate the photo capture request success.');
});
// 需要在拍照结束之后调用以下关闭摄像头和释放会话流程，避免拍照未结束就将会话释放。
// 停止当前会话
await photoSession.stop();
// 释放相机输入流
await cameraInput.close();
// 释放预览输出流
await previewOutput.release();
// 释放拍照输出流
await photoOutput.release();
// 释放会话
await photoSession.release();
// 会话置空
photoSession = undefined;
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-best-practices-arkts-V14
爬取时间: 2025-04-28 20:15:04
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-picker-V14
爬取时间: 2025-04-28 20:15:17
来源: Huawei Developer
应用可调用CameraPicker拍摄照片或录制视频，无需申请相机权限。
CameraPicker的相机交互界面由系统提供，在用户点击拍摄和确认按钮后，调用CameraPicker的应用获取对应的照片或者视频。
应用开发者如果只是需要获取即时拍摄的照片或者视频，则可以使用CameraPicker能力来轻松实现。
由于照片的拍摄和确认都是由用户进行主动确认，因此应用开发者可以不用申请操作相机的相关权限。
开发步骤
详细的API说明请参考CameraPicker API参考。
1.  导入相关接口，导入方法如下。
```typescript
import { camera, cameraPicker as picker } from '@kit.CameraKit'
import { fileIo, fileUri } from '@kit.CoreFileKit'
```
2.  配置PickerProfile。 PickerProfile的saveUri为可选参数，如果未配置该项，拍摄的照片和视频默认存入媒体库中。 如果不想将照片和视频存入媒体库，请自行配置应用沙箱内的文件路径。 应用沙箱内的这个文件必须是一个存在的、可写的文件。这个文件的uri传入picker接口之后，相当于应用给系统相机授权该文件的读写权限。系统相机在拍摄结束之后，会对此文件进行覆盖写入。
```typescript
let pathDir = getContext().filesDir;
let fileName = `${new Date().getTime()}`
let filePath = pathDir + `/${fileName}.tmp`
fileIo.createRandomAccessFileSync(filePath, fileIo.OpenMode.CREATE);
let uri = fileUri.getUriFromPath(filePath);
let pickerProfile: picker.PickerProfile = {
cameraPosition: camera.CameraPosition.CAMERA_POSITION_BACK,
saveUri: uri
};
```
3.  调用picker拍摄接口获取拍摄的结果。
```typescript
let result: picker.PickerResult =
await picker.pick(getContext(), [picker.PickerMediaType.PHOTO, picker.PickerMediaType.VIDEO],
pickerProfile);
console.info(`picker resultCode: ${result.resultCode},resultUri: ${result.resultUri},mediaType: ${result.mediaType}`);
```
完整示例
```typescript
import { camera, cameraPicker as picker } from '@kit.CameraKit'
import { fileIo, fileUri } from '@kit.CoreFileKit'
@Entry
@Component
struct Index {
@State imgSrc: string = '';
@State videoSrc: string = '';
build() {
RelativeContainer() {
Column() {
Image(this.imgSrc).width(200).height(200).backgroundColor(Color.Black).margin(5);
Video({ src: this.videoSrc}).width(200).height(200).autoPlay(true);
Button("Test Picker Photo&Video").fontSize(20)
.fontWeight(FontWeight.Bold)
.onClick(async () => {
let pathDir = getContext().filesDir;
let fileName = `${new Date().getTime()}`
let filePath = pathDir + `/${fileName}.tmp`
fileIo.createRandomAccessFileSync(filePath, fileIo.OpenMode.CREATE);
let uri = fileUri.getUriFromPath(filePath);
let pickerProfile: picker.PickerProfile = {
cameraPosition: camera.CameraPosition.CAMERA_POSITION_BACK,
saveUri: uri
};
let result: picker.PickerResult =
await picker.pick(getContext(), [picker.PickerMediaType.PHOTO, picker.PickerMediaType.VIDEO],
pickerProfile);
console.info(`picker resultCode: ${result.resultCode},resultUri: ${result.resultUri},mediaType: ${result.mediaType}`);
if (result.resultCode == 0) {
if (result.mediaType === picker.PickerMediaType.PHOTO) {
this.imgSrc = result.resultUri;
} else {
this.videoSrc = result.resultUri;
}
}
}).margin(5);
}.alignRules({
center: { anchor: '__container__', align: VerticalAlign.Center },
middle: { anchor: '__container__', align: HorizontalAlign.Center }
});
}
.height('100%')
.width('100%')
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-shooting-case-V14
爬取时间: 2025-04-28 20:15:31
来源: Huawei Developer
在开发相机应用时，需要先参考开发准备申请相关权限。
当前示例提供完整的拍照流程介绍，方便开发者了解完整的接口调用顺序。
在参考以下示例前，建议开发者查看相机开发指导(ArkTS)的具体章节，了解设备输入、会话管理、拍照等单个流程。
开发流程
在获取到相机支持的输出流能力后，开始创建拍照流，开发流程如下。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165945.27810039909797617621033230914503:50001231000000:2800:CE54C6E092951AB807DC9738964A6775E0BB9981D9EADED8C78FACC6832C7A0D.png)
完整示例
Context获取方式请参考：获取UIAbility的上下文信息。
如需要在图库中看到所保存的图片、视频资源，需要将其保存到媒体库，保存方式请参考：保存媒体库资源。
需要在photoOutput.on('photoAvailable')接口获取到buffer时，将buffer在安全控件中保存到媒体库。
```typescript
import { camera } from '@kit.CameraKit';
import { image } from '@kit.ImageKit';
import { BusinessError } from '@kit.BasicServicesKit';
import { common } from '@kit.AbilityKit';
import { fileIo as fs } from '@kit.CoreFileKit';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
let context = getContext(this);
function setPhotoOutputCb(photoOutput: camera.PhotoOutput): void {
//设置回调之后，调用photoOutput的capture方法，就会将拍照的buffer回传到回调中
photoOutput.on('photoAvailable', (errCode: BusinessError, photo: camera.Photo): void => {
console.info('getPhoto start');
console.info(`err: ${JSON.stringify(errCode)}`);
if (errCode || photo === undefined) {
console.error('getPhoto failed');
return;
}
let imageObj = photo.main;
imageObj.getComponent(image.ComponentType.JPEG, (errCode: BusinessError, component: image.Component): void => {
console.info('getComponent start');
if (errCode || component === undefined) {
console.error('getComponent failed');
return;
}
let buffer: ArrayBuffer;
if (component.byteBuffer) {
buffer = component.byteBuffer;
} else {
console.error('byteBuffer is null');
return;
}
// 如需要在图库中看到所保存的图片、视频资源，请使用用户无感的安全控件创建媒体资源。
// buffer处理结束后需要释放该资源，如果未正确释放资源会导致后续拍照获取不到buffer
imageObj.release();
});
});
}
async function cameraShootingCase(baseContext: common.BaseContext, surfaceId: string): Promise<void> {
// 创建CameraManager对象
let cameraManager: camera.CameraManager = camera.getCameraManager(baseContext);
if (!cameraManager) {
console.error("camera.getCameraManager error");
return;
}
// 监听相机状态变化
cameraManager.on('cameraStatus', (err: BusinessError, cameraStatusInfo: camera.CameraStatusInfo) => {
if (err !== undefined && err.code !== 0) {
console.error('cameraStatus with errorCode = ' + err.code);
return;
}
console.info(`camera : ${cameraStatusInfo.camera.cameraId}`);
console.info(`status: ${cameraStatusInfo.status}`);
});
// 获取相机列表
let cameraArray: Array<camera.CameraDevice> = cameraManager.getSupportedCameras();
if (cameraArray.length <= 0) {
console.error("cameraManager.getSupportedCameras error");
return;
}
for (let index = 0; index < cameraArray.length; index++) {
console.info('cameraId : ' + cameraArray[index].cameraId);                          // 获取相机ID
console.info('cameraPosition : ' + cameraArray[index].cameraPosition);              // 获取相机位置
console.info('cameraType : ' + cameraArray[index].cameraType);                      // 获取相机类型
console.info('connectionType : ' + cameraArray[index].connectionType);              // 获取相机连接类型
}
// 创建相机输入流
let cameraInput: camera.CameraInput | undefined = undefined;
try {
cameraInput = cameraManager.createCameraInput(cameraArray[0]);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to createCameraInput errorCode = ' + err.code);
}
if (cameraInput === undefined) {
return;
}
// 监听cameraInput错误信息
let cameraDevice: camera.CameraDevice = cameraArray[0];
cameraInput.on('error', cameraDevice, (error: BusinessError) => {
console.error(`Camera input error code: ${error.code}`);
})
// 打开相机
await cameraInput.open();
// 获取支持的模式类型
let sceneModes: Array<camera.SceneMode> = cameraManager.getSupportedSceneModes(cameraArray[0]);
let isSupportPhotoMode: boolean = sceneModes.indexOf(camera.SceneMode.NORMAL_PHOTO) >= 0;
if (!isSupportPhotoMode) {
console.error('photo mode not support');
return;
}
// 获取相机设备支持的输出流能力
let cameraOutputCap: camera.CameraOutputCapability = cameraManager.getSupportedOutputCapability(cameraArray[0], camera.SceneMode.NORMAL_PHOTO);
if (!cameraOutputCap) {
console.error("cameraManager.getSupportedOutputCapability error");
return;
}
console.info("outputCapability: " + JSON.stringify(cameraOutputCap));
let previewProfilesArray: Array<camera.Profile> = cameraOutputCap.previewProfiles;
if (!previewProfilesArray) {
console.error("createOutput previewProfilesArray == null || undefined");
}
let photoProfilesArray: Array<camera.Profile> = cameraOutputCap.photoProfiles;
if (!photoProfilesArray) {
console.error("createOutput photoProfilesArray == null || undefined");
}
// 创建预览输出流,其中参数 surfaceId 参考上文 XComponent 组件，预览流为XComponent组件提供的surface
let previewOutput: camera.PreviewOutput | undefined = undefined;
try {
previewOutput = cameraManager.createPreviewOutput(previewProfilesArray[0], surfaceId);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to create the PreviewOutput instance. error code: ${err.code}`);
}
if (previewOutput === undefined) {
return;
}
// 监听预览输出错误信息
previewOutput.on('error', (error: BusinessError) => {
console.error(`Preview output error code: ${error.code}`);
});
// 创建拍照输出流
let photoOutput: camera.PhotoOutput | undefined = undefined;
try {
photoOutput = cameraManager.createPhotoOutput(photoProfilesArray[0]);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to createPhotoOutput errorCode = ' + err.code);
}
if (photoOutput === undefined) {
return;
}
//调用上面的回调函数来保存图片
setPhotoOutputCb(photoOutput);
//创建会话
let photoSession: camera.PhotoSession | undefined = undefined;
try {
photoSession = cameraManager.createSession(camera.SceneMode.NORMAL_PHOTO) as camera.PhotoSession;
} catch (error) {
let err = error as BusinessError;
console.error('Failed to create the session instance. errorCode = ' + err.code);
}
if (photoSession === undefined) {
return;
}
// 监听session错误信息
photoSession.on('error', (error: BusinessError) => {
console.error(`Capture session error code: ${error.code}`);
});
// 开始配置会话
try {
photoSession.beginConfig();
} catch (error) {
let err = error as BusinessError;
console.error('Failed to beginConfig. errorCode = ' + err.code);
}
// 向会话中添加相机输入流
try {
photoSession.addInput(cameraInput);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to addInput. errorCode = ' + err.code);
}
// 向会话中添加预览输出流
try {
photoSession.addOutput(previewOutput);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to addOutput(previewOutput). errorCode = ' + err.code);
}
// 向会话中添加拍照输出流
try {
photoSession.addOutput(photoOutput);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to addOutput(photoOutput). errorCode = ' + err.code);
}
// 提交会话配置
await photoSession.commitConfig();
// 启动会话
await photoSession.start().then(() => {
console.info('Promise returned to indicate the session start success.');
});
// 判断设备是否支持闪光灯
let flashStatus: boolean = false;
try {
flashStatus = photoSession.hasFlash();
} catch (error) {
let err = error as BusinessError;
console.error('Failed to hasFlash. errorCode = ' + err.code);
}
console.info('Returned with the flash light support status:' + flashStatus);
if (flashStatus) {
// 判断是否支持自动闪光灯模式
let flashModeStatus: boolean = false;
try {
let status: boolean = photoSession.isFlashModeSupported(camera.FlashMode.FLASH_MODE_AUTO);
flashModeStatus = status;
} catch (error) {
let err = error as BusinessError;
console.error('Failed to check whether the flash mode is supported. errorCode = ' + err.code);
}
if(flashModeStatus) {
// 设置自动闪光灯模式
try {
photoSession.setFlashMode(camera.FlashMode.FLASH_MODE_AUTO);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to set the flash mode. errorCode = ' + err.code);
}
}
}
// 判断是否支持连续自动变焦模式
let focusModeStatus: boolean = false;
try {
let status: boolean = photoSession.isFocusModeSupported(camera.FocusMode.FOCUS_MODE_CONTINUOUS_AUTO);
focusModeStatus = status;
} catch (error) {
let err = error as BusinessError;
console.error('Failed to check whether the focus mode is supported. errorCode = ' + err.code);
}
if (focusModeStatus) {
// 设置连续自动变焦模式
try {
photoSession.setFocusMode(camera.FocusMode.FOCUS_MODE_CONTINUOUS_AUTO);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to set the focus mode. errorCode = ' + err.code);
}
}
// 获取相机支持的可变焦距比范围
let zoomRatioRange: Array<number> = [];
try {
zoomRatioRange = photoSession.getZoomRatioRange();
} catch (error) {
let err = error as BusinessError;
console.error('Failed to get the zoom ratio range. errorCode = ' + err.code);
}
if (zoomRatioRange.length <= 0) {
return;
}
// 设置可变焦距比
try {
photoSession.setZoomRatio(zoomRatioRange[0]);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to set the zoom ratio value. errorCode = ' + err.code);
}
let photoCaptureSetting: camera.PhotoCaptureSetting = {
quality: camera.QualityLevel.QUALITY_LEVEL_HIGH, // 设置图片质量高
rotation: camera.ImageRotation.ROTATION_0 // 设置图片旋转角度0
}
// 使用当前拍照设置进行拍照
photoOutput.capture(photoCaptureSetting, (err: BusinessError) => {
if (err) {
console.error(`Failed to capture the photo ${err.message}`);
return;
}
console.info('Callback invoked to indicate the photo capture request success.');
});
// 需要在拍照结束之后调用以下关闭摄像头和释放会话流程，避免拍照未结束就将会话释放。
// 停止当前会话
await photoSession.stop();
// 释放相机输入流
await cameraInput.close();
// 释放预览输出流
await previewOutput.release();
// 释放拍照输出流
await photoOutput.release();
// 释放会话
await photoSession.release();
// 会话置空
photoSession = undefined;
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-recording-case-V14
爬取时间: 2025-04-28 20:15:45
来源: Huawei Developer
在开发相机应用时，需要先参考开发准备申请相关权限。
当前示例提供完整的录像流程介绍，方便开发者了解完整的接口调用顺序。
在参考以下示例前，建议开发者查看相机开发指导(ArkTS)的具体章节，了解设备输入、会话管理、录像等单个流程。
如需要将视频保存到媒体库中可参考保存媒体库资源。
开发流程
在获取到相机支持的输出流能力后，开始创建录像流，开发流程如下。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165945.16419414837099042382371989641070:50001231000000:2800:42A1F64176438F3CFA82FB74C5205A7CA342E82AA55AD316CE08E8D1CE0A8BCC.png)
完整示例
Context获取方式请参考：获取UIAbility的上下文信息。
```typescript
import { camera } from '@kit.CameraKit';
import { BusinessError } from '@kit.BasicServicesKit';
import { media } from '@kit.MediaKit';
import { common } from '@kit.AbilityKit';
import { fileIo as fs } from '@kit.CoreFileKit';
async function videoRecording(context: common.Context, surfaceId: string): Promise<void> {
// 创建CameraManager对象
let cameraManager: camera.CameraManager = camera.getCameraManager(context);
if (!cameraManager) {
console.error("camera.getCameraManager error");
return;
}
// 监听相机状态变化
cameraManager.on('cameraStatus', (err: BusinessError, cameraStatusInfo: camera.CameraStatusInfo) => {
if (err !== undefined && err.code !== 0) {
console.error('cameraStatus with errorCode = ' + err.code);
return;
}
console.info(`camera : ${cameraStatusInfo.camera.cameraId}`);
console.info(`status: ${cameraStatusInfo.status}`);
});
// 获取相机列表
let cameraArray: Array<camera.CameraDevice> = [];
try {
cameraArray = cameraManager.getSupportedCameras();
} catch (error) {
let err = error as BusinessError;
console.error(`getSupportedCameras call failed. error code: ${err.code}`);
}
if (cameraArray.length <= 0) {
console.error("cameraManager.getSupportedCameras error");
return;
}
// 获取支持的模式类型
let sceneModes: Array<camera.SceneMode> = cameraManager.getSupportedSceneModes(cameraArray[0]);
let isSupportVideoMode: boolean = sceneModes.indexOf(camera.SceneMode.NORMAL_VIDEO) >= 0;
if (!isSupportVideoMode) {
console.error('video mode not support');
return;
}
// 获取相机设备支持的输出流能力
let cameraOutputCap: camera.CameraOutputCapability = cameraManager.getSupportedOutputCapability(cameraArray[0], camera.SceneMode.NORMAL_VIDEO);
if (!cameraOutputCap) {
console.error("cameraManager.getSupportedOutputCapability error")
return;
}
console.info("outputCapability: " + JSON.stringify(cameraOutputCap));
let previewProfilesArray: Array<camera.Profile> = cameraOutputCap.previewProfiles;
if (!previewProfilesArray) {
console.error("createOutput previewProfilesArray == null || undefined");
}
let photoProfilesArray: Array<camera.Profile> = cameraOutputCap.photoProfiles;
if (!photoProfilesArray) {
console.error("createOutput photoProfilesArray == null || undefined");
}
let videoProfilesArray: Array<camera.VideoProfile> = cameraOutputCap.videoProfiles;
if (!videoProfilesArray || videoProfilesArray.length === 0) {
console.error("createOutput videoProfilesArray == null || undefined");
}
// videoProfile的宽高需要与AVRecorderProfile的宽高保持一致，并且需要使用AVRecorderProfile所支持的宽高
// 示例代码默认选择第一个videoProfile，实际开发需根据所需筛选videoProfile
let videoProfile: camera.VideoProfile = videoProfilesArray[0];
let isHdr = videoProfile.format === camera.CameraFormat.CAMERA_FORMAT_YCBCR_P010 || videoProfile.format === camera.CameraFormat.CAMERA_FORMAT_YCRCB_P010;
// 配置参数以实际硬件设备支持的范围为准
let aVRecorderProfile: media.AVRecorderProfile = {
audioBitrate: 48000,
audioChannels: 2,
audioCodec: media.CodecMimeType.AUDIO_AAC,
audioSampleRate: 48000,
fileFormat: media.ContainerFormatType.CFT_MPEG_4,
videoBitrate: 2000000,
videoCodec: isHdr ? media.CodecMimeType.VIDEO_HEVC : media.CodecMimeType.VIDEO_AVC,
videoFrameWidth: videoProfile.size.width,
videoFrameHeight: videoProfile.size.height,
videoFrameRate: 30,
isHdr: isHdr
};
let videoUri: string = `file://${context.filesDir}/${Date.now()}.mp4`; // 本地沙箱路径
let file: fs.File = fs.openSync(videoUri, fs.OpenMode.READ_WRITE | fs.OpenMode.CREATE);
let aVRecorderConfig: media.AVRecorderConfig = {
audioSourceType: media.AudioSourceType.AUDIO_SOURCE_TYPE_MIC,
videoSourceType: media.VideoSourceType.VIDEO_SOURCE_TYPE_SURFACE_YUV,
profile: aVRecorderProfile,
url: `fd://${file.fd.toString()}`, // 文件需先由调用者创建，赋予读写权限，将文件fd传给此参数，eg.fd://45--file:///data/media/01.mp4
rotation: 0, // 合理值0、90、180、270，非合理值prepare接口将报错
location: { latitude: 30, longitude: 130 }
};
let avRecorder: media.AVRecorder | undefined = undefined;
try {
avRecorder = await media.createAVRecorder();
} catch (error) {
let err = error as BusinessError;
console.error(`createAVRecorder call failed. error code: ${err.code}`);
}
if (avRecorder === undefined) {
return;
}
try {
await avRecorder.prepare(aVRecorderConfig);
} catch (error) {
let err = error as BusinessError;
console.error(`prepare call failed. error code: ${err.code}`);
}
let videoSurfaceId: string | undefined = undefined; // 该surfaceID用于传递给相机接口创造videoOutput
try {
videoSurfaceId = await avRecorder.getInputSurface();
} catch (error) {
let err = error as BusinessError;
console.error(`getInputSurface call failed. error code: ${err.code}`);
}
if (videoSurfaceId === undefined) {
return;
}
// 创建VideoOutput对象
let videoOutput: camera.VideoOutput | undefined = undefined;
try {
videoOutput = cameraManager.createVideoOutput(videoProfile, videoSurfaceId);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to create the videoOutput instance. error: ${JSON.stringify(err)}`);
}
if (videoOutput === undefined) {
return;
}
// 监听视频输出错误信息
videoOutput.on('error', (error: BusinessError) => {
console.error(`Preview output error code: ${error.code}`);
});
//创建会话
let videoSession: camera.VideoSession | undefined = undefined;
try {
videoSession = cameraManager.createSession(camera.SceneMode.NORMAL_VIDEO) as camera.VideoSession;
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to create the session instance. error: ${JSON.stringify(err)}`);
}
if (videoSession === undefined) {
return;
}
// 监听session错误信息
videoSession.on('error', (error: BusinessError) => {
console.error(`Video session error code: ${error.code}`);
});
// 开始配置会话
try {
videoSession.beginConfig();
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to beginConfig. error: ${JSON.stringify(err)}`);
}
// 创建相机输入流
let cameraInput: camera.CameraInput | undefined = undefined;
try {
cameraInput = cameraManager.createCameraInput(cameraArray[0]);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to createCameraInput. error: ${JSON.stringify(err)}`);
}
if (cameraInput === undefined) {
return;
}
// 监听cameraInput错误信息
let cameraDevice: camera.CameraDevice = cameraArray[0];
cameraInput.on('error', cameraDevice, (error: BusinessError) => {
console.error(`Camera input error code: ${error.code}`);
});
// 打开相机
try {
await cameraInput.open();
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to open cameraInput. error: ${JSON.stringify(err)}`);
}
// 向会话中添加相机输入流
try {
videoSession.addInput(cameraInput);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to add cameraInput. error: ${JSON.stringify(err)}`);
}
// 创建预览输出流，其中参数 surfaceId 参考下面 XComponent 组件，预览流为XComponent组件提供的surface
let previewOutput: camera.PreviewOutput | undefined = undefined;
let previewProfile = previewProfilesArray.find((previewProfile: camera.Profile) => {
return Math.abs((previewProfile.size.width / previewProfile.size.height) - (videoProfile.size.width / videoProfile.size.height)) < Number.EPSILON;
}); // 筛选与录像分辨率宽高比一致的预览分辨率
if (previewProfile === undefined) {
return;
}
try {
previewOutput = cameraManager.createPreviewOutput(previewProfile, surfaceId);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to create the PreviewOutput instance. error: ${JSON.stringify(err)}`);
}
if (previewOutput === undefined) {
return;
}
// 向会话中添加预览输出流
try {
videoSession.addOutput(previewOutput);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to add previewOutput. error: ${JSON.stringify(err)}`);
}
// 向会话中添加录像输出流
try {
videoSession.addOutput(videoOutput);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to add videoOutput. error: ${JSON.stringify(err)}`);
}
// 提交会话配置
try {
await videoSession.commitConfig();
} catch (error) {
let err = error as BusinessError;
console.error(`videoSession commitConfig error: ${JSON.stringify(err)}`);
}
// 启动会话
try {
await videoSession.start();
} catch (error) {
let err = error as BusinessError;
console.error(`videoSession start error: ${JSON.stringify(err)}`);
}
// 启动录像输出流
videoOutput.start((err: BusinessError) => {
if (err) {
console.error(`Failed to start the video output. error: ${JSON.stringify(err)}`);
return;
}
console.info('Callback invoked to indicate the video output start success.');
});
// 开始录像
try {
await avRecorder.start();
} catch (error) {
let err = error as BusinessError;
console.error(`avRecorder start error: ${JSON.stringify(err)}`);
}
// 停止录像输出流
videoOutput.stop((err: BusinessError) => {
if (err) {
console.error(`Failed to stop the video output. error: ${JSON.stringify(err)}`);
return;
}
console.info('Callback invoked to indicate the video output stop success.');
});
// 停止录像
try {
await avRecorder.stop();
} catch (error) {
let err = error as BusinessError;
console.error(`avRecorder stop error: ${JSON.stringify(err)}`);
}
// 停止当前会话
await videoSession.stop();
// 关闭文件
fs.closeSync(file);
// 释放相机输入流
await cameraInput.close();
// 释放预览输出流
await previewOutput.release();
// 释放录像输出流
await videoOutput.release();
// 释放会话
await videoSession.release();
// 会话置空
videoSession = undefined;
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-dual-channel-preview-V14
爬取时间: 2025-04-28 20:15:59
来源: Huawei Developer
在开发相机应用时，需要先参考开发准备申请相关权限。
双路预览，即应用可同时使用两路预览流，一路用于在屏幕上显示，一路用于图像处理等其他操作，提升处理效率。
相机应用通过控制相机，实现图像显示（预览）、照片保存（拍照）、视频录制（录像）等基础操作。相机开发模型为Surface模型，即应用通过Surface进行数据传递，通过ImageReceiver的surface获取拍照流的数据、通过XComponent的surface获取预览流的数据。
如果要实现双路预览，即将拍照流改为预览流，将拍照流中的surface改为预览流的surface，通过ImageReceiver的surface创建previewOutput，其余流程与拍照流和预览流一致。
详细的API说明请参考Camera API参考。
约束与限制
调用流程
双路方案调用流程图建议如下：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165945.32499304937450200810001054877348:50001231000000:2800:B910A5BC4959DA44FB95ACBF597DCA07F64916DC33BA5F99825AB7ED807394B9.png)
开发步骤
用于处理图像的第一路预览流
1.  获取第一路预览流SurfaceId：创建ImageReceiver对象，通过ImageReceiver对象可获取其SurfaceId。
```typescript
import { image } from '@kit.ImageKit';
let imageWidth: number = 1920; // 请使用设备支持profile的size的宽。
let imageHeight: number = 1080; // 请使用设备支持profile的size的高。
async function initImageReceiver():Promise<void>{
// 创建ImageReceiver对象
let size: image.Size = { width: imageWidth, height: imageHeight };
let imageReceiver = image.createImageReceiver(size, image.ImageFormat.JPEG, 8);
// 获取取第一路流SurfaceId
let imageReceiverSurfaceId = await imageReceiver.getReceivingSurfaceId();
console.info(`initImageReceiver imageReceiverSurfaceId:${imageReceiverSurfaceId}`);
}
```
2.  注册监听处理预览流每帧图像数据：通过ImageReceiver组件中imageArrival事件监听获取底层返回的图像数据，详细的API说明请参考Image API参考。 通过image.Component解析图片buffer数据参考： 需要确认图像的宽width是否与行距rowStride一致，如果不一致可参考以下方式处理： 方式一：去除imgComponent.byteBuffer中stride数据，拷贝得到新的buffer，调用不支持stride的接口处理buffer。 方式二：根据stride*height创建pixelMap，然后调用pixelMap的cropSync方法裁剪掉多余的像素。 方式三：将原始imgComponent.byteBuffer和stride信息一起传给支持stride的接口处理。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
import { image } from '@kit.ImageKit';
function onImageArrival(receiver: image.ImageReceiver): void {
// 注册imageArrival监听
receiver.on('imageArrival', () => {
// 获取图像
receiver.readNextImage((err: BusinessError, nextImage: image.Image) => {
if (err || nextImage === undefined) {
console.error('readNextImage failed');
return;
}
// 解析图像内容
nextImage.getComponent(image.ComponentType.JPEG, async (err: BusinessError, imgComponent: image.Component) => {
if (err || imgComponent === undefined) {
console.error('getComponent failed');
}
if (imgComponent.byteBuffer) {
// 详情见下方解析图片buffer数据参考，本示例以方式一为例
let width = nextImage.size.width; // 获取图片的宽
let height = nextImage.size.height; // 获取图片的高
let stride = imgComponent.rowStride; // 获取图片的stride
console.debug(`getComponent with width:${width} height:${height} stride:${stride}`);
// stride与width一致
if (stride == width) {
let pixelMap = await image.createPixelMap(imgComponent.byteBuffer, {
size: { height: height, width: width },
srcPixelFormat: 8,
})
} else {
// stride与width不一致
const dstBufferSize = width * height * 1.5
const dstArr = new Uint8Array(dstBufferSize)
for (let j = 0; j < height * 1.5; j++) {
const srcBuf = new Uint8Array(imgComponent.byteBuffer, j * stride, width)
dstArr.set(srcBuf, j * width)
}
let pixelMap = await image.createPixelMap(dstArr.buffer, {
size: { height: height, width: width },
srcPixelFormat: 8,
})
}
} else {
console.error('byteBuffer is null');
}
// 确保当前buffer没有在使用的情况下，可进行资源释放
// 如果对buffer进行异步操作，需要在异步操作结束后再释放该资源（nextImage.release()）
nextImage.release();
})
})
})
}
```
用于显示画面的第二路预览流
获取第二路预览流SurfaceId：创建XComponent组件用于预览流显示，获取surfaceId请参考XComponent组件提供的getXcomponentSurfaceId方法，而XComponent的能力由UI提供，相关介绍可参考XComponent组件参考。
```typescript
@Component
struct example {
xComponentCtl: XComponentController = new XComponentController();
surfaceId:string = '';
imageWidth: number = 1920;
imageHeight: number = 1080;
build() {
XComponent({
id: 'componentId',
type: 'surface',
controller: this.xComponentCtl
})
.onLoad(async () => {
console.info('onLoad is called');
this.surfaceId = this.xComponentCtl.getXComponentSurfaceId(); // 获取组件surfaceId
// 使用surfaceId创建预览流，开启相机，组件实时渲染每帧预览流数据
})
// surface的宽、高设置与XComponent组件的宽、高设置相反，或使用.renderFit(RenderFit.RESIZE_CONTAIN)自动填充显示无需设置宽、高。
.width(px2vp(this.imageHeight))
.height(px2vp(this.imageWidth))
}
}
```
创建预览流获取数据
通过两个SurfaceId分别创建两路预览流输出，加入相机会话，启动相机会话，获取预览流数据。
```typescript
function createDualPreviewOutput(cameraManager: camera.CameraManager, previewProfile: camera.Profile,
session: camera.Session,
imageReceiverSurfaceId: string, xComponentSurfaceId: string): void {
// 使用imageReceiverSurfaceId创建第一路预览
let previewOutput1 = cameraManager.createPreviewOutput(previewProfile, imageReceiverSurfaceId);
if (!previewOutput1) {
console.error('createPreviewOutput1 error');
}
// 使用xComponentSurfaceId创建第二路预览
let previewOutput2 = cameraManager.createPreviewOutput(previewProfile, xComponentSurfaceId);
if (!previewOutput2) {
console.error('createPreviewOutput2 error');
}
// 添加第一路预览流输出
session.addOutput(previewOutput1);
// 添加第二路预览流输出
session.addOutput(previewOutput2);
}
```
完整示例
```typescript
import { camera } from '@kit.CameraKit';
import { image } from '@kit.ImageKit';
import { BusinessError } from '@kit.BasicServicesKit';
@Entry
@Component
struct Index {
private imageReceiver: image.ImageReceiver | undefined = undefined;
private imageReceiverSurfaceId: string = '';
private xComponentCtl: XComponentController = new XComponentController();
private xComponentSurfaceId: string = '';
@State imageWidth: number = 1920;
@State imageHeight: number = 1080;
private cameraManager: camera.CameraManager | undefined = undefined;
private cameras: Array<camera.CameraDevice> | Array<camera.CameraDevice> = [];
private cameraInput: camera.CameraInput | undefined = undefined;
private previewOutput1: camera.PreviewOutput | undefined = undefined;
private previewOutput2: camera.PreviewOutput | undefined = undefined;
private session: camera.VideoSession | undefined = undefined;
onPageShow(): void {
console.info('onPageShow');
this.initImageReceiver();
if (this.xComponentSurfaceId !== '') {
this.initCamera();
}
}
onPageHide(): void {
console.info('onPageHide');
this.releaseCamera();
}
/**
* 获取ImageReceiver的SurfaceId
* @param receiver
* @returns
*/
async initImageReceiver(): Promise<void> {
if (!this.imageReceiver) {
// 创建ImageReceiver
let size: image.Size = { width: this.imageWidth, height: this.imageHeight };
this.imageReceiver = image.createImageReceiver(size, image.ImageFormat.JPEG, 8);
// 获取取第一路流SurfaceId
this.imageReceiverSurfaceId = await this.imageReceiver.getReceivingSurfaceId();
console.info(`initImageReceiver imageReceiverSurfaceId:${this.imageReceiverSurfaceId}`);
// 注册监听处理预览流每帧图像数据
this.onImageArrival(this.imageReceiver);
}
}
/**
* 注册ImageReceiver图像监听
* @param receiver
*/
onImageArrival(receiver: image.ImageReceiver): void {
// 注册imageArrival监听
receiver.on('imageArrival', () => {
console.info('image arrival');
// 获取图像
receiver.readNextImage((err: BusinessError, nextImage: image.Image) => {
if (err || nextImage === undefined) {
console.error('readNextImage failed');
return;
}
// 解析图像内容
nextImage.getComponent(image.ComponentType.JPEG, async (err: BusinessError, imgComponent: image.Component) => {
if (err || imgComponent === undefined) {
console.error('getComponent failed');
}
if (imgComponent.byteBuffer) {
// 请参考步骤7解析buffer数据，本示例以方式一为例
let width = nextImage.size.width; // 获取图片的宽
let height = nextImage.size.height; // 获取图片的高
let stride = imgComponent.rowStride; // 获取图片的stride
console.debug(`getComponent with width:${width} height:${height} stride:${stride}`);
// stride与width一致
if (stride == width) {
let pixelMap = await image.createPixelMap(imgComponent.byteBuffer, {
size: { height: height, width: width },
srcPixelFormat: 8,
})
} else {
// stride与width不一致
const dstBufferSize = width * height * 1.5 // 以NV21为例（YUV_420_SP格式的图片）YUV_420_SP内存计算公式：长x宽+(长x宽)/2
const dstArr = new Uint8Array(dstBufferSize)
for (let j = 0; j < height * 1.5; j++) {
const srcBuf = new Uint8Array(imgComponent.byteBuffer, j * stride, width)
dstArr.set(srcBuf, j * width)
}
let pixelMap = await image.createPixelMap(dstArr.buffer, {
size: { height: height, width: width },
srcPixelFormat: 8,
})
}
} else {
console.error('byteBuffer is null');
}
// 确保当前buffer没有在使用的情况下，可进行资源释放
// 如果对buffer进行异步操作，需要在异步操作结束后再释放该资源（nextImage.release()）
nextImage.release();
console.info('image process done');
})
})
})
}
build() {
Column() {
XComponent({
id: 'componentId',
type: 'surface',
controller: this.xComponentCtl
})
.onLoad(async () => {
console.info('onLoad is called');
this.xComponentSurfaceId = this.xComponentCtl.getXComponentSurfaceId(); // 获取组件surfaceId
// 初始化相机，组件实时渲染每帧预览流数据
this.initCamera()
})
.width(px2vp(this.imageHeight))
.height(px2vp(this.imageWidth))
}.justifyContent(FlexAlign.Center)
.height('100%')
.width('100%')
}
// 初始化相机
async initCamera(): Promise<void> {
console.info(`initCamera imageReceiverSurfaceId:${this.imageReceiverSurfaceId} xComponentSurfaceId:${this.xComponentSurfaceId}`);
try {
// 获取相机管理器实例
this.cameraManager = camera.getCameraManager(getContext(this));
if (!this.cameraManager) {
console.error('initCamera getCameraManager');
}
// 获取当前设备支持的相机device列表
this.cameras = this.cameraManager.getSupportedCameras();
if (!this.cameras) {
console.error('initCamera getSupportedCameras');
}
// 选择一个相机device，创建cameraInput输出对象
this.cameraInput = this.cameraManager.createCameraInput(this.cameras[0]);
if (!this.cameraInput) {
console.error('initCamera createCameraInput');
}
// 打开相机
await this.cameraInput.open().catch((err: BusinessError) => {
console.error(`initCamera open fail: ${JSON.stringify(err)}`);
})
// 获取相机device支持的profile
let capability: camera.CameraOutputCapability =
this.cameraManager.getSupportedOutputCapability(this.cameras[0], camera.SceneMode.NORMAL_VIDEO);
if (!capability) {
console.error('initCamera getSupportedOutputCapability');
}
// 根据业务需求选择一个支持的预览流profile
let previewProfile: camera.Profile = capability.previewProfiles[0];
this.imageWidth = previewProfile.size.width; // 更新xComponent组件的宽
this.imageHeight = previewProfile.size.height; // 更新xComponent组件的高
console.info(`initCamera imageWidth:${this.imageWidth} imageHeight:${this.imageHeight}`);
// 使用imageReceiverSurfaceId创建第一路预览
this.previewOutput1 = this.cameraManager.createPreviewOutput(previewProfile, this.imageReceiverSurfaceId);
if (!this.previewOutput1) {
console.error('initCamera createPreviewOutput1');
}
// 使用xComponentSurfaceId创建第二路预览
this.previewOutput2 = this.cameraManager.createPreviewOutput(previewProfile, this.xComponentSurfaceId);
if (!this.previewOutput2) {
console.error('initCamera createPreviewOutput2');
}
// 创建录像模式相机会话
this.session = this.cameraManager.createSession(camera.SceneMode.NORMAL_VIDEO) as camera.VideoSession;
if (!this.session) {
console.error('initCamera createSession');
}
// 开始配置会话
this.session.beginConfig();
// 添加相机设备输入
this.session.addInput(this.cameraInput);
// 添加第一路预览流输出
this.session.addOutput(this.previewOutput1);
// 添加第二路预览流输出
this.session.addOutput(this.previewOutput2);
// 提交会话配置
await this.session.commitConfig();
// 开始启动已配置的输入输出流
await this.session.start();
} catch (error) {
console.error(`initCamera fail: ${JSON.stringify(error)}`);
}
}
// 释放相机
async releaseCamera(): Promise<void> {
console.info('releaseCamera E');
try {
// 停止当前会话
await this.session?.stop();
// 释放相机输入流
await this.cameraInput?.close();
// 释放预览输出流
await this.previewOutput1?.release();
// 释放拍照输出流
await this.previewOutput2?.release();
// 释放会话
await this.session?.release();
} catch (error) {
console.error(`initCamera fail: ${JSON.stringify(error)}`);
}
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-deferred-capture-case-V14
爬取时间: 2025-04-28 20:16:13
来源: Huawei Developer
在开发相机应用时，需要先参考开发准备申请相关权限。
当前示例提供完整的分段式拍照流程介绍，方便开发者了解完整的接口调用顺序。
在参考以下示例前，建议开发者查看分段式拍照(ArkTS)的具体章节，了解设备输入、会话管理、拍照等单个流程。
开发流程
在获取到相机支持的输出流能力后，开始创建拍照流，开发流程如下。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165946.90927006976277929873813869049770:50001231000000:2800:65697C03DC5FE4C04516FFABCD8248A62EC189269E9E92EF6CC696C2B4F3634A.png)
完整示例
Context获取方式请参考：获取UIAbility的上下文信息。
```typescript
import { camera } from '@kit.CameraKit';
import { BusinessError } from '@kit.BasicServicesKit';
import { common } from '@kit.AbilityKit';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
let context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
let photoSession: camera.PhotoSession | undefined = undefined;
let cameraInput: camera.CameraInput | undefined = undefined;
let previewOutput: camera.PreviewOutput | undefined = undefined;
let photoOutput: camera.PhotoOutput | undefined = undefined;
class MediaDataHandler implements photoAccessHelper.MediaAssetDataHandler<ArrayBuffer> {
onDataPrepared(data: ArrayBuffer) {
if (data === undefined) {
console.error('Error occurred when preparing data');
return;
}
console.info('on image data prepared');
// 请在获取到拍照buffer后，再释放session，提前释放session，会导致无法正常出图。
releaseCamSession();
}
}
async function mediaLibRequestBuffer(photoAsset: photoAccessHelper.PhotoAsset) {
let requestOptions: photoAccessHelper.RequestOptions = {
deliveryMode: photoAccessHelper.DeliveryMode.FAST_MODE,
}
const handler = new MediaDataHandler();
await photoAccessHelper.MediaAssetManager.requestImageData(context, photoAsset, requestOptions, handler);
console.info('requestImageData successfully');
}
async function mediaLibSavePhoto(photoAsset: photoAccessHelper.PhotoAsset): Promise<void> {
try {
let assetChangeRequest: photoAccessHelper.MediaAssetChangeRequest =
new photoAccessHelper.MediaAssetChangeRequest(photoAsset);
assetChangeRequest.saveCameraPhoto();
await phAccessHelper.applyChanges(assetChangeRequest);
console.info('apply saveCameraPhoto successfully');
} catch (err) {
console.error(`apply saveCameraPhoto failed with error: ${err.code}, ${err.message}`);
}
}
function setPhotoOutputCb(photoOutput: camera.PhotoOutput): void {
//监听回调之后，调用photoOutput的capture方法，低质量图上报后触发回调
photoOutput.on('photoAssetAvailable', (err: BusinessError, photoAsset: photoAccessHelper.PhotoAsset): void => {
console.info('getPhotoAsset start');
console.info(`err: ${JSON.stringify(err)}`);
if ((err !== undefined && err.code !== 0) || photoAsset === undefined) {
console.error('getPhotoAsset failed');
return;
}
// 调用媒体库落盘接口保存一阶段低质量图，二阶段真图就绪后媒体库会主动帮应用替换落盘图片
mediaLibSavePhoto(photoAsset);
// 调用媒体库接口注册低质量图或高质量图buffer回调，自定义处理
mediaLibRequestBuffer(photoAsset);
});
}
async function deferredCaptureCase(baseContext: common.BaseContext, surfaceId: string): Promise<void> {
// 创建CameraManager对象
let cameraManager: camera.CameraManager = camera.getCameraManager(baseContext);
if (!cameraManager) {
console.error('camera.getCameraManager error');
return;
}
// 监听相机状态变化
cameraManager.on('cameraStatus', (err: BusinessError, cameraStatusInfo: camera.CameraStatusInfo) => {
if (err !== undefined && err.code !== 0) {
console.error('cameraStatus with errorCode = ' + err.code);
return;
}
console.info(`camera : ${cameraStatusInfo.camera.cameraId}`);
console.info(`status: ${cameraStatusInfo.status}`);
});
// 获取相机列表
let cameraArray: Array<camera.CameraDevice> = cameraManager.getSupportedCameras();
if (cameraArray.length <= 0) {
console.error('cameraManager.getSupportedCameras error');
return;
}
for (let index = 0; index < cameraArray.length; index++) {
console.info('cameraId : ' + cameraArray[index].cameraId); // 获取相机ID
console.info('cameraPosition : ' + cameraArray[index].cameraPosition); // 获取相机位置
console.info('cameraType : ' + cameraArray[index].cameraType); // 获取相机类型
console.info('connectionType : ' + cameraArray[index].connectionType); // 获取相机连接类型
}
// 创建相机输入流
try {
cameraInput = cameraManager.createCameraInput(cameraArray[0]);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to createCameraInput errorCode = ' + err.code);
}
if (cameraInput === undefined) {
return;
}
// 监听cameraInput错误信息
let cameraDevice: camera.CameraDevice = cameraArray[0];
cameraInput.on('error', cameraDevice, (error: BusinessError) => {
console.error(`Camera input error code: ${error.code}`);
})
// 打开相机
await cameraInput.open();
// 获取支持的模式类型
let sceneModes: Array<camera.SceneMode> = cameraManager.getSupportedSceneModes(cameraArray[0]);
let isSupportPhotoMode: boolean = sceneModes.indexOf(camera.SceneMode.NORMAL_PHOTO) >= 0;
if (!isSupportPhotoMode) {
console.error('photo mode not support');
return;
}
// 获取相机设备支持的输出流能力
let cameraOutputCap: camera.CameraOutputCapability =
cameraManager.getSupportedOutputCapability(cameraArray[0], camera.SceneMode.NORMAL_PHOTO);
if (!cameraOutputCap) {
console.error('cameraManager.getSupportedOutputCapability error');
return;
}
console.info('outputCapability: ' + JSON.stringify(cameraOutputCap));
let previewProfilesArray: Array<camera.Profile> = cameraOutputCap.previewProfiles;
if (!previewProfilesArray) {
console.error('createOutput previewProfilesArray == null || undefined');
}
let photoProfilesArray: Array<camera.Profile> = cameraOutputCap.photoProfiles;
if (!photoProfilesArray) {
console.error('createOutput photoProfilesArray == null || undefined');
}
// 创建预览输出流,其中参数 surfaceId 参考上文 XComponent 组件，预览流为XComponent组件提供的surface
try {
previewOutput = cameraManager.createPreviewOutput(previewProfilesArray[0], surfaceId);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to create the PreviewOutput instance. error code: ${err.code}`);
}
if (previewOutput === undefined) {
return;
}
// 监听预览输出错误信息
previewOutput.on('error', (error: BusinessError) => {
console.error(`Preview output error code: ${error.code}`);
});
// 创建拍照输出流
try {
photoOutput = cameraManager.createPhotoOutput(photoProfilesArray[0]);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to createPhotoOutput errorCode = ' + err.code);
}
if (photoOutput === undefined) {
return;
}
//注册监听photoAssetAvailable回调
setPhotoOutputCb(photoOutput);
//创建会话
try {
photoSession = cameraManager.createSession(camera.SceneMode.NORMAL_PHOTO) as camera.PhotoSession;
} catch (error) {
let err = error as BusinessError;
console.error('Failed to create the session instance. errorCode = ' + err.code);
}
if (photoSession === undefined) {
return;
}
// 监听session错误信息
photoSession.on('error', (error: BusinessError) => {
console.error(`Capture session error code: ${error.code}`);
});
// 开始配置会话
try {
photoSession.beginConfig();
} catch (error) {
let err = error as BusinessError;
console.error('Failed to beginConfig. errorCode = ' + err.code);
}
// 向会话中添加相机输入流
try {
photoSession.addInput(cameraInput);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to addInput. errorCode = ' + err.code);
}
// 向会话中添加预览输出流
try {
photoSession.addOutput(previewOutput);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to addOutput(previewOutput). errorCode = ' + err.code);
}
// 向会话中添加拍照输出流
try {
photoSession.addOutput(photoOutput);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to addOutput(photoOutput). errorCode = ' + err.code);
}
// 提交会话配置
await photoSession.commitConfig();
// 启动会话
await photoSession.start().then(() => {
console.info('Promise returned to indicate the session start success.');
});
// 判断设备是否支持闪光灯
let flashStatus: boolean = false;
try {
flashStatus = photoSession.hasFlash();
} catch (error) {
let err = error as BusinessError;
console.error('Failed to hasFlash. errorCode = ' + err.code);
}
console.info('Returned with the flash light support status:' + flashStatus);
if (flashStatus) {
// 判断是否支持自动闪光灯模式
let flashModeStatus: boolean = false;
try {
let status: boolean = photoSession.isFlashModeSupported(camera.FlashMode.FLASH_MODE_AUTO);
flashModeStatus = status;
} catch (error) {
let err = error as BusinessError;
console.error('Failed to check whether the flash mode is supported. errorCode = ' + err.code);
}
if (flashModeStatus) {
// 设置自动闪光灯模式
try {
photoSession.setFlashMode(camera.FlashMode.FLASH_MODE_AUTO);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to set the flash mode. errorCode = ' + err.code);
}
}
}
// 判断是否支持连续自动变焦模式
let focusModeStatus: boolean = false;
try {
let status: boolean = photoSession.isFocusModeSupported(camera.FocusMode.FOCUS_MODE_CONTINUOUS_AUTO);
focusModeStatus = status;
} catch (error) {
let err = error as BusinessError;
console.error('Failed to check whether the focus mode is supported. errorCode = ' + err.code);
}
if (focusModeStatus) {
// 设置连续自动变焦模式
try {
photoSession.setFocusMode(camera.FocusMode.FOCUS_MODE_CONTINUOUS_AUTO);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to set the focus mode. errorCode = ' + err.code);
}
}
// 获取相机支持的可变焦距比范围
let zoomRatioRange: Array<number> = [];
try {
zoomRatioRange = photoSession.getZoomRatioRange();
} catch (error) {
let err = error as BusinessError;
console.error('Failed to get the zoom ratio range. errorCode = ' + err.code);
}
if (zoomRatioRange.length <= 0) {
return;
}
// 设置可变焦距比
try {
photoSession.setZoomRatio(zoomRatioRange[0]);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to set the zoom ratio value. errorCode = ' + err.code);
}
let photoCaptureSetting: camera.PhotoCaptureSetting = {
quality: camera.QualityLevel.QUALITY_LEVEL_HIGH, // 设置图片质量高
rotation: camera.ImageRotation.ROTATION_0 // 设置图片旋转角度0
}
// 使用当前拍照设置触发一次拍照
photoOutput.capture(photoCaptureSetting, (err: BusinessError) => {
if (err) {
console.error(`Failed to capture the photo ${err.message}`);
return;
}
console.info('Callback invoked to indicate the photo capture request success.');
});
}
async function releaseCamSession() {
// 停止当前会话
await photoSession?.stop();
// 释放相机输入流
await cameraInput?.close();
// 释放预览输出流
await previewOutput?.release();
// 释放拍照输出流
await photoOutput?.release();
// 释放会话
await photoSession?.release();
// 会话置空
photoSession = undefined;
}
@Entry
@Component
struct Index {
@State message: string = 'PhotoAssetDemo';
private mXComponentController: XComponentController = new XComponentController();
private surfaceId = '';
build() {
Column() {
Column() {
XComponent({
id: 'componentId',
type: XComponentType.SURFACE,
controller: this.mXComponentController
})
.onLoad(async () => {
console.info('onLoad is called');
this.surfaceId = this.mXComponentController.getXComponentSurfaceId();
console.info(`onLoad surfaceId: ${this.surfaceId}`);
deferredCaptureCase(context, this.surfaceId);
})// The width and height of the surface are opposite to those of the XComponent.
.renderFit(RenderFit.RESIZE_CONTAIN)
}.height('95%')
.justifyContent(FlexAlign.Center)
Text(this.message)
.id('PhotoAssetDemo')
.fontSize(38)
.fontWeight(FontWeight.Bold)
.alignRules({
center: { anchor: '__container__', align: VerticalAlign.Center },
middle: { anchor: '__container__', align: HorizontalAlign.Center }
})
}
.height('100%')
.width('100%')
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-background-recovery-V14
爬取时间: 2025-04-28 20:16:27
来源: Huawei Developer
当前示例提供完整的相机应用从后台切换至前台启动恢复的流程介绍，方便开发者了解完整的接口调用顺序。
相机应用在前后台切换过程中的状态变化说明：
在参考以下示例前，建议开发者建议开发者查看相机开发指导(ArkTS)的具体章节，了解相机管理、设备输入、会话管理等单个操作。
开发流程
相机应用从后台切换至前台启动恢复调用流程图建议如下：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165946.05424325491839255513512616300277:50001231000000:2800:AC99FE02736E94F0FEC4C3E33CBCC56B452D64A4AB069684B0D7CB6C4FD87790.png)
完整示例
Context获取方式请参考：获取UIAbility的上下文信息。
相机应用从后台切换至前台启动恢复需要在页面生命周期回调函数onPageShow中调用，重新初始化相机设备。
```typescript
import { camera } from '@kit.CameraKit';
import { BusinessError } from '@kit.BasicServicesKit';
import { common } from '@kit.AbilityKit';
let context: common.BaseContext;
let surfaceId: string = '';
async function onPageShow(): Promise<void> {
// 当应用从后台切换至前台页面显示时，重新初始化相机设备
await initCamera(context, surfaceId);
}
async function initCamera(baseContext: common.BaseContext, surfaceId: string): Promise<void> {
console.info('onForeGround recovery begin.');
let cameraManager: camera.CameraManager = camera.getCameraManager(context);
if (!cameraManager) {
console.error("camera.getCameraManager error");
return;
}
// 监听相机状态变化
cameraManager.on('cameraStatus', (err: BusinessError, cameraStatusInfo: camera.CameraStatusInfo) => {
if (err !== undefined && err.code !== 0) {
console.error('cameraStatus with errorCode = ' + err.code);
return;
}
console.info(`camera : ${cameraStatusInfo.camera.cameraId}`);
console.info(`status: ${cameraStatusInfo.status}`);
});
// 获取相机列表
let cameraArray: Array<camera.CameraDevice> = cameraManager.getSupportedCameras();
if (cameraArray.length <= 0) {
console.error("cameraManager.getSupportedCameras error");
return;
}
for (let index = 0; index < cameraArray.length; index++) {
console.info('cameraId : ' + cameraArray[index].cameraId);                       // 获取相机ID
console.info('cameraPosition : ' + cameraArray[index].cameraPosition);           // 获取相机位置
console.info('cameraType : ' + cameraArray[index].cameraType);                   // 获取相机类型
console.info('connectionType : ' + cameraArray[index].connectionType);           // 获取相机连接类型
}
// 创建相机输入流
let cameraInput: camera.CameraInput | undefined = undefined;
try {
cameraInput = cameraManager.createCameraInput(cameraArray[0]);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to createCameraInput errorCode = ' + err.code);
}
if (cameraInput === undefined) {
return;
}
// 监听cameraInput错误信息
let cameraDevice: camera.CameraDevice = cameraArray[0];
cameraInput.on('error', cameraDevice, (error: BusinessError) => {
console.error(`Camera input error code: ${error.code}`);
});
// 打开相机
await cameraInput.open();
// 获取支持的模式类型
let sceneModes: Array<camera.SceneMode> = cameraManager.getSupportedSceneModes(cameraArray[0]);
let isSupportPhotoMode: boolean = sceneModes.indexOf(camera.SceneMode.NORMAL_PHOTO) >= 0;
if (!isSupportPhotoMode) {
console.error('photo mode not support');
return;
}
// 获取相机设备支持的输出流能力
let cameraOutputCap: camera.CameraOutputCapability = cameraManager.getSupportedOutputCapability(cameraArray[0], camera.SceneMode.NORMAL_PHOTO);
if (!cameraOutputCap) {
console.error("cameraManager.getSupportedOutputCapability error");
return;
}
console.info("outputCapability: " + JSON.stringify(cameraOutputCap));
let previewProfilesArray: Array<camera.Profile> = cameraOutputCap.previewProfiles;
if (!previewProfilesArray) {
console.error("createOutput previewProfilesArray == null || undefined");
}
let photoProfilesArray: Array<camera.Profile> = cameraOutputCap.photoProfiles;
if (!photoProfilesArray) {
console.error("createOutput photoProfilesArray == null || undefined");
}
// 创建预览输出流,其中参数 surfaceId 参考上文 XComponent 组件，预览流为XComponent组件提供的surface
let previewOutput: camera.PreviewOutput | undefined = undefined;
try {
previewOutput = cameraManager.createPreviewOutput(previewProfilesArray[0], surfaceId);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to create the PreviewOutput instance. error code: ${err.code}`);
}
if (previewOutput === undefined) {
return;
}
// 监听预览输出错误信息
previewOutput.on('error', (error: BusinessError) => {
console.error(`Preview output error code: ${error.code}`);
});
// 创建拍照输出流
let photoOutput: camera.PhotoOutput | undefined = undefined;
try {
photoOutput = cameraManager.createPhotoOutput(photoProfilesArray[0]);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to createPhotoOutput errorCode = ' + err.code);
}
if (photoOutput === undefined) {
return;
}
//创建会话
let photoSession: camera.PhotoSession | undefined = undefined;
try {
photoSession = cameraManager.createSession(camera.SceneMode.NORMAL_PHOTO) as camera.PhotoSession;
} catch (error) {
let err = error as BusinessError;
console.error('Failed to create the session instance. errorCode = ' + err.code);
}
if (photoSession === undefined) {
return;
}
// 监听session错误信息
photoSession.on('error', (error: BusinessError) => {
console.error(`Capture session error code: ${error.code}`);
});
// 开始配置会话
try {
photoSession.beginConfig();
} catch (error) {
let err = error as BusinessError;
console.error('Failed to beginConfig. errorCode = ' + err.code);
}
// 向会话中添加相机输入流
try {
photoSession.addInput(cameraInput);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to addInput. errorCode = ' + err.code);
}
// 向会话中添加预览输出流
try {
photoSession.addOutput(previewOutput);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to addOutput(previewOutput). errorCode = ' + err.code);
}
// 向会话中添加拍照输出流
try {
photoSession.addOutput(photoOutput);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to addOutput(photoOutput). errorCode = ' + err.code);
}
// 提交会话配置
await photoSession.commitConfig();
// 启动会话
await photoSession.start().then(() => {
console.info('Promise returned to indicate the session start success.');
});
// 判断设备是否支持闪光灯
let flashStatus: boolean = false;
try {
flashStatus = photoSession.hasFlash();
} catch (error) {
let err = error as BusinessError;
console.error('Failed to hasFlash. errorCode = ' + err.code);
}
console.info('Returned with the flash light support status:' + flashStatus);
if (flashStatus) {
// 判断是否支持自动闪光灯模式
let flashModeStatus: boolean = false;
try {
let status: boolean = photoSession.isFlashModeSupported(camera.FlashMode.FLASH_MODE_AUTO);
flashModeStatus = status;
} catch (error) {
let err = error as BusinessError;
console.error('Failed to check whether the flash mode is supported. errorCode = ' + err.code);
}
if(flashModeStatus) {
// 设置自动闪光灯模式
try {
photoSession.setFlashMode(camera.FlashMode.FLASH_MODE_AUTO);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to set the flash mode. errorCode = ' + err.code);
}
}
}
// 判断是否支持连续自动变焦模式
let focusModeStatus: boolean = false;
try {
let status: boolean = photoSession.isFocusModeSupported(camera.FocusMode.FOCUS_MODE_CONTINUOUS_AUTO);
focusModeStatus = status;
} catch (error) {
let err = error as BusinessError;
console.error('Failed to check whether the focus mode is supported. errorCode = ' + err.code);
}
if (focusModeStatus) {
// 设置连续自动变焦模式
try {
photoSession.setFocusMode(camera.FocusMode.FOCUS_MODE_CONTINUOUS_AUTO);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to set the focus mode. errorCode = ' + err.code);
}
}
// 获取相机支持的可变焦距比范围
let zoomRatioRange: Array<number> = [];
try {
zoomRatioRange = photoSession.getZoomRatioRange();
} catch (error) {
let err = error as BusinessError;
console.error('Failed to get the zoom ratio range. errorCode = ' + err.code);
}
if (zoomRatioRange.length <= 0) {
return;
}
// 设置可变焦距比
try {
photoSession.setZoomRatio(zoomRatioRange[0]);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to set the zoom ratio value. errorCode = ' + err.code);
}
let photoCaptureSetting: camera.PhotoCaptureSetting = {
quality: camera.QualityLevel.QUALITY_LEVEL_HIGH, // 设置图片质量高
rotation: camera.ImageRotation.ROTATION_0 // 设置图片旋转角度0
}
// 使用当前拍照设置进行拍照
photoOutput.capture(photoCaptureSetting, (err: BusinessError) => {
if (err) {
console.error(`Failed to capture the photo ${err.message}`);
return;
}
console.info('Callback invoked to indicate the photo capture request success.');
});
console.info('onForeGround recovery end.');
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-hdr-shooting-V14
爬取时间: 2025-04-28 20:16:41
来源: Huawei Developer
HarmonyOS支持调用接口拍摄HDR Vivid照片，可以拍出层次表现更细腻、光影细节更丰富的画面，提升画面质感，呈现更卓越的视觉效果。
当前示例提供完整的HDR Vivid拍照开发步骤，方便开发者实现HDR拍照的功能。更多HDR Vivid的开发指导，请参考使用HDR Vivid特性开发媒体应用。
在参考以下示例前，建议开发者查看相机开发指导(ArkTS)的具体章节，了解设备输入、会话管理、拍照等单个流程。
开发步骤
1.  导入接口。
```typescript
import { camera } from '@kit.CameraKit';
import { colorSpaceManager } from '@kit.ArkGraphics2D';
import { image } from '@kit.ImageKit';
import { BusinessError } from '@kit.BasicServicesKit';
import { common } from '@kit.AbilityKit';
import { fileIo as fs } from '@kit.CoreFileKit';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
```
2.  查询支持的色彩空间。
```typescript
function getSupportedColorSpaces(session: camera.PhotoSession): Array<colorSpaceManager.ColorSpace> {
let colorSpaces: Array<colorSpaceManager.ColorSpace> = [];
try {
colorSpaces = session.getSupportedColorSpaces();
} catch (error) {
let err = error as BusinessError;
console.error(`The getSupportedColorSpaces call failed. error code: ${err.code}`);
}
return colorSpaces;
}
```
3.  设置色彩空间。 如果是SDR拍照色彩空间需要设置为SRGB，如果是HDR拍照色彩空间需要设置为DISPLAY_P3。具体参考setColorSpace。
```typescript
function setColorSpaceBeforeCommitConfig(session: camera.PhotoSession, isHdr: boolean): void {
let colorSpace: colorSpaceManager.ColorSpace = isHdr? colorSpaceManager.ColorSpace.DISPLAY_P3 : colorSpaceManager.ColorSpace.SRGB;
let colorSpaces: Array<colorSpaceManager.ColorSpace> = getSupportedColorSpaces(session);
let isSupportedColorSpaces = colorSpaces.indexOf(colorSpace) >= 0;
if (isSupportedColorSpaces) {
console.info(`setColorSpace: ${colorSpace}`);
session.setColorSpace(colorSpace);
let activeColorSpace:colorSpaceManager.ColorSpace = session.getActiveColorSpace();
console.info(`activeColorSpace: ${activeColorSpace}`);
} else {
console.info(`colorSpace: ${colorSpace} is not support`);
}
}
```
4.  实现HDR拍照。 在提交会话配置前执行步骤3设置色彩空间，其余流程按照正常拍照流程开发。
```typescript
let context = getContext(this);
async function savePicture(buffer: ArrayBuffer, img: image.Image): Promise<void> {
let accessHelper: photoAccessHelper.PhotoAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
let options: photoAccessHelper.CreateOptions = {
title: Date.now().toString()
};
let photoUri: string = await accessHelper.createAsset(photoAccessHelper.PhotoType.IMAGE, 'jpg', options);
//createAsset的调用需要ohos.permission.READ_IMAGEVIDEO和ohos.permission.WRITE_IMAGEVIDEO的权限
let file: fs.File = fs.openSync(photoUri, fs.OpenMode.READ_WRITE | fs.OpenMode.CREATE);
await fs.write(file.fd, buffer);
fs.closeSync(file);
img.release();
}
function setPhotoOutputCb(photoOutput: camera.PhotoOutput): void {
//设置回调之后，调用photoOutput的capture方法，就会将拍照的buffer回传到回调中
photoOutput.on('photoAvailable', (errCode: BusinessError, photo: camera.Photo): void => {
console.info('getPhoto start');
console.info(`err: ${JSON.stringify(errCode)}`);
if (errCode || photo === undefined) {
console.error('getPhoto failed');
return;
}
let imageObj = photo.main;
imageObj.getComponent(image.ComponentType.JPEG, (errCode: BusinessError, component: image.Component): void => {
console.info('getComponent start');
if (errCode || component === undefined) {
console.error('getComponent failed');
return;
}
let buffer: ArrayBuffer;
if (component.byteBuffer) {
buffer = component.byteBuffer;
} else {
console.error('byteBuffer is null');
return;
}
savePicture(buffer, imageObj);
});
});
}
async function cameraHdrShootingCase(baseContext: common.BaseContext, surfaceId: string): Promise<void> {
// 创建CameraManager对象
let cameraManager: camera.CameraManager = camera.getCameraManager(baseContext);
if (!cameraManager) {
console.error("camera.getCameraManager error");
return;
}
// 监听相机状态变化
cameraManager.on('cameraStatus', (err: BusinessError, cameraStatusInfo: camera.CameraStatusInfo) => {
if (err !== undefined && err.code !== 0) {
console.error('cameraStatus with errorCode = ' + err.code);
return;
}
console.info(`camera : ${cameraStatusInfo.camera.cameraId}`);
console.info(`status: ${cameraStatusInfo.status}`);
});
// 获取相机列表
let cameraArray: Array<camera.CameraDevice> = cameraManager.getSupportedCameras();
if (cameraArray.length <= 0) {
console.error("cameraManager.getSupportedCameras error");
return;
}
for (let index = 0; index < cameraArray.length; index++) {
console.info('cameraId : ' + cameraArray[index].cameraId);                          // 获取相机ID
console.info('cameraPosition : ' + cameraArray[index].cameraPosition);              // 获取相机位置
console.info('cameraType : ' + cameraArray[index].cameraType);                      // 获取相机类型
console.info('connectionType : ' + cameraArray[index].connectionType);              // 获取相机连接类型
}
// 创建相机输入流
let cameraInput: camera.CameraInput | undefined = undefined;
try {
cameraInput = cameraManager.createCameraInput(cameraArray[0]);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to createCameraInput errorCode = ' + err.code);
}
if (cameraInput === undefined) {
return;
}
// 监听cameraInput错误信息
let cameraDevice: camera.CameraDevice = cameraArray[0];
cameraInput.on('error', cameraDevice, (error: BusinessError) => {
console.error(`Camera input error code: ${error.code}`);
})
// 打开相机
await cameraInput.open();
// 获取支持的模式类型
let sceneModes: Array<camera.SceneMode> = cameraManager.getSupportedSceneModes(cameraArray[0]);
let isSupportPhotoMode: boolean = sceneModes.indexOf(camera.SceneMode.NORMAL_PHOTO) >= 0;
if (!isSupportPhotoMode) {
console.error('photo mode not support');
return;
}
// 获取相机设备支持的输出流能力
let cameraOutputCap: camera.CameraOutputCapability = cameraManager.getSupportedOutputCapability(cameraArray[0], camera.SceneMode.NORMAL_PHOTO);
if (!cameraOutputCap) {
console.error("cameraManager.getSupportedOutputCapability error");
return;
}
console.info("outputCapability: " + JSON.stringify(cameraOutputCap));
let previewProfilesArray: Array<camera.Profile> = cameraOutputCap.previewProfiles;
if (!previewProfilesArray) {
console.error("createOutput previewProfilesArray == null || undefined");
}
let photoProfilesArray: Array<camera.Profile> = cameraOutputCap.photoProfiles;
if (!photoProfilesArray) {
console.error("createOutput photoProfilesArray == null || undefined");
}
// 创建预览输出流,其中参数 surfaceId 参考上文 XComponent 组件，预览流为XComponent组件提供的surface
let previewOutput: camera.PreviewOutput | undefined = undefined;
try {
previewOutput = cameraManager.createPreviewOutput(previewProfilesArray[0], surfaceId);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to create the PreviewOutput instance. error code: ${err.code}`);
}
if (previewOutput === undefined) {
return;
}
// 监听预览输出错误信息
previewOutput.on('error', (error: BusinessError) => {
console.error(`Preview output error code: ${error.code}`);
});
// 创建拍照输出流
let photoOutput: camera.PhotoOutput | undefined = undefined;
try {
photoOutput = cameraManager.createPhotoOutput(photoProfilesArray[0]);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to createPhotoOutput errorCode = ' + err.code);
}
if (photoOutput === undefined) {
return;
}
//调用上面的回调函数来保存图片
setPhotoOutputCb(photoOutput);
//创建会话
let photoSession: camera.PhotoSession | undefined = undefined;
try {
photoSession = cameraManager.createSession(camera.SceneMode.NORMAL_PHOTO) as camera.PhotoSession;
} catch (error) {
let err = error as BusinessError;
console.error('Failed to create the session instance. errorCode = ' + err.code);
}
if (photoSession === undefined) {
return;
}
// 监听session错误信息
photoSession.on('error', (error: BusinessError) => {
console.error(`Capture session error code: ${error.code}`);
});
// 开始配置会话
try {
photoSession.beginConfig();
} catch (error) {
let err = error as BusinessError;
console.error('Failed to beginConfig. errorCode = ' + err.code);
}
// 向会话中添加相机输入流
try {
photoSession.addInput(cameraInput);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to addInput. errorCode = ' + err.code);
}
// 向会话中添加预览输出流
try {
photoSession.addOutput(previewOutput);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to addOutput(previewOutput). errorCode = ' + err.code);
}
// 向会话中添加拍照输出流
try {
photoSession.addOutput(photoOutput);
} catch (error) {
let err = error as BusinessError;
console.error('Failed to addOutput(photoOutput). errorCode = ' + err.code);
}
// 设置色彩空间
setColorSpaceBeforeCommitConfig(photoSession, true);
// 提交会话配置
await photoSession.commitConfig();
// 启动会话
await photoSession.start().then(() => {
console.info('Promise returned to indicate the session start success.');
});
let photoCaptureSetting: camera.PhotoCaptureSetting = {
quality: camera.QualityLevel.QUALITY_LEVEL_HIGH, // 设置图片质量高
rotation: camera.ImageRotation.ROTATION_0 // 设置图片旋转角度0
}
// 使用当前拍照设置进行拍照
photoOutput.capture(photoCaptureSetting, (err: BusinessError) => {
if (err) {
console.error(`Failed to capture the photo ${err.message}`);
return;
}
console.info('Callback invoked to indicate the photo capture request success.');
});
// 需要在拍照结束之后调用以下关闭摄像头和释放会话流程，避免拍照未结束就将会话释放。
// 停止当前会话
await photoSession.stop();
// 释放相机输入流
await cameraInput.close();
// 释放预览输出流
await previewOutput.release();
// 释放拍照输出流
await photoOutput.release();
// 释放会话
await photoSession.release();
// 会话置空
photoSession = undefined;
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-hdr-recording-V14
爬取时间: 2025-04-28 20:16:54
来源: Huawei Developer
HarmonyOS支持调用接口，录制HDR Vivid视频，可以拍出层次表现更细腻、光影细节更丰富的画面，提升画面质感，呈现更卓越的视觉效果。
当前示例提供完整的HDR Vivid录像开发步骤，方便开发者实现录制HDR Vivid视频的功能。更多HDR Vivid的开发指导，请参考使用HDR Vivid特性开发媒体应用。
在参考以下示例前，建议开发者查看相机开发指导(ArkTS)的具体章节，了解设备输入、会话管理、录像等单个流程。
开发步骤
1.  导入接口。
```typescript
import { camera } from '@kit.CameraKit';
import { colorSpaceManager } from '@kit.ArkGraphics2D';
import { BusinessError } from '@kit.BasicServicesKit';
import { media } from '@kit.MediaKit';
import { common } from '@kit.AbilityKit';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
import { fileIo as fs } from '@kit.CoreFileKit';
```
2.  获取预览、录像的配置项。 HDR录像的输出格式需要设置成10bit的CAMERA_FORMAT_YCRCB_P010。具体参考setColorSpace。 预览流与录像输出流的分辨率的宽高比要保持一致，如果设置XComponent组件中的Surface显示区域宽高比为1920:1080 = 16:9，则需要预览流中的分辨率的宽高比也为16:9，如分辨率选择640:360，或960:540，或1920:1080，以此类推。
```typescript
function getPreviewProfile(previewProfiles: Array<camera.Profile>, size: camera.Size): undefined | camera.Profile {
let previewProfile: undefined | camera.Profile = previewProfiles.find((profile: camera.Profile) => {
return profile.format === camera.CameraFormat.CAMERA_FORMAT_YCRCB_P010 &&
profile.size.width === size.width && profile.size.height == size.height
});
return previewProfile;
}
function getVideoProfile(videoProfiles: Array<camera.VideoProfile>, size: camera.Size): undefined | camera.VideoProfile {
let videoProfile: undefined | camera.VideoProfile = videoProfiles.find((profile: camera.VideoProfile) => {
return profile.format === camera.CameraFormat.CAMERA_FORMAT_YCRCB_P010 &&
profile.size.width === size.width && profile.size.height == size.height
});
return videoProfile;
}
```
3.  查询是否支持视频防抖。 HDR录像需要支持视频防抖。
```typescript
function isVideoStabilizationModeSupported(session: camera.VideoSession, mode: camera.VideoStabilizationMode): boolean {
let isSupported: boolean = false;
try {
isSupported = session.isVideoStabilizationModeSupported(mode);
} catch (error) {
// 失败返回错误码error.code并处理
let err = error as BusinessError;
console.error(`The isVideoStabilizationModeSupported call failed. error code: ${err.code}`);
}
return isSupported;
}
```
4.  设置视频防抖。
```typescript
function setVideoStabilizationMode(session: camera.VideoSession): boolean {
let mode: camera.VideoStabilizationMode = camera.VideoStabilizationMode.AUTO;
// 查询是否支持视频防抖
let isSupported: boolean = isVideoStabilizationModeSupported(session, mode);
if (isSupported) {
console.info(`setVideoStabilizationMode: ${mode}`);
// 设置视频防抖
session.setVideoStabilizationMode(mode);
let activeVideoStabilizationMode = session.getActiveVideoStabilizationMode();
console.info(`activeVideoStabilizationMode: ${activeVideoStabilizationMode}`);
} else {
console.info(`videoStabilizationMode: ${mode} is not support`);
}
return isSupported;
}
```
5.  查询支持的色彩空间。
```typescript
function getSupportedColorSpaces(session: camera.VideoSession): Array<colorSpaceManager.ColorSpace> {
let colorSpaces: Array<colorSpaceManager.ColorSpace> = [];
try {
colorSpaces = session.getSupportedColorSpaces();
} catch (error) {
let err = error as BusinessError;
console.error(`The getSupportedColorSpaces call failed. error code: ${err.code}`);
}
return colorSpaces;
}
```
6.  设置色彩空间。 如果是SDR录像色彩空间需要设置为BT709_LIMIT，如果是HDR录像色彩空间需要设置为BT2020_HLG_LIMIT。具体参考setColorSpace。
```typescript
function setColorSpaceAfterCommitConfig(session: camera.VideoSession, isHdr: boolean): void {
let colorSpace: colorSpaceManager.ColorSpace = isHdr? colorSpaceManager.ColorSpace.BT2020_HLG_LIMIT : colorSpaceManager.ColorSpace.BT709_LIMIT;
let colorSpaces: Array<colorSpaceManager.ColorSpace> = getSupportedColorSpaces(session);
let isSupportedColorSpaces = colorSpaces.indexOf(colorSpace) >= 0;
if (isSupportedColorSpaces) {
console.info(`setColorSpace: ${colorSpace}`);
session.setColorSpace(colorSpace);
let activeColorSpace:colorSpaceManager.ColorSpace = session.getActiveColorSpace();
console.info(`activeColorSpace: ${activeColorSpace}`);
} else {
console.info(`colorSpace: ${colorSpace} is not support`);
}
}
```
7.  实现HDR录像。 在创建预览输出、录像输出前执行步骤2获取配置项，提交会话配置前执行步骤4设置视频防抖、执行步骤6设置色彩空间，其余流程按照正常录像流程开发。
```typescript
async function cameraHdrRecordingCase(context: common.Context, surfaceId: string): Promise<void> {
// 创建CameraManager对象
let cameraManager: camera.CameraManager = camera.getCameraManager(context);
if (!cameraManager) {
console.error("camera.getCameraManager error");
return;
}
// 监听相机状态变化
cameraManager.on('cameraStatus', (err: BusinessError, cameraStatusInfo: camera.CameraStatusInfo) => {
if (err !== undefined && err.code !== 0) {
console.error('cameraStatus with errorCode = ' + err.code);
return;
}
console.info(`camera : ${cameraStatusInfo.camera.cameraId}`);
console.info(`status: ${cameraStatusInfo.status}`);
});
// 获取相机列表
let cameraArray: Array<camera.CameraDevice> = [];
try {
cameraArray = cameraManager.getSupportedCameras();
} catch (error) {
let err = error as BusinessError;
console.error(`getSupportedCameras call failed. error code: ${err.code}`);
}
if (cameraArray.length <= 0) {
console.error("cameraManager.getSupportedCameras error");
return;
}
// 获取支持的模式类型
let sceneModes: Array<camera.SceneMode> = cameraManager.getSupportedSceneModes(cameraArray[0]);
let isSupportVideoMode: boolean = sceneModes.indexOf(camera.SceneMode.NORMAL_VIDEO) >= 0;
if (!isSupportVideoMode) {
console.error('video mode not support');
return;
}
// 获取相机设备支持的输出流能力
let cameraOutputCap: camera.CameraOutputCapability = cameraManager.getSupportedOutputCapability(cameraArray[0], camera.SceneMode.NORMAL_VIDEO);
if (!cameraOutputCap) {
console.error("cameraManager.getSupportedOutputCapability error")
return;
}
console.info("outputCapability: " + JSON.stringify(cameraOutputCap));
let previewProfilesArray: Array<camera.Profile> = cameraOutputCap.previewProfiles;
if (!previewProfilesArray) {
console.error("createOutput previewProfilesArray == null || undefined");
return;
}
let videoProfilesArray: Array<camera.VideoProfile> = cameraOutputCap.videoProfiles;
if (!videoProfilesArray) {
console.error("createOutput videoProfilesArray == null || undefined");
return;
}
// videoProfile的宽高需要与AVRecorderProfile的宽高保持一致，并且需要使用AVRecorderProfile锁支持的宽高
let videoSize: camera.Size = {
width: 640,
height: 480
}
let previewProfile: undefined | camera.Profile = getPreviewProfile(previewProfilesArray, videoSize);
if (!previewProfile) {
console.error('previewProfile is not found');
return;
}
let videoProfile: undefined | camera.VideoProfile = getVideoProfile(videoProfilesArray, videoSize);
if (!videoProfile) {
console.error('videoProfile is not found');
return;
}
// 配置参数以实际硬件设备支持的范围为准
let aVRecorderProfile: media.AVRecorderProfile = {
audioBitrate: 48000,
audioChannels: 2,
audioCodec: media.CodecMimeType.AUDIO_AAC,
audioSampleRate: 48000,
fileFormat: media.ContainerFormatType.CFT_MPEG_4,
videoBitrate: 2000000,
videoCodec: media.CodecMimeType.VIDEO_HEVC,
videoFrameWidth: videoSize.width,
videoFrameHeight: videoSize.height,
videoFrameRate: 30,
isHdr: true
};
let options: photoAccessHelper.CreateOptions = {
title: Date.now().toString()
};
let accessHelper: photoAccessHelper.PhotoAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
let videoUri: string = await accessHelper.createAsset(photoAccessHelper.PhotoType.VIDEO, 'mp4', options);
let file: fs.File = fs.openSync(videoUri, fs.OpenMode.READ_WRITE | fs.OpenMode.CREATE);
let aVRecorderConfig: media.AVRecorderConfig = {
audioSourceType: media.AudioSourceType.AUDIO_SOURCE_TYPE_MIC,
videoSourceType: media.VideoSourceType.VIDEO_SOURCE_TYPE_SURFACE_YUV,
profile: aVRecorderProfile,
url: `fd://${file.fd.toString()}`, // 文件需先由调用者创建，赋予读写权限，将文件fd传给此参数，eg.fd://45--file:///data/media/01.mp4
rotation: 0, // 合理值0、90、180、270，非合理值prepare接口将报错
location: { latitude: 30, longitude: 130 }
};
let avRecorder: media.AVRecorder | undefined = undefined;
try {
avRecorder = await media.createAVRecorder();
} catch (error) {
let err = error as BusinessError;
console.error(`createAVRecorder call failed. error code: ${err.code}`);
}
if (avRecorder === undefined) {
return;
}
try {
await avRecorder.prepare(aVRecorderConfig);
} catch (error) {
let err = error as BusinessError;
console.error(`prepare call failed. error code: ${err.code}`);
}
let videoSurfaceId: string | undefined = undefined; // 该surfaceID用于传递给相机接口创造videoOutput
try {
videoSurfaceId = await avRecorder.getInputSurface();
} catch (error) {
let err = error as BusinessError;
console.error(`getInputSurface call failed. error code: ${err.code}`);
}
if (videoSurfaceId === undefined) {
return;
}
// 创建VideoOutput对象
let videoOutput: camera.VideoOutput | undefined = undefined;
try {
videoOutput = cameraManager.createVideoOutput(videoProfile, videoSurfaceId);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to create the videoOutput instance. error: ${JSON.stringify(err)}`);
}
if (videoOutput === undefined) {
return;
}
// 监听视频输出错误信息
videoOutput.on('error', (error: BusinessError) => {
console.error(`Preview output error code: ${error.code}`);
});
//创建会话
let videoSession: camera.VideoSession | undefined = undefined;
try {
videoSession = cameraManager.createSession(camera.SceneMode.NORMAL_VIDEO) as camera.VideoSession;
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to create the session instance. error: ${JSON.stringify(err)}`);
}
if (videoSession === undefined) {
return;
}
// 监听session错误信息
videoSession.on('error', (error: BusinessError) => {
console.error(`Video session error code: ${error.code}`);
});
// 开始配置会话
try {
videoSession.beginConfig();
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to beginConfig. error: ${JSON.stringify(err)}`);
}
// 创建相机输入流
let cameraInput: camera.CameraInput | undefined = undefined;
try {
cameraInput = cameraManager.createCameraInput(cameraArray[0]);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to createCameraInput. error: ${JSON.stringify(err)}`);
}
if (cameraInput === undefined) {
return;
}
// 监听cameraInput错误信息
let cameraDevice: camera.CameraDevice = cameraArray[0];
cameraInput.on('error', cameraDevice, (error: BusinessError) => {
console.error(`Camera input error code: ${error.code}`);
});
// 打开相机
try {
await cameraInput.open();
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to open cameraInput. error: ${JSON.stringify(err)}`);
}
// 向会话中添加相机输入流
try {
videoSession.addInput(cameraInput);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to add cameraInput. error: ${JSON.stringify(err)}`);
}
// 创建预览输出流，其中参数 surfaceId 参考下面 XComponent 组件，预览流为XComponent组件提供的surface
let previewOutput: camera.PreviewOutput | undefined = undefined;
try {
previewOutput = cameraManager.createPreviewOutput(previewProfile, surfaceId);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to create the PreviewOutput instance. error: ${JSON.stringify(err)}`);
}
if (previewOutput === undefined) {
return;
}
// 向会话中添加预览输出流
try {
videoSession.addOutput(previewOutput);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to add previewOutput. error: ${JSON.stringify(err)}`);
}
// 向会话中添加录像输出流
try {
videoSession.addOutput(videoOutput);
} catch (error) {
let err = error as BusinessError;
console.error(`Failed to add videoOutput. error: ${JSON.stringify(err)}`);
}
// 提交会话配置
try {
await videoSession.commitConfig();
} catch (error) {
let err = error as BusinessError;
console.error(`videoSession commitConfig error: ${JSON.stringify(err)}`);
}
// 设置视频防抖
if (setVideoStabilizationMode(videoSession)) {
// 设置色彩空间
setColorSpaceAfterCommitConfig(videoSession, true);
}
// 启动会话
try {
await videoSession.start();
} catch (error) {
let err = error as BusinessError;
console.error(`videoSession start error: ${JSON.stringify(err)}`);
}
// 启动录像输出流
videoOutput.start((err: BusinessError) => {
if (err) {
console.error(`Failed to start the video output. error: ${JSON.stringify(err)}`);
return;
}
console.info('Callback invoked to indicate the video output start success.');
});
// 开始录像
try {
await avRecorder.start();
} catch (error) {
let err = error as BusinessError;
console.error(`avRecorder start error: ${JSON.stringify(err)}`);
}
// 停止录像输出流
videoOutput.stop((err: BusinessError) => {
if (err) {
console.error(`Failed to stop the video output. error: ${JSON.stringify(err)}`);
return;
}
console.info('Callback invoked to indicate the video output stop success.');
});
// 停止录像
try {
await avRecorder.stop();
} catch (error) {
let err = error as BusinessError;
console.error(`avRecorder stop error: ${JSON.stringify(err)}`);
}
// 停止当前会话
await videoSession.stop();
// 关闭文件
fs.closeSync(file);
// 释放相机输入流
await cameraInput.close();
// 释放预览输出流
await previewOutput.release();
// 释放录像输出流
await videoOutput.release();
// 释放会话
await videoSession.release();
// 会话置空
videoSession = undefined;
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-dev-native-V14
爬取时间: 2025-04-28 20:17:48
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/native-camera-device-management-V14
爬取时间: 2025-04-28 20:18:01
来源: Huawei Developer
在开发一个相机应用前，需要先通过调用相机接口来创建一个独立的相机设备。
开发步骤
详细的API说明请参考Camera API参考。
1.  导入NDK接口。选择系统提供的NDK接口能力，导入NDK接口的方法如下。
2.  在CMake脚本中链接相关动态库。
3.  通过OH_Camera_GetCameraManager()方法，获取cameraManager对象。 如果获取对象失败，说明相机可能被占用或无法使用。如果被占用，须等到相机被释放后才能重新获取。
4.  通过OH_CameraManager_GetSupportedCameras()方法，获取当前设备支持的相机列表，列表中存储了设备支持的所有相机ID。若列表不为空，则说明列表中的每个ID都支持独立创建相机对象；否则，说明当前设备无可用相机，不可继续后续操作。
状态监听
在相机应用开发过程中，可以随时监听相机状态，包括新相机的出现、相机的移除、相机的可用状态。在回调函数中，通过相机ID、相机状态这两个参数进行监听，如当有新相机出现时，可以将新相机加入到应用的备用相机中。
通过注册cameraStatus事件，通过回调返回监听结果，callback返回Camera_StatusInfo参数，参数的具体内容可参考相机管理器回调接口实例Camera_StatusInfo。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/native-camera-device-input-V14
爬取时间: 2025-04-28 20:18:17
来源: Huawei Developer
在开发相机应用时，需要先参考开发准备申请相关权限。
相机应用通过调用和控制相机设备，完成预览、拍照和录像等基础操作。
开发步骤
详细的API说明请参考Camera API参考。
1.  导入NDK接口。选择系统提供的NDK接口能力，导入NDK接口的方法如下。
2.  在CMake脚本中链接相关动态库。
3.  通过OH_CameraManager_CreateCameraInput()方法，获取cameraInput对象。 在相机设备输入之前需要先完成相机管理，详细开发步骤请参考相机管理
4.  通过OH_CameraManager_GetSupportedSceneModes()方法，获获取当前相机设备支持的模式列表，列表中存储了相机设备支持的所有模式Camera_SceneMode。
5.  通过OH_CameraManager_GetSupportedCameraOutputCapabilityWithSceneMode()方法，获取当前设备支持的所有输出流，如预览流、拍照流等。输出流在CameraOutputCapability中的各个profile字段中。根据相机设备指定模式Camera_SceneMode的不同，需要添加不同类型的输出流。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/native-camera-session-management-V14
爬取时间: 2025-04-28 20:18:31
来源: Huawei Developer
相机使用预览、拍照、录像、元数据功能前，均需要创建相机会话。
在会话中，可以完成以下功能：
-  配置相机的输入流和输出流。相机在拍摄前，必须完成输入输出流的配置。 配置输入流即添加设备输入，对用户而言，相当于选择设备的某一摄像头拍摄；配置输出流，即选择数据将以什么形式输出。当应用需要实现拍照时，输出流应配置为预览流和拍照流，预览流的数据将显示在XComponent组件上，拍照流的数据将通过ImageReceiver接口的能力保存到相册中。
-  添加闪光灯、调整焦距等配置。具体支持的配置及接口说明请参考Camera API参考。
-  会话切换控制。应用可以通过移除和添加输出流的方式，切换相机模式。如当前会话的输出流为拍照流，应用可以将拍照流移除，然后添加视频流作为输出流，即完成了拍照到录像的切换。
完成会话配置后，应用提交和开启会话，可以开始调用相机相关功能。
开发步骤
1.  导入NDK相关接口，导入方法如下。
2.  在CMake脚本中链接相关动态库。
3.  调用cameraManager类中的OH_CameraManager_CreateCaptureSession()方法创建一个会话。
4.  调用captureSession类中的OH_CaptureSession_SetSessionMode()方法配置会话模式。
5.  调用captureSession类中的OH_CaptureSession_BeginConfig()方法配置会话。
6.  使能。向会话中添加相机的输入流和输出流，调用OH_CaptureSession_AddInput()添加相机的输入流；调用OH_CaptureSession_AddPreviewOutput()和OH_CaptureSession_AddPhotoOutput()添加相机的输出流。以下示例代码以添加预览流previewOutput和拍照流photoOutput为例，即当前模式支持拍照和预览。 调用captureSession类中的OH_CaptureSession_CommitConfig()和OH_CaptureSession_Start()方法提交相关配置，并启动会话。
7.  会话控制。调用captureSession类中的OH_CaptureSession_Stop()方法可以停止当前会话。调用OH_CaptureSession_RemovePhotoOutput()和OH_CaptureSession_AddVideoOutput()方法可以完成会话切换控制。以下示例代码以移除拍照流photoOutput，添加视频流videoOutput为例，完成了拍照到录像的切换。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/native-camera-preview-V14
爬取时间: 2025-04-28 20:18:45
来源: Huawei Developer
预览是启动相机后看见的画面，通常在拍照和录像前执行。
开发步骤
详细的API说明请参考Camera API参考。
1.  导入NDK接口，接口中提供了相机相关的属性和方法，导入方法如下。
2.  在CMake脚本中链接相关动态库。
3.  获取SurfaceId。 XComponent组件为预览流提供的SurfaceId，而XComponent的能力由UI提供，相关介绍可参考XComponent组件参考。
4.  根据传入的SurfaceId，通过OH_CameraManager_GetSupportedCameraOutputCapability()方法获取当前设备支持的预览能力。通过OH_CameraManager_CreatePreviewOutput()方法创建预览输出流，其中，OH_CameraManager_CreatePreviewOutput()方法中的参数分别是cameraManager指针，previewProfiles数组中的第一项，步骤三中获取的surfaceId，以及返回的previewOutput指针。
5.  使能。当session完成CommitConfig后通过调用OH_CaptureSession_Start()方法输出预览流，接口调用失败会返回相应错误码，错误码类型参见Camera_ErrorCode。
6.  通过OH_CaptureSession_Stop()方法停止预览流，接口调用失败会返回相应错误码，错误码类型参见Camera_ErrorCode。
状态监听
在相机应用开发过程中，可以随时监听预览输出流状态，包括预览流启动、预览流结束、预览流输出错误。
-  通过注册固定的frameStart回调函数获取监听预览启动结果，previewOutput创建成功时即可监听，预览第一次曝光时触发，有该事件返回结果则认为预览流已启动。
-  通过注册固定的frameEnd回调函数获取监听预览结束结果，previewOutput创建成功时即可监听，预览完成最后一帧时触发，有该事件返回结果则认为预览流已结束。
-  通过注册固定的error回调函数获取监听预览输出错误结果，callback返回预览输出接口使用错误时对应的错误码，错误码类型参见Camera_ErrorCode。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/native-camera-preview-imagereceiver-V14
爬取时间: 2025-04-28 20:18:59
来源: Huawei Developer
通过ImageReceiver创建预览输出，获取预览流实时数据，以供后续进行图像二次处理，比如应用可以对其添加滤镜算法等。
开发步骤
详细的API说明请参考Camera API参考。
1.  导入NDK接口，接口中提供了相机相关的属性和方法，导入方法如下。
2.  在CMake脚本中链接相关动态库。
3.  初始化图片接收器ImageReceiver实例，获取SurfaceId。 通过image的OH_ImageReceiverNative_Create方法创建OH_ImageReceiverNative实例，再通过实例的OH_ImageReceiverNative_GetReceivingSurfaceId方法获取SurfaceId。
4.  通过上一步获取到的SurfaceId创建预览流（在创建预览流之前需要将SurfaceId类型转成char *），参考预览(C/C++)步骤4。
5.  创建会话，使能会话，参考会话管理(C/C++)。
6.  注册ImageReceiver图片接收器的回调，监听获取每帧上报图像内容。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/native-camera-shooting-V14
爬取时间: 2025-04-28 20:19:12
来源: Huawei Developer
拍照是相机的最重要功能之一，拍照模块基于相机复杂的逻辑，为了保证用户拍出的照片质量，在中间步骤可以设置分辨率、闪光灯、焦距、照片质量及旋转角度等信息。
开发步骤
详细的API说明请参考Camera API参考。
1.  导入NDK接口，接口中提供了相机相关的属性和方法，导入方法如下。
2.  在CMake脚本中链接相关动态库。
3.  创建并打开相机设备，参考设备输入(C/C++)步骤3-5。
4.  选择设备支持的输出流能力，创建拍照输出流。 通过OH_CameraManager_CreatePhotoOutputWithoutSurface()方法创建拍照输出流。
5.  注册单段式(PhotoAvailable)拍照回调，若应用希望快速得到回图，推荐使用分段式拍照回调(PhotoAssetAvailable)。 如果已经注册了PhotoAssetAvailable回调，并且在Session开始之后又注册了PhotoAvailable回调，PhotoAssetAvailable和PhotoAvailable同时注册，会导致流被重启，仅PhotoAssetAvailable生效。 不建议开发者同时注册PhotoAssetAvailable和PhotoAvailable。 单段式拍照开发流程（PhotoAssetAvailable）： NAPI层buffer回处理参考示例代码： ArkTS侧buffer处理参考示例代码：
6.  创建拍照类型会话，参考会话管理(C/C++)，开启会话，准备拍照。
7.  配置拍照参数（可选）。 配置相机的参数可以调整拍照的一些功能，包括闪光灯、变焦、焦距等。
8.  触发拍照。 通过OH_PhotoOutput_Capture()方法，执行拍照任务。
状态监听
在相机应用开发过程中，可以随时监听拍照输出流状态，包括拍照流开始、拍照帧的开始与结束、拍照输出流的错误。
-  通过注册固定的onFrameStart回调函数获取监听拍照开始结果，photoOutput创建成功时即可监听，拍照第一次曝光时触发。
-  通过注册固定的onFrameEnd回调函数获取监听拍照结束结果，photoOutput创建成功时即可监听。
-  通过注册固定的onError回调函数获取监听拍照输出流的错误结果。callback返回拍照输出接口使用错误时的对应错误码，错误码类型参见Camera_ErrorCode。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/native-camera-deferred-capture-V14
爬取时间: 2025-04-28 20:19:26
来源: Huawei Developer
分段式拍照是相机的最重要功能之一，相机输出低质量图用作快速显示，提升用户感知拍照速度，同时使用高质量图保证最后的成图质量达到系统相机的水平，既满足了后处理算法的需求，又不阻塞前台的拍照速度，构筑相机性能竞争力，提升了用户的体验。
开发步骤
详细的API说明请参考Camera API参考。
1.  导入NDK接口，接口中提供了相机相关的属性和方法，导入方法如下。
2.  在CMake脚本中链接相关动态库。
3.  相机初始化及拍照触发参考拍照(C/C++)。
4.  注册**分段式(PhotoAssetAvailable)**拍照回调，对比单端式拍照，仅注册的拍照回调接口不同。 如果已经注册了PhotoAssetAvailable回调，并且在Session开始之后又注册了PhotoAvailable回调，PhotoAssetAvailable和PhotoAvailable同时注册，会导致流被重启，仅PhotoAssetAvailable生效。 不建议开发者同时注册PhotoAssetAvailable和PhotoAvailable。 注册PhotoAssetAvailableCallback回调，接收分段式拍照回图示例： 分段式拍照开发流程（PhotoAssetAvailableCallback）：

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/native-camera-recording-V14
爬取时间: 2025-04-28 20:19:40
来源: Huawei Developer
录像也是相机应用的最重要功能之一，录像是循环帧的捕获。对于录像的流畅度，开发者可以参考拍照参考中的步骤5，设置分辨率、闪光灯、焦距、照片质量及旋转角度等信息。
开发步骤
详细的API说明请参考Camera API参考。
1.  导入NDK接口，接口中提供了相机相关的属性和方法，导入方法如下。
2.  在CMake脚本中链接相关动态库。
3.  获取SurfaceId。 系统提供的media接口可以创建一个录像AVRecorder实例，通过该实例的getInputSurface()方法获取SurfaceId。
4.  创建录像输出流。 根据传入的SurfaceId，通过CameraOutputCapability类中的videoProfiles，可获取当前设备支持的录像输出流。然后，定义创建录像的参数，通过createVideoOutput()方法创建录像输出流。
5.  开始录像。 通过videoOutput的OH_VideoOutput_Start()方法启动录像输出流。
6.  停止录像。 通过videoOutput的OH_VideoOutput_Stop()方法停止录像输出流。
状态监听
在相机应用开发过程中，可以随时监听录像输出流状态，包括录像开始、录像结束、录像流输出的错误。
-  通过注册固定的frameStart回调函数获取监听录像开始结果，videoOutput创建成功时即可监听，录像第一次曝光时触发，有该事件返回结果则认为录像开始。
-  通过注册固定的frameEnd回调函数获取监听录像结束结果，videoOutput创建成功时即可监听，录像完成最后一帧时触发，有该事件返回结果则认为录像流已结束。
-  通过注册固定的error回调函数获取监听录像输出错误结果，callback返回预览输出接口使用错误时对应的错误码，错误码类型参见Camera_ErrorCode。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/native-camera-metadata-V14
爬取时间: 2025-04-28 20:19:54
来源: Huawei Developer
元数据（Metadata）是对相机返回的图像信息数据的描述和上下文，针对图像信息，提供的更详细的数据，如照片或视频中，识别人像的取景框坐标等信息。
Metadata主要是通过一个TAG（Key），去找对应的Data，用于传递参数和配置信息，减少内存拷贝操作。
开发步骤
详细的API说明请参考Camera API参考。
1.  导入NDK接口，导入方法如下。
2.  在CMake脚本中链接相关动态库。
3.  调用OH_CameraManager_GetSupportedCameraOutputCapability()方法，获取当前设备支持的元数据类型metaDataObjectType，并通过OH_CameraManager_CreateMetadataOutput()方法创建元数据输出流。
4.  调用start()方法输出metadata数据，接口调用失败时，会返回相应错误码。
5.  调用stop()方法停止输出metadata数据，接口调用失败会返回相应错误码。
状态监听
在相机应用开发过程中，可以随时监听metadata数据以及输出流的状态。
-  通过注册监听获取metadata对象，监听事件固定为metadataObjectsAvailable。检测到有效metadata数据时，callback返回相应的metadata数据信息，metadataOutput创建成功时可监听。 当前的元数据类型仅支持人脸检测（FACE_DETECTION）功能。元数据信息对象为识别到的人脸区域的矩形信息（Rect），包含矩形区域的左上角x坐标、y坐标和矩形的宽高数据。
-  通过注册回调函数，获取监听metadata流的错误结果，callback返回metadata输出接口使用错误时返回的错误码，错误码类型参见Camera_ErrorCode。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-focus-native-V14
爬取时间: 2025-04-28 20:20:07
来源: Huawei Developer
相机框架提供对设备对焦的能力，业务应用可以根据使用场景进行对焦模式和对焦点的设置。
开发步骤
详细的API说明请参考Camera API参考。
1.  导入NDK接口，导入方法如下。
2.  在CMake脚本中链接相关动态库。
3.  调用OH_CaptureSession_SetFocusMode设置对焦模式。
4.  需要在Session调用OH_CaptureSession_CommitConfig完成配流之后调用。
状态监听
在相机应用开发过程中，可以随时监听相机聚焦的状态变化。
通过注册focusStateChange的回调函数获取监听结果，仅当自动对焦模式时，且相机对焦状态发生改变时触发该事件。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/native-camera-torch-use-V14
爬取时间: 2025-04-28 20:20:21
来源: Huawei Developer
手电筒模式的使用是通过手机启用手电筒功能，使设备的手电筒功能持续保持常亮状态。
在使用相机应用并操作手电筒功能时，存在以下几种情况说明：
开发步骤
详细的API说明请参考Camera API参考。
1.  导入NDK接口。选择系统提供的NDK接口能力，导入NDK接口的方法如下。
2.  在CMake脚本中链接相关动态库。
3.  通过OH_CameraManager_IsTorchSupported()方法，检测当前设备是否支持手电筒。
4.  通过OH_CameraManager_IsTorchSupportedByTorchMode()方法，检测当前设备是否支持指定的手电筒模式。
5.  通过OH_CameraManager_SetTorchMode()方法，设置当前设备的手电筒模式。
状态监听
在相机应用开发过程中，可以随时监听手电筒状态，包括手电筒打开、手电筒关闭、手电筒不可用、手电筒恢复可用。手电筒状态发生变化，可通过回调函数获取手电筒模式的变化。
通过注册torchStatus事件，通过回调返回监听结果，callback返回Camera_TorchStatusInfo参数，参数的具体内容可参考相机管理器回调接口实例Camera_TorchStatusInfo。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-setframerate-native-V14
爬取时间: 2025-04-28 20:20:35
来源: Huawei Developer
动态调整帧率是直播、视频等场景下控制预览效果的重要能力之一。应用可通过此能力，显性地控制流输出帧率，以适应不同帧率下的业务目标。
某些场景下降低帧率可在相机设备启用时降低功耗。
约束与限制
支持的帧率范围及帧率的设置依赖于硬件能力的实现，不同的硬件平台可能拥有不同的默认帧率。
开发流程
相机使用预览功能前，均需要创建相机会话。完成会话配置后，应用提交和开启会话，才可以开始调用相机相关功能。
流程图如下所示：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165947.61324429638416598467584328182435:50001231000000:2800:CCE050A6ADC0C68128D95723C10A490C23861D6461D7B3E35F8112008B2A119B.png)
与普通的预览流程相比，动态调整预览帧率的注意点如图上标识：
1.  仅当Session处于NORMAL_PHOTO或NORMAL_VIDEO模式时，支持调整预览流帧率。调整帧率的创建会话方式见创建Session会话并指定模式。
如何配置会话（Session）、释放资源，请参考会话管理>预览。
创建Session会话并指定模式
相机使用预览等功能前，均需创建相机会话，调用OH_CameraManager_CreateCaptureSession创建一个会话。
创建会话时调用OH_CaptureSession_SetSessionMode指定Camera_SceneMode为NORMAL_PHOTO或NORMAL_VIDEO，创建出的Session处于拍照或录像模式。
以创建Session会话并指定为NORMAL_PHOTO模式为例：
动态调整帧率
1.  需要在Session调用OH_CaptureSession_CommitConfig完成配流之后调用。
2.  根据实际开发需求，调用OH_PreviewOutput_SetFrameRate接口对帧率进行动态调整。
3.  （可选）通过OH_PreviewOutput_GetActiveFrameRate接口查询已设置过并生效的帧率。 仅通过OH_PreviewOutput_SetFrameRate接口显性设置过帧率才可查询当前生效帧率信息。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-preconfig-native-V14
爬取时间: 2025-04-28 20:20:49
来源: Huawei Developer
相机预配置（Preconfig），对常用的场景和分辨率进行了预配置集成，可简化开发相机应用流程，提高应用的开发效率。
开发者在开发相机应用时，在获取到CameraDevice之后，如果遵循通用流程开发，步骤较为繁琐。需要先查询当前相机在指定模式下所支持的各类输出的配置信息，拿到Camera_OutputCapability之后，应用开发者还需要对里面的各类数据进行解析，筛选，找到自己需要的配置数据Camera_Profile和Camera_VideoProfile。最后使用对应的Profile以及VideoProfile创建对应的Camera_PreviewOutput、Camera_PhotoOutput以及Camera_VideoOutput。
为了解决上述问题，优化应用开发流程，系统针对拍照、录像两类场景（即Camera_SceneMode为NORMAL_PHOTO或NORMAL_VIDEO），提供了OH_CameraManager_CreatePreviewOutputUsedInPreconfig、OH_CameraManager_CreatePhotoOutputUsedInPreconfig、OH_CameraManager_CreateVideoOutputUsedInPreconfig接口帮助开发者快速完成相机参数配置。推荐仅需要自定义拍照界面的无需开发专业相机应用的开发者，使用相机预配置功能快速开发应用。
规格说明
系统提供了4种预配置类型（Camera_PreconfigType），分别为PRECONFIG_720P、PRECONFIG_1080P、PRECONFIG_4K、PRECONFIG_HIGH_QUALITY。以及3种画幅比例规格（Camera_PreconfigRatio），1:1画幅（PRECONFIG_RATIO_1_1）、4:3画幅（PRECONFIG_RATIO_4_3）、16:9画幅（PRECONFIG_RATIO_16_9）。
由于不同的设备所支持的能力不同。使用相机预配置（preconfig）功能时，需要先调用OH_CaptureSession_CanPreconfig或OH_CaptureSession_CanPreconfigWithRatio检查对应的PreconfigType和PreconfigRatio的组合在当前设备上是否支持。
在不同的画幅比例下，其分辨率规格不同，详见下表。
-  预配置类型PreconfigType PRECONFIG_RATIO_1_1 PRECONFIG_RATIO_4_3 PRECONFIG_RATIO_16_9 PRECONFIG_720P 720x720 960x720 1280x720 PRECONFIG_1080P 1080x1080 1440x1080 1920x1080 PRECONFIG_4K 1080x1080 1440x1080 1920x1080 PRECONFIG_HIGH_QUALITY 1440x1440 1920x1440 2560x1440
-  预配置类型PreconfigType PRECONFIG_RATIO_1_1 PRECONFIG_RATIO_4_3 PRECONFIG_RATIO_16_9 PRECONFIG_720P 720x720 960x720 1280x720 PRECONFIG_1080P 1080x1080 1440x1080 1920x1080 PRECONFIG_4K 2160x2160 2880x2160 3840x2160 PRECONFIG_HIGH_QUALITY 跟随Sensor最大能力 跟随Sensor最大能力 跟随Sensor最大能力
-  预配置类型PreconfigType PRECONFIG_RATIO_1_1 PRECONFIG_RATIO_4_3 PRECONFIG_RATIO_16_9 PRECONFIG_720P 720x720 960x720 1280x720 PRECONFIG_1080P 1080x1080 1440x1080 1920x1080 PRECONFIG_4K 1080x1080 1440x1080 1920x1080 PRECONFIG_HIGH_QUALITY 1080x1080 1440x1080 1920x1080
-  预配置类型PreconfigType PRECONFIG_RATIO_1_1 PRECONFIG_RATIO_4_3 PRECONFIG_RATIO_16_9 PRECONFIG_720P 720x720 960x720 1280x720 PRECONFIG_1080P 1080x1080 1440x1080 1920x1080 PRECONFIG_4K 2160x2160 2880x2160 3840x2160 PRECONFIG_HIGH_QUALITY 2160x2160 2880x2160 3840x2160
-  预配置类型PreconfigType PRECONFIG_RATIO_1_1 PRECONFIG_RATIO_4_3 PRECONFIG_RATIO_16_9 PRECONFIG_720P 跟随Sensor最大能力 跟随Sensor最大能力 跟随Sensor最大能力 PRECONFIG_1080P 跟随Sensor最大能力 跟随Sensor最大能力 跟随Sensor最大能力 PRECONFIG_4K 跟随Sensor最大能力 跟随Sensor最大能力 跟随Sensor最大能力 PRECONFIG_HIGH_QUALITY 跟随Sensor最大能力 跟随Sensor最大能力 跟随Sensor最大能力
| 预配置类型PreconfigType  | PRECONFIG_RATIO_1_1  | PRECONFIG_RATIO_4_3  | PRECONFIG_RATIO_16_9  |
| --- | --- | --- | --- |
| PRECONFIG_720P  | 720x720  | 960x720  | 1280x720  |
| PRECONFIG_1080P  | 1080x1080  | 1440x1080  | 1920x1080  |
| PRECONFIG_4K  | 1080x1080  | 1440x1080  | 1920x1080  |
| PRECONFIG_HIGH_QUALITY  | 1440x1440  | 1920x1440  | 2560x1440  |
| 预配置类型PreconfigType  | PRECONFIG_RATIO_1_1  | PRECONFIG_RATIO_4_3  | PRECONFIG_RATIO_16_9  |
| --- | --- | --- | --- |
| PRECONFIG_720P  | 720x720  | 960x720  | 1280x720  |
| PRECONFIG_1080P  | 1080x1080  | 1440x1080  | 1920x1080  |
| PRECONFIG_4K  | 2160x2160  | 2880x2160  | 3840x2160  |
| PRECONFIG_HIGH_QUALITY  | 跟随Sensor最大能力  | 跟随Sensor最大能力  | 跟随Sensor最大能力  |
| 预配置类型PreconfigType  | PRECONFIG_RATIO_1_1  | PRECONFIG_RATIO_4_3  | PRECONFIG_RATIO_16_9  |
| --- | --- | --- | --- |
| PRECONFIG_720P  | 720x720  | 960x720  | 1280x720  |
| PRECONFIG_1080P  | 1080x1080  | 1440x1080  | 1920x1080  |
| PRECONFIG_4K  | 1080x1080  | 1440x1080  | 1920x1080  |
| PRECONFIG_HIGH_QUALITY  | 1080x1080  | 1440x1080  | 1920x1080  |
| 预配置类型PreconfigType  | PRECONFIG_RATIO_1_1  | PRECONFIG_RATIO_4_3  | PRECONFIG_RATIO_16_9  |
| --- | --- | --- | --- |
| PRECONFIG_720P  | 720x720  | 960x720  | 1280x720  |
| PRECONFIG_1080P  | 1080x1080  | 1440x1080  | 1920x1080  |
| PRECONFIG_4K  | 2160x2160  | 2880x2160  | 3840x2160  |
| PRECONFIG_HIGH_QUALITY  | 2160x2160  | 2880x2160  | 3840x2160  |
| 预配置类型PreconfigType  | PRECONFIG_RATIO_1_1  | PRECONFIG_RATIO_4_3  | PRECONFIG_RATIO_16_9  |
| --- | --- | --- | --- |
| PRECONFIG_720P  | 跟随Sensor最大能力  | 跟随Sensor最大能力  | 跟随Sensor最大能力  |
| PRECONFIG_1080P  | 跟随Sensor最大能力  | 跟随Sensor最大能力  | 跟随Sensor最大能力  |
| PRECONFIG_4K  | 跟随Sensor最大能力  | 跟随Sensor最大能力  | 跟随Sensor最大能力  |
| PRECONFIG_HIGH_QUALITY  | 跟随Sensor最大能力  | 跟随Sensor最大能力  | 跟随Sensor最大能力  |
开发示例
1.  在CMake脚本中链接相关动态库。
2.  cpp侧导入NDK接口，并根据传入的SurfaceId进行拍照。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/camera-best-practices-native-V14
爬取时间: 2025-04-28 20:21:02
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/native-camera-shooting-case-V14
爬取时间: 2025-04-28 20:21:16
来源: Huawei Developer
在开发相机应用时，需要先参考开发准备申请相关权限。
当前示例提供完整的拍照流程及其接口调用顺序的介绍。对于单个流程（如设备输入、会话管理、拍照）的介绍请参考相机开发指导(Native)的具体章节。
开发流程
在获取到相机支持的输出流能力后，开始创建拍照流，开发流程如下。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165948.20629356252094315588863396260970:50001231000000:2800:40D72A18907EAE124494CBCB2674CA04BA336C78BA7A0A6A589CF381CD8622A8.png)
完整示例
1.  在CMake脚本中链接相关动态库。
2.  创建头文件ndk_camera.h
3.  cpp侧导入NDK接口，并根据传入的SurfaceId进行拍照。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/native-camera-recording-case-V14
爬取时间: 2025-04-28 20:21:29
来源: Huawei Developer
在开发相机应用时，需要先参考开发准备申请相关权限。
当前示例提供完整的录像流程及其接口调用顺序的介绍。对于单个流程（如设备输入、会话管理、录像）的介绍请参考相机开发指导(Native)的具体章节。
开发流程
在获取到相机支持的输出流能力后，开始创建录像流，开发流程如下。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165948.86044452856078194931656345711322:50001231000000:2800:48A51F2D28A5CB52394BFB901FAE044A136080A05B8820F240F3D53079B25FAA.png)
完整示例
1.  在CMake脚本中链接相关动态库。
2.  创建头文件ndk_camera.h。
3.  cpp侧导入NDK接口，并根据传入的SurfaceId进行录像。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/drm-kit-V14
爬取时间: 2025-04-28 20:21:43
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/drm-overview-V14
爬取时间: 2025-04-28 20:21:57
来源: Huawei Developer
开发者通过调用DRM Kit（Digital Rights Management Kit）提供的接口可以实现DRM加密音视频的解密，支持设备DRM证书管理、许可证管理、内容解密等。
能力范围
-  解密播放：提供音视频统一的解密能力，音视频类应用接入DRM后，可以播放带有数字版权的节目。
-  DRM解决方案支持：支持不同的DRM解决方案。
-  在线证书下载：支持设备证书认证申请、设置。
-  许可证在线/离线授权：支持在线许可证请求和设置，支持离线许可证的申请、加载、更新、删除及状态查询。
亮点/特征
-  支持许可证及解密会话管理 支持多会话，并支持在会话中申请和设置许可证，将解密会话与许可证绑定。
-  支持安全/非安全视频通路 支持安全视频通路，安全内存解密调用，安全解码；支持非安全通路，非安全内存解密调用，非安全解密。
基础概念
在开发前，需要先了解以下基础概念：
-  DRM解决方案（DRM Plugin） DRM框架层以下实现DRM HDI层接口的驱动，内容解密在驱动中完成。
-  DRM会话（MediaKeySession） DRM会话用于媒体密钥管理及媒体解密，会话的生命周期由DRM实例（MediaKeySystem）管理。
开发模型
数字版权保护技术的具体实现方式和技术细节会根据不同的内容类型、保护需求和应用场景有所不同。
数字版权保护的工作流程，可概括为设备证书管理、许可证管理和解密管理三部分。
-  客户端创建DRM实例完成，创建DRM解密会话，检测到设备没有设备DRM证书或证书异常，启动在线证书下载流程。
-  客户端在处理完证书下载后，创建DRM解密会话，生成许可证请求，客户端获取服务端返回的许可证请求响应并设置到设备上。许可证包含在线和离线两种，支持状态检查、移除、获取许可证ID等功能。
-  客户端设置解密配置，进行内容解密等。
了解数字版权保护工作流程后，建议开发者了解数字版权保护的开发模型，便于更好地开发带有数字版权保护功能的应用。
图1数字版权保护开发模型
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165948.57957809099019348162176911144875:50001231000000:2800:DBAAC6DCFFAAD4BDC175FDB6F4128DABEB54BFE86631DAAEAD2740C12C28E385.png)
播放器应用通过调用DRM（数字版权保护）接口，实现设备证书管理、许可证管理、解密管理等基本操作。在实现基本操作过程中，先进行DRM实例创建和设备证书申请与设置，其次进行会话实例创建和许可证申请与设置，等加密码流到达，通过底层的设备硬件接口（HDI，Hardware Device Interfaces）送到具体的DRM解决方案中进行解密。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/drm-arkts-dev-V14
爬取时间: 2025-04-28 20:22:10
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/drm-mediakeysystem-management-V14
爬取时间: 2025-04-28 20:22:24
来源: Huawei Developer
DRM系统管理（MediaKeySystem）支持MediaKeySystem实例管理、设备DRM证书申请与处理、会话实例管理、离线媒体密钥管理、获取DRM度量统计信息、设备属性等。在使用DRM Kit功能时，先查询设备是否支持对应DRM解决方案的DRM功能。在DRM Kit里DRM解决方案以插件形式存在，所以也叫DRM插件。
开发步骤
详细的API说明请参考DRM API参考。
1.  导入相关接口，导入方法如下。
```typescript
import { drm } from '@kit.DrmKit';
```
2.  导入BusinessError模块，用于获取drm模块相关接口抛出的错误码。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
```
3.  查询设备是否支持对应的DRM解决方案。 如果查询为false，说明该设备不支持对应的DRM解决方案。
```typescript
let isSupported: boolean = drm.isMediaKeySystemSupported("com.wiseplay.drm", "video/avc", drm.ContentProtectionLevel.CONTENT_PROTECTION_LEVEL_SW_CRYPTO);
```
4.  （可选）获取设备上DRM解决方案的名称和唯一标识的列表。 如果查询出的数组为空，说明该设备中不存在支持的DRM解决方案。
```typescript
let description: drm.MediaKeySystemDescription[] = drm.getMediaKeySystems();
```
5.  创建MediaKeySystem实例。 如果创建失败则返回undefined，说明该设备不支持DRM能力。
```typescript
let mediaKeySystem: drm.MediaKeySystem = drm.createMediaKeySystem("com.wiseplay.drm");
```
6.  （可选）获取指定DRM解决方案名称对应的唯一标识。 如果查询出的uuid的长度为0，说明该设备中不存在支持的DRM解决方案。
```typescript
let uuid: string = drm.getMediaKeySystemUuid("com.wiseplay.drm");
```
7.  （可选）设置和获取DRM解决方案支持属性值。
```typescript
// DRM解决方案支持属性设置时，设置DRM解决方案支持的字符串类型的属性值
mediaKeySystem.setConfigurationString("configName", "configValue");
// 获取字符串类型的属性值
let configValueString : string = mediaKeySystem.getConfigurationString("version");
let configValueUint8ArrayA: Uint8Array = new Uint8Array([0x00, 0x00, 0x00, 0x00]);
// DRM解决方案支持属性设置时，设置DRM解决方案支持的字符数组类型的属性值
mediaKeySystem.setConfigurationByteArray("Uint8ArrayConfigName", configValueUint8ArrayA);
// 获取字符数组类型的属性值
let configValueUint8ArrayB: Uint8Array = mediaKeySystem.getConfigurationByteArray("Uint8ArrayConfigName");
```
8.  （可选）获取设备支持的最大内容保护级别。
```typescript
let contentProtectionLevel: drm.ContentProtectionLevel = drm.ContentProtectionLevel.CONTENT_PROTECTION_LEVEL_UNKNOWN;
try {
contentProtectionLevel = mediaKeySystem.getMaxContentProtectionLevel();
} catch (err) {
let error = err as BusinessError;
console.error(`getMaxContentProtectionLevel ERROR: ${error}`);
}
```
9.  状态监听。 监听MediaKeySystem设备DRM证书请求事件。 通过注册的keySystemRequired回调函数监听设备DRM证书请求，MediaKeySystem创建成功时即可监听，需要设备DRM证书时触发。
```typescript
mediaKeySystem.on('keySystemRequired', (eventInfo: drm.EventInfo) => {
console.log('keySystemRequired' + 'extra:' + eventInfo.extraInfo + ' data:' + eventInfo.info);
});
```
10.  （可选）获取设备DRM证书状态。
```typescript
let certificateStatus: drm.CertificateStatus = mediaKeySystem.getCertificateStatus();
```
11.  生成设备DRM证书请求。
DRM解决方案创建MediaKeySession会话时，如果没有设备DRM证书会触发DRM证书请求事件，此时，先获取设备上设备DRM证书状态，若设备上没有DRM证书或DRM证书状态异常（状态不是drm.CertificateStatus.CERT_STATUS_PROVISIONED），生成设备DRM证书请求。
```typescript
if(certificateStatus != drm.CertificateStatus.CERT_STATUS_PROVISIONED){
mediaKeySystem.generateKeySystemRequest().then(async (drmRequest: drm.ProvisionRequest) => {
console.info("generateKeySystemRequest success", drmRequest.data, drmRequest.defaultURL);
}).catch((err:BusinessError) =>{
console.info("generateKeySystemRequest err end", err.code);
});
} else {
console.info("The certificate already exists.");
}
```
在将设备DRM证书请求发送到DRM服务获取设备DRM证书请求响应后，处理设备DRM证书响应。
```typescript
// 将设备DRM证书请求返回的drmRequest.data通过网络请求发送给DRM证书服务获取设备DRM证书请求响应，设置设备DRM证书请求响应
let provisionResponseByte = new Uint8Array([0x00, 0x00, 0x00, 0x00]);
mediaKeySystem.processKeySystemResponse(provisionResponseByte).then(() => {
console.info("processKeySystemResponse success");
}).catch((err:BusinessError) =>{
console.info("processKeySystemResponse err end", err.code);
});
```
1.  创建MediaKeySession会话。 创建指定内容保护级别的MediaKeySession会话、或创建DRM解决方案默认内容保护级别的MediaKeySession会话。
```typescript
let mediaKeySession: drm.MediaKeySession = mediaKeySystem.createMediaKeySession();
```
2.  （可选）获取离线媒体密钥标识列表，媒体密钥标识用于对离线媒体密钥的管理。
```typescript
let offlineMediaKeyIds: Uint8Array[] = mediaKeySystem.getOfflineMediaKeyIds();
```
3.  （可选）获取离线媒体密钥状态。
```typescript
try {
let offlineMediaKeyStatus: drm.OfflineMediaKeyStatus = mediaKeySystem.getOfflineMediaKeyStatus(offlineMediaKeyIds[0]);
} catch (err) {
let error = err as BusinessError;
console.error(`getOfflineMediaKeyStatus ERROR: ${error}`);
}
```
4.  （可选）移除离线媒体密钥。
```typescript
try {
mediaKeySystem.clearOfflineMediaKeys(offlineMediaKeyIds[0]);
} catch (err) {
let error = err as BusinessError;
console.error(`clearOfflineMediaKeys ERROR: ${error}`);
}
```
5.  （可选）获取DRM度量记录，包括当前会话数、插件版本信息、解密次数和解密失败次数等。
```typescript
let statisticKeyValue: drm.StatisticKeyValue[] = mediaKeySystem.getStatistics();
```
6.  销毁MediaKeySession实例。 完成加密媒体解密，MediaKeySession实例不再使用时，销毁MediaKeySession实例。
```typescript
// MediaKeySession实例使用完需要进行资源释放
mediaKeySession.destroy();
```
7.  销毁MediaKeySystem实例。 完成DRM功能使用，MediaKeySystem实例不再使用，销毁MediaKeySystem实例。
```typescript
// MediaKeySystem实例使用完需要进行资源释放
mediaKeySystem.destroy();
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/drm-mediakeysession-management-V14
爬取时间: 2025-04-28 20:22:38
来源: Huawei Developer
DRM会话管理（MediaKeySession）支持媒体密钥管理及媒体解密等，MediaKeySession实例由系统管理里的MediaKeySystem实例创建和销毁。
开发步骤
详细的API说明请参考DRM API参考。
1.  导入相关接口，导入方法如下。
```typescript
import { drm } from '@kit.DrmKit';
```
2.  导入BusinessError模块，用于获取drm模块相关接口抛出的错误码。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
```
3.  状态监听。 监听MediaKeySession实例的事件，包括密钥请求事件、密钥过期事件、第三方自定义事件、密钥过期更新事件、密钥变换事件。 监听密钥请求事件，媒体密钥请求时触发。 监听媒体密钥过期事件，媒体密钥过期时触发。 监听DRM解决方案自定义事件，DRM解决方案自定义事件发生时触发。 监听媒体密钥过期更新事件，媒体密钥过期更新时触发。 监听密钥变换事件，媒体密钥变换时触发。
```typescript
mediaKeySession.on('keyRequired', (eventInfo: drm.EventInfo) => {
console.log('keyRequired' + 'info:' + eventInfo.info + ' extraInfo:' + eventInfo.extraInfo);
});
```
4.  监听密钥请求事件，媒体密钥请求时触发。
```typescript
mediaKeySession.on('keyRequired', (eventInfo: drm.EventInfo) => {
console.log('keyRequired' + 'info:' + eventInfo.info + ' extraInfo:' + eventInfo.extraInfo);
});
```
5.  监听媒体密钥过期事件，媒体密钥过期时触发。
```typescript
mediaKeySession.on('keyExpired', (eventInfo: drm.EventInfo) => {
console.log('keyExpired' + 'info:' + eventInfo.info + ' extraInfo:' + eventInfo.extraInfo);
});
```
6.  监听DRM解决方案自定义事件，DRM解决方案自定义事件发生时触发。
```typescript
mediaKeySession.on('vendorDefined', (eventInfo: drm.EventInfo) => {
console.log('vendorDefined' + 'info:' + eventInfo.info + ' extraInfo:' + eventInfo.extraInfo);
});
```
7.  监听媒体密钥过期更新事件，媒体密钥过期更新时触发。
```typescript
mediaKeySession.on('expirationUpdate', (eventInfo: drm.EventInfo) => {
console.log('expirationUpdate' + 'info:' + eventInfo.info + ' extraInfo:' + eventInfo.extraInfo);
});
```
8.  监听密钥变换事件，媒体密钥变换时触发。
```typescript
mediaKeySession.on('keysChange', (keyInfo : drm.KeysInfo[], newKeyAvailable:boolean) => {
for(let i = 0; i < keyInfo.length; i++){
console.log('keysChange' + 'info:' + keyInfo[i].keyId + ' extraInfo:' + keyInfo[i].value);
}
});
```
9.  生成媒体密钥请求、设置媒体密钥请求响应。
```typescript
let initData = new Uint8Array([0x00, 0x00, 0x00, 0x00]);
// 根据DRM解决方案要求设置可选数据的值
let optionalData:drm.OptionsData[] = [{
name: "...",
value: "..."
}];
// 以下示例完成在线媒体密钥请求和响应设置
mediaKeySession.generateMediaKeyRequest("video/avc", initData, drm.MediaKeyType.MEDIA_KEY_TYPE_ONLINE, optionalData).then(async (licenseRequest) => {
console.info("generateMediaKeyRequest success", licenseRequest.mediaKeyRequestType, licenseRequest.data, licenseRequest.defaultURL);
// 将媒体密钥请求返回的licenseRequest.data通过网络请求发送给DRM服务获取媒体密钥请求响应，设置媒体密钥请求响应
let licenseResponse = new Uint8Array([0x00, 0x00, 0x00, 0x00]);
mediaKeySession.processMediaKeyResponse(licenseResponse).then((mediaKeyId: Uint8Array) => {
console.info("processMediaKeyResponse success");
}).catch((err:BusinessError) =>{
console.info("processMediaKeyResponse err end", err.code);
});
}).catch((err:BusinessError) =>{
console.info("generateMediaKeyRequest err end", err.code);
});
// 以下示例完成离线媒体密钥请求和响应设置
let offlineMediaKeyId = new Uint8Array([0x00, 0x00, 0x00, 0x00]);
mediaKeySession.generateMediaKeyRequest("video/avc", initData, drm.MediaKeyType.MEDIA_KEY_TYPE_OFFLINE, optionalData).then((licenseRequest: drm.MediaKeyRequest) => {
console.info("generateMediaKeyRequest success", licenseRequest.mediaKeyRequestType, licenseRequest.data, licenseRequest.defaultURL);
// 将媒体密钥请求返回的licenseRequest.data通过网络请求发送给DRM服务获取媒体密钥请求响应，设置媒体密钥请求响应
let licenseResponse = new Uint8Array([0x00, 0x00, 0x00, 0x00]);
mediaKeySession.processMediaKeyResponse(licenseResponse).then((mediaKeyId: Uint8Array) => {
offlineMediaKeyId = mediaKeyId;
console.info("processMediaKeyResponse success");
}).catch((err:BusinessError) =>{
console.info("processMediaKeyResponse err end", err.code);
});
}).catch((err:BusinessError) =>{
console.info("generateMediaKeyRequest err end", err.code);
});
```
10.  （可选）检查当前MediaKeySession会话的媒体密钥状态。
```typescript
try {
let keyvalue: drm.MediaKeyStatus[] = mediaKeySession.checkMediaKeyStatus();
console.info("checkMediaKeyStatus success", keyvalue[0].value);
} catch (err) {
let error = err as BusinessError;
console.error(`checkMediaKeyStatus ERROR: ${error}`);
}
```
11.  （可选）生成离线媒体密钥释放请求和处理离线媒体密钥释放响应。
```typescript
mediaKeySession.generateOfflineReleaseRequest(offlineMediaKeyId).then((OfflineReleaseRequest: Uint8Array) => {
console.info("generateOfflineReleaseRequest success", OfflineReleaseRequest);
// 将媒体密钥释放请求返回的OfflineReleaseRequest通过网络请求发送给DRM服务获取媒体密钥释放请求响应，设置媒体密钥释放请求响应
let OfflineReleaseResponse = new Uint8Array([0x00, 0x00, 0x00, 0x00]);
mediaKeySession.processOfflineReleaseResponse(offlineMediaKeyId, OfflineReleaseResponse).then(() => {
console.info("processOfflineReleaseResponse success");
}).catch((err:BusinessError) =>{
console.info("processOfflineReleaseResponse err end", err.code);
});
}).catch((err:BusinessError) =>{
console.info("generateOfflineReleaseRequest err end", err.code);
});
```
12.  （可选）恢复离线媒体密钥。
```typescript
// 恢复指定媒体密钥信息到当前会话
mediaKeySession.restoreOfflineMediaKeys(offlineMediaKeyId).then(() => {
console.log("restoreOfflineMediaKeys success.");
}).catch((err: BusinessError) => {
console.error(`restoreOfflineMediaKeys: ERROR: ${err}`);
});
```
13.  （可选）获取当前会话的安全级别。
```typescript
try {
let contentProtectionLevel: drm.ContentProtectionLevel = mediaKeySession.getContentProtectionLevel();
} catch (err) {
let error = err as BusinessError;
console.error(`getContentProtectionLevel ERROR: ${error}`);
}
```
14.  （可选）查询是否需要安全解码。
```typescript
try {
let status: boolean = mediaKeySession.requireSecureDecoderModule("video/avc");
} catch (err) {
let error = err as BusinessError;
console.error(`requireSecureDecoderModule ERROR: ${error}`);
}
```
15.  （可选）删除当前会话的媒体密钥。
```typescript
try {
mediaKeySession.clearMediaKeys();
} catch (err) {
let error = err as BusinessError;
console.error(`clearMediaKeys ERROR: ${error}`);
}
```
-  监听密钥请求事件，媒体密钥请求时触发。
```typescript
mediaKeySession.on('keyRequired', (eventInfo: drm.EventInfo) => {
console.log('keyRequired' + 'info:' + eventInfo.info + ' extraInfo:' + eventInfo.extraInfo);
});
```
-  监听媒体密钥过期事件，媒体密钥过期时触发。
```typescript
mediaKeySession.on('keyExpired', (eventInfo: drm.EventInfo) => {
console.log('keyExpired' + 'info:' + eventInfo.info + ' extraInfo:' + eventInfo.extraInfo);
});
```
-  监听DRM解决方案自定义事件，DRM解决方案自定义事件发生时触发。
```typescript
mediaKeySession.on('vendorDefined', (eventInfo: drm.EventInfo) => {
console.log('vendorDefined' + 'info:' + eventInfo.info + ' extraInfo:' + eventInfo.extraInfo);
});
```
-  监听媒体密钥过期更新事件，媒体密钥过期更新时触发。
```typescript
mediaKeySession.on('expirationUpdate', (eventInfo: drm.EventInfo) => {
console.log('expirationUpdate' + 'info:' + eventInfo.info + ' extraInfo:' + eventInfo.extraInfo);
});
```
-  监听密钥变换事件，媒体密钥变换时触发。
```typescript
mediaKeySession.on('keysChange', (keyInfo : drm.KeysInfo[], newKeyAvailable:boolean) => {
for(let i = 0; i < keyInfo.length; i++){
console.log('keysChange' + 'info:' + keyInfo[i].keyId + ' extraInfo:' + keyInfo[i].value);
}
});
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/drm-native-V14
爬取时间: 2025-04-28 20:22:51
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/native-drm-mediakeysystem-management-V14
爬取时间: 2025-04-28 20:23:05
来源: Huawei Developer
DRM系统管理（MediaKeySystem）支持MediaKeySystem实例管理、设备DRM证书申请与处理、会话实例管理、离线媒体密钥管理、获取DRM度量统计信息、设备属性等。在使用DRM Kit功能时，先查询设备是否支持对应DRM解决方案的DRM功能。在DRM Kit里DRM解决方案以插件形式存在，所以也叫DRM插件。
开发步骤
详细的API说明请参考DRM API参考。
1.  导入NDK相关接口，导入方法如下。
2.  在CMake脚本中链接Drm NDK动态库。
3.  查询设备是否支持对应DRM解决方案名称、媒体类型、安全保护级别的DRM解决方案。
4.（可选）获取设备支持的DRM解决方案的名称和唯一标识的列表。
1.  创建MediaKeySystem实例。
6.（可选）声明MediaKeySystem事件监听回调。
7.（可选）设置MediaKeySystem事件监听回调。
1.  创建MediaKeySession会话实例。
2.  检查设备DRM证书状态，设备DRM证书不存在或状态异常，则生成设备DRM证书请求，处理设备DRM证书响应。
3.  (可选)获取离线媒体密钥标识，获取离线媒体密钥状态、清理离线媒体密钥。
不同的DRM方案的配置属性信息可能存在差别，支持的属性名包含："vendor"、"version"、"description"、"algorithms"、"maxSessionNum"、"currentHDCPLevel"。需解决方案支持属性值设置能力才能设置DRM配置属性信息。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/native-drm-mediakeysession-management-V14
爬取时间: 2025-04-28 20:23:19
来源: Huawei Developer
DRM会话管理（MediaKeySession）支持媒体密钥管理及媒体解密等，MediaKeySession实例由系统管理里的MediaKeySystem实例创建和销毁。
开发步骤
详细的API说明请参考DRM API参考。
1.  导入NDK接口，接口中提供了DRM相关的属性和方法，导入方法如下。
2.  在CMake脚本中链接Drm NDK动态库。
3.  声明MediaKeySystem监听回调。
4.  设置MediaKeySystem监听回调。
5.  生成媒体密钥请求，媒体密钥请求响应处理。
6.  （可选）检查当前MediaKeySession会话的媒体密钥状态。
7.  （可选）清理当前会话下所有媒体密钥。
8.  （可选）生成离线媒体密钥释放请求和处理离线媒体密钥释放响应。
9.  （可选）恢复离线媒体密钥到当前会话。
10.  （可选）调用MediaKeySession类中的OH_MediaKeySession_GetContentProtectionLevel方法获取当前会话的内容保护级别。
11.  （可选）获取是否需要安全解码。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-kit-V14
爬取时间: 2025-04-28 20:23:33
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-overview-V14
爬取时间: 2025-04-28 20:23:46
来源: Huawei Developer
应用开发中的图片开发是对图片像素数据进行解析、处理、构造的过程，达到目标图片效果，主要涉及图片解码、图片处理、图片编码等。
在学习图片开发前，需要熟悉以下基本概念：
-  图片解码 指将所支持格式的存档图片解码成统一的PixelMap，以便在应用或系统中进行图片显示或图片处理。
-  PixelMap 指图片解码后无压缩的位图，用于图片显示或图片处理。
-  Picture 多图对象，包含主图与辅助图，用于多图对象的显示或处理。
-  图片处理 指对PixelMap进行相关的操作，如旋转、缩放、设置透明度、获取图片信息、读写像素数据等。
-  图片编码 指将PixelMap编码成不同格式的存档图片，用于后续处理，如保存、传输等。
图片开发的主要流程如下图所示。
图1图片开发流程示意图
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165949.26129555763259585661460825209799:50001231000000:2800:26C7B1808A38063FCDD2C0368AE1E89EE5EFE0AE08076286703FB6362FDCBD51.png)
1.  获取图片：通过应用沙箱等方式获取原始图片。
2.  创建ImageSource实例：ImageSource是图片解码出来的图片源类，用于获取或修改图片相关信息。
3.  图片解码：通过ImageSource解码生成PixelMap。
4.  图片处理：对PixelMap进行处理，更改图片属性实现图片的旋转、缩放、裁剪等效果。然后通过Image组件显示图片。
5.  图片编码：使用图片打包器类ImagePacker，将PixelMap或ImageSource进行压缩编码，生成一张新的图片。
除上述基本图片开发能力外，HarmonyOS还提供常用图片工具，供开发者选择使用。
亮点/特征
Image Kit编解码支持多种图片格式，并采用了高效的算法和优化策略，提高了图片处理的速度和效率。
约束与限制
-  读写权限限制： 在图片处理中，可能需要使用用户图片，应用需要向用户申请对应的读写操作权限才能保证功能的正常运行。
-  选择合适的C API接口： 图片框架当前提供了两套C API接口，分别为依赖于JS对象的C API和不依赖于JS对象的C API。 依赖于JS对象的C接口 这类接口可以完成图片编解码，图片接收器，处理图像数据等功能，相关示例代码可以参考图片开发指导(依赖JS对象)(C/C++)节点下的内容。开发者可查看Image模块下的C API，确认API范围。这部分API在API 11之前发布，在后续的版本不再增加新功能。 不依赖于JS对象的C接口 这类接口除了提供上述图片框架基础功能，还可以完成多图编解码等新特性，相关开发指导请参考图片开发指导(C/C++)节点下的内容。开发者可查看Image_NativeModule模块下的C API，确认API范围。这部分API从API 12开始支持，并将持续演进，推荐开发者使用。 两套C API不建议同时使用，在部分场景下存在不兼容的问题。
-  依赖于JS对象的C接口 这类接口可以完成图片编解码，图片接收器，处理图像数据等功能，相关示例代码可以参考图片开发指导(依赖JS对象)(C/C++)节点下的内容。开发者可查看Image模块下的C API，确认API范围。这部分API在API 11之前发布，在后续的版本不再增加新功能。
-  不依赖于JS对象的C接口 这类接口除了提供上述图片框架基础功能，还可以完成多图编解码等新特性，相关开发指导请参考图片开发指导(C/C++)节点下的内容。开发者可查看Image_NativeModule模块下的C API，确认API范围。这部分API从API 12开始支持，并将持续演进，推荐开发者使用。
-  依赖于JS对象的C接口 这类接口可以完成图片编解码，图片接收器，处理图像数据等功能，相关示例代码可以参考图片开发指导(依赖JS对象)(C/C++)节点下的内容。开发者可查看Image模块下的C API，确认API范围。这部分API在API 11之前发布，在后续的版本不再增加新功能。
-  不依赖于JS对象的C接口 这类接口除了提供上述图片框架基础功能，还可以完成多图编解码等新特性，相关开发指导请参考图片开发指导(C/C++)节点下的内容。开发者可查看Image_NativeModule模块下的C API，确认API范围。这部分API从API 12开始支持，并将持续演进，推荐开发者使用。
与相关Kit的关系
图片框架提供图片编解码能力，为Image组件及图库等应用提供支撑，其解码结果可以传给Image组件显示。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-arkts-dev-V14
爬取时间: 2025-04-28 20:24:00
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-decoding-V14
爬取时间: 2025-04-28 20:24:15
来源: Huawei Developer
图片解码指将所支持格式的存档图片解码成统一的PixelMap，以便在应用或系统中进行图片显示或图片处理。当前支持的存档图片格式包括JPEG、PNG、GIF、WebP、BMP、SVG、ICO、DNG、HEIF(不同硬件设备支持情况不同)。
开发步骤
图片解码相关API的详细介绍请参见：图片解码接口说明。
1.  全局导入Image模块。
```typescript
import { image } from '@kit.ImageKit';
```
2.  获取图片。 方法一：获取沙箱路径。具体请参考获取应用文件路径。应用沙箱的介绍及如何向应用沙箱推送文件，请参考文件管理。 方法二：通过沙箱路径获取图片的文件描述符。具体请参考file.fs API参考文档。 该方法需要先导入@kit.CoreFileKit模块。 然后调用fs.openSync()获取文件描述符。 方法三：通过资源管理器获取资源文件的ArrayBuffer。具体请参考ResourceManager API参考文档。 不同模型获取资源管理器的方式不同，获取资源管理器后，再调用resourceMgr.getRawFileContent()获取资源文件的ArrayBuffer。 方法四：通过资源管理器获取资源文件的RawFileDescriptor。具体请参考ResourceManager API参考文档。 不同模型获取资源管理器的方式不同，获取资源管理器后，再调用resourceMgr.getRawFd()获取资源文件的RawFileDescriptor。
```typescript
const context : Context = getContext(this);
const filePath : string = context.cacheDir + '/test.jpg';
```
3.  方法一：获取沙箱路径。具体请参考获取应用文件路径。应用沙箱的介绍及如何向应用沙箱推送文件，请参考文件管理。
```typescript
const context : Context = getContext(this);
const filePath : string = context.cacheDir + '/test.jpg';
```
4.  方法二：通过沙箱路径获取图片的文件描述符。具体请参考file.fs API参考文档。 该方法需要先导入@kit.CoreFileKit模块。 然后调用fs.openSync()获取文件描述符。
```typescript
import { fileIo as fs } from '@kit.CoreFileKit';
```
5.  方法三：通过资源管理器获取资源文件的ArrayBuffer。具体请参考ResourceManager API参考文档。 不同模型获取资源管理器的方式不同，获取资源管理器后，再调用resourceMgr.getRawFileContent()获取资源文件的ArrayBuffer。
```typescript
// 导入resourceManager资源管理器
import { resourceManager } from '@kit.LocalizationKit';
const context : Context = getContext(this);
// 获取resourceManager资源管理器
const resourceMgr : resourceManager.ResourceManager = context.resourceManager;
```
6.  方法四：通过资源管理器获取资源文件的RawFileDescriptor。具体请参考ResourceManager API参考文档。 不同模型获取资源管理器的方式不同，获取资源管理器后，再调用resourceMgr.getRawFd()获取资源文件的RawFileDescriptor。
```typescript
// 导入resourceManager资源管理器
import { resourceManager } from '@kit.LocalizationKit';
const context : Context = getContext(this);
// 获取resourceManager资源管理器
const resourceMgr : resourceManager.ResourceManager = context.resourceManager;
```
7.  创建ImageSource实例。 方法一：通过沙箱路径创建ImageSource。沙箱路径可以通过步骤2的方法一获取。 方法二：通过文件描述符fd创建ImageSource。文件描述符可以通过步骤2的方法二获取。 方法三：通过缓冲区数组创建ImageSource。缓冲区数组可以通过步骤2的方法三获取。 方法四：通过资源文件的RawFileDescriptor创建ImageSource。RawFileDescriptor可以通过步骤2的方法四获取。
```typescript
// path为已获得的沙箱路径
const imageSource : image.ImageSource = image.createImageSource(filePath);
```
8.  方法一：通过沙箱路径创建ImageSource。沙箱路径可以通过步骤2的方法一获取。
```typescript
// path为已获得的沙箱路径
const imageSource : image.ImageSource = image.createImageSource(filePath);
```
9.  方法二：通过文件描述符fd创建ImageSource。文件描述符可以通过步骤2的方法二获取。
```typescript
// fd为已获得的文件描述符
const imageSource : image.ImageSource = image.createImageSource(fd);
```
10.  方法三：通过缓冲区数组创建ImageSource。缓冲区数组可以通过步骤2的方法三获取。
```typescript
const imageSource : image.ImageSource = image.createImageSource(buffer);
```
11.  方法四：通过资源文件的RawFileDescriptor创建ImageSource。RawFileDescriptor可以通过步骤2的方法四获取。
```typescript
const imageSource : image.ImageSource = image.createImageSource(rawFileDescriptor);
```
12.  设置解码参数DecodingOptions，解码获取pixelMap图片对象。 解码完成，获取到pixelMap对象后，可以进行后续图片处理。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
import { image } from '@kit.ImageKit';
let img = await getContext(this).resourceManager.getMediaContent($r('app.media.image'));
let imageSource:image.ImageSource = image.createImageSource(img.buffer.slice(0));
let decodingOptions : image.DecodingOptions = {
editable: true,
desiredPixelFormat: 3,
}
// 创建pixelMap
imageSource.createPixelMap(decodingOptions).then((pixelMap : image.PixelMap) => {
console.log("Succeeded in creating PixelMap")
}).catch((err : BusinessError) => {
console.error("Failed to create PixelMap")
});
```
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
import { image } from '@kit.ImageKit';
let img = await getContext(this).resourceManager.getMediaContent($r('app.media.image'));
let imageSource:image.ImageSource = image.createImageSource(img.buffer.slice(0));
let decodingOptions : image.DecodingOptions = {
editable: true,
desiredPixelFormat: 3,
}
// 创建pixelMap
imageSource.createPixelMap(decodingOptions).then((pixelMap : image.PixelMap) => {
console.log("Succeeded in creating PixelMap")
}).catch((err : BusinessError) => {
console.error("Failed to create PixelMap")
});
```
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
import { image } from '@kit.ImageKit';
let img = await getContext(this).resourceManager.getMediaContent($r('app.media.CUVAHdr'));
let imageSource:image.ImageSource = image.createImageSource(img.buffer.slice(0));
let decodingOptions : image.DecodingOptions = {
//设置为AUTO会根据图片资源格式解码，如果图片资源为HDR资源则会解码为HDR的pixelmap。
desiredDynamicRange: image.DecodingDynamicRange.AUTO,
}
// 创建pixelMap
imageSource.createPixelMap(decodingOptions).then((pixelMap : image.PixelMap) => {
console.log("Succeeded in creating PixelMap")
// 判断pixelmap是否为hdr内容
let info = pixelMap.getImageInfoSync();
console.log("pixelmap isHdr:" + info.isHdr);
}).catch((err : BusinessError) => {
console.error("Failed to create PixelMap")
});
```
13.  释放pixelMap和imageSource。 需确认pixelMap和imageSource异步方法已经执行完成，不再使用该变量后可按需手动调用下面方法释放。
```typescript
pixelMap.release();
imageSource.release();
```
-  方法一：获取沙箱路径。具体请参考获取应用文件路径。应用沙箱的介绍及如何向应用沙箱推送文件，请参考文件管理。
```typescript
const context : Context = getContext(this);
const filePath : string = context.cacheDir + '/test.jpg';
```
-  方法二：通过沙箱路径获取图片的文件描述符。具体请参考file.fs API参考文档。 该方法需要先导入@kit.CoreFileKit模块。 然后调用fs.openSync()获取文件描述符。
```typescript
import { fileIo as fs } from '@kit.CoreFileKit';
```
-  方法三：通过资源管理器获取资源文件的ArrayBuffer。具体请参考ResourceManager API参考文档。 不同模型获取资源管理器的方式不同，获取资源管理器后，再调用resourceMgr.getRawFileContent()获取资源文件的ArrayBuffer。
```typescript
// 导入resourceManager资源管理器
import { resourceManager } from '@kit.LocalizationKit';
const context : Context = getContext(this);
// 获取resourceManager资源管理器
const resourceMgr : resourceManager.ResourceManager = context.resourceManager;
```
-  方法四：通过资源管理器获取资源文件的RawFileDescriptor。具体请参考ResourceManager API参考文档。 不同模型获取资源管理器的方式不同，获取资源管理器后，再调用resourceMgr.getRawFd()获取资源文件的RawFileDescriptor。
```typescript
// 导入resourceManager资源管理器
import { resourceManager } from '@kit.LocalizationKit';
const context : Context = getContext(this);
// 获取resourceManager资源管理器
const resourceMgr : resourceManager.ResourceManager = context.resourceManager;
```
-  方法一：通过沙箱路径创建ImageSource。沙箱路径可以通过步骤2的方法一获取。
```typescript
// path为已获得的沙箱路径
const imageSource : image.ImageSource = image.createImageSource(filePath);
```
-  方法二：通过文件描述符fd创建ImageSource。文件描述符可以通过步骤2的方法二获取。
```typescript
// fd为已获得的文件描述符
const imageSource : image.ImageSource = image.createImageSource(fd);
```
-  方法三：通过缓冲区数组创建ImageSource。缓冲区数组可以通过步骤2的方法三获取。
```typescript
const imageSource : image.ImageSource = image.createImageSource(buffer);
```
-  方法四：通过资源文件的RawFileDescriptor创建ImageSource。RawFileDescriptor可以通过步骤2的方法四获取。
```typescript
const imageSource : image.ImageSource = image.createImageSource(rawFileDescriptor);
```
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
import { image } from '@kit.ImageKit';
let img = await getContext(this).resourceManager.getMediaContent($r('app.media.image'));
let imageSource:image.ImageSource = image.createImageSource(img.buffer.slice(0));
let decodingOptions : image.DecodingOptions = {
editable: true,
desiredPixelFormat: 3,
}
// 创建pixelMap
imageSource.createPixelMap(decodingOptions).then((pixelMap : image.PixelMap) => {
console.log("Succeeded in creating PixelMap")
}).catch((err : BusinessError) => {
console.error("Failed to create PixelMap")
});
```
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
import { image } from '@kit.ImageKit';
let img = await getContext(this).resourceManager.getMediaContent($r('app.media.CUVAHdr'));
let imageSource:image.ImageSource = image.createImageSource(img.buffer.slice(0));
let decodingOptions : image.DecodingOptions = {
//设置为AUTO会根据图片资源格式解码，如果图片资源为HDR资源则会解码为HDR的pixelmap。
desiredDynamicRange: image.DecodingDynamicRange.AUTO,
}
// 创建pixelMap
imageSource.createPixelMap(decodingOptions).then((pixelMap : image.PixelMap) => {
console.log("Succeeded in creating PixelMap")
// 判断pixelmap是否为hdr内容
let info = pixelMap.getImageInfoSync();
console.log("pixelmap isHdr:" + info.isHdr);
}).catch((err : BusinessError) => {
console.error("Failed to create PixelMap")
});
```
开发示例-对资源文件中的图片进行解码
1.  获取resourceManager资源管理。
```typescript
// 导入resourceManager资源管理器
import { resourceManager } from '@kit.LocalizationKit';
const context : Context = getContext(this);
// 获取resourceManager资源管理
const resourceMgr : resourceManager.ResourceManager = context.resourceManager;
```
2.  创建ImageSource。 方式一：通过rawfile文件夹下test.jpg的ArrayBuffer创建。 方式二：通过rawfile文件夹下test.jpg的RawFileDescriptor创建。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
resourceMgr.getRawFileContent('test.jpg').then((fileData : Uint8Array) => {
console.log("Succeeded in getting RawFileContent")
// 获取图片的ArrayBuffer
const buffer = fileData.buffer.slice(0);
const imageSource : image.ImageSource = image.createImageSource(buffer);
}).catch((err : BusinessError) => {
console.error("Failed to get RawFileContent")
});
```
3.  方式一：通过rawfile文件夹下test.jpg的ArrayBuffer创建。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
resourceMgr.getRawFileContent('test.jpg').then((fileData : Uint8Array) => {
console.log("Succeeded in getting RawFileContent")
// 获取图片的ArrayBuffer
const buffer = fileData.buffer.slice(0);
const imageSource : image.ImageSource = image.createImageSource(buffer);
}).catch((err : BusinessError) => {
console.error("Failed to get RawFileContent")
});
```
4.  方式二：通过rawfile文件夹下test.jpg的RawFileDescriptor创建。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
resourceMgr.getRawFd('test.jpg').then((rawFileDescriptor : resourceManager.RawFileDescriptor) => {
console.log("Succeeded in getting RawFd")
const imageSource : image.ImageSource = image.createImageSource(rawFileDescriptor);
}).catch((err : BusinessError) => {
console.error("Failed to get RawFd")
});
```
5.  创建pixelMap。
```typescript
imageSource.createPixelMap().then((pixelMap: image.PixelMap) => {
console.log("Succeeded in creating PixelMap")
}).catch((err : BusinessError) => {
console.error("Failed to creating PixelMap")
});
```
6.  释放pixelMap和imageSource。 需确认pixelMap和imageSource异步方法已经执行完成，不再使用该变量后可按需手动调用下面方法释放。
```typescript
pixelMap.release();
imageSource.release();
```
-  方式一：通过rawfile文件夹下test.jpg的ArrayBuffer创建。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
resourceMgr.getRawFileContent('test.jpg').then((fileData : Uint8Array) => {
console.log("Succeeded in getting RawFileContent")
// 获取图片的ArrayBuffer
const buffer = fileData.buffer.slice(0);
const imageSource : image.ImageSource = image.createImageSource(buffer);
}).catch((err : BusinessError) => {
console.error("Failed to get RawFileContent")
});
```
-  方式二：通过rawfile文件夹下test.jpg的RawFileDescriptor创建。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
resourceMgr.getRawFd('test.jpg').then((rawFileDescriptor : resourceManager.RawFileDescriptor) => {
console.log("Succeeded in getting RawFd")
const imageSource : image.ImageSource = image.createImageSource(rawFileDescriptor);
}).catch((err : BusinessError) => {
console.error("Failed to get RawFd")
});
```
示例代码

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-picture-decoding-V14
爬取时间: 2025-04-28 20:24:29
来源: Huawei Developer
图片解码指将所支持格式的存档图片解码成统一的Picture。当前支持的存档图片格式包括JPEG、HEIF。
开发步骤
图片解码相关API的详细介绍请参见：图片解码接口说明。
1.  全局导入Image模块。
```typescript
import { image } from '@kit.ImageKit';
```
2.  获取图片。 方法一：获取沙箱路径。具体请参考获取应用文件路径。应用沙箱的介绍及如何向应用沙箱推送文件，请参考文件管理。 方法二：通过沙箱路径获取图片的文件描述符。具体请参考file.fs API参考文档。 该方法需要先导入@kit.CoreFileKit模块。 然后调用fileIo.openSync()获取文件描述符。 方法三：通过资源管理器获取资源文件的ArrayBuffer。具体请参考ResourceManager API参考文档。 不同模型获取资源管理器的方式不同，获取资源管理器后，再调用resourceMgr.getRawFileContent()获取资源文件的ArrayBuffer。 方法四：通过资源管理器获取资源文件的RawFileDescriptor。具体请参考ResourceManager API参考文档。 不同模型获取资源管理器的方式不同，获取资源管理器后，再调用resourceMgr.getRawFd()获取资源文件的RawFileDescriptor。
```typescript
const context : Context = getContext(this);
const filePath : string = context.cacheDir + '/test.jpg';
```
3.  方法一：获取沙箱路径。具体请参考获取应用文件路径。应用沙箱的介绍及如何向应用沙箱推送文件，请参考文件管理。
```typescript
const context : Context = getContext(this);
const filePath : string = context.cacheDir + '/test.jpg';
```
4.  方法二：通过沙箱路径获取图片的文件描述符。具体请参考file.fs API参考文档。 该方法需要先导入@kit.CoreFileKit模块。 然后调用fileIo.openSync()获取文件描述符。
```typescript
import { fileIo } from '@kit.CoreFileKit';
```
5.  方法三：通过资源管理器获取资源文件的ArrayBuffer。具体请参考ResourceManager API参考文档。 不同模型获取资源管理器的方式不同，获取资源管理器后，再调用resourceMgr.getRawFileContent()获取资源文件的ArrayBuffer。
```typescript
const context : Context = getContext(this);
// 获取resourceManager资源管理器
const resourceMgr : resourceManager.ResourceManager = context.resourceManager;
```
6.  方法四：通过资源管理器获取资源文件的RawFileDescriptor。具体请参考ResourceManager API参考文档。 不同模型获取资源管理器的方式不同，获取资源管理器后，再调用resourceMgr.getRawFd()获取资源文件的RawFileDescriptor。
```typescript
const context : Context = getContext(this);
// 获取resourceManager资源管理器
const resourceMgr : resourceManager.ResourceManager = context.resourceManager;
```
7.  创建ImageSource实例。 方法一：通过沙箱路径创建ImageSource。沙箱路径可以通过步骤2的方法一获取。 方法二：通过文件描述符fd创建ImageSource。文件描述符可以通过步骤2的方法二获取。 方法三：通过缓冲区数组创建ImageSource。缓冲区数组可以通过步骤2的方法三获取。 方法四：通过资源文件的RawFileDescriptor创建ImageSource。RawFileDescriptor可以通过步骤2的方法四获取。
```typescript
// path为已获得的沙箱路径
const imageSource : image.ImageSource = image.createImageSource(filePath);
```
8.  方法一：通过沙箱路径创建ImageSource。沙箱路径可以通过步骤2的方法一获取。
```typescript
// path为已获得的沙箱路径
const imageSource : image.ImageSource = image.createImageSource(filePath);
```
9.  方法二：通过文件描述符fd创建ImageSource。文件描述符可以通过步骤2的方法二获取。
```typescript
// fd为已获得的文件描述符
const imageSource : image.ImageSource = image.createImageSource(fd);
```
10.  方法三：通过缓冲区数组创建ImageSource。缓冲区数组可以通过步骤2的方法三获取。
```typescript
const imageSource : image.ImageSource = image.createImageSource(buffer);
```
11.  方法四：通过资源文件的RawFileDescriptor创建ImageSource。RawFileDescriptor可以通过步骤2的方法四获取。
```typescript
const imageSource : image.ImageSource = image.createImageSource(rawFileDescriptor);
```
12.  设置解码参数DecodingOptions，解码获取picture多图对象。 设置期望的format进行解码：
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
import image from '@kit.ImageKit';
let img = await getContext(this).resourceManager.getMediaContent($r('app.media.picture'));
let imageSource:image.ImageSource = image.createImageSource(img.buffer.slice(0));
let options: image.DecodingOptionsForPicture = {
desiredAuxiliaryPictures: [image.AuxiliaryPictureType.GAINMAP] // GAINMAP为需要解码的辅助图类型
};
// 创建picture
imageSource.createPicture(options).then((picture: image.Picture) => {
console.log("Create picture succeeded.")
}).catch((err: BusinessError) => {
console.error("Create picture failed.")
});
```
13.  对picture进行操作，如获取辅助图等。对于picture和辅助图的操作具体请参考Image API参考文档。
```typescript
// 获取辅助图对象
let type: image.AuxiliaryPictureType = image.AuxiliaryPictureType.GAINMAP;
let auxPicture: image.AuxiliaryPicture | null = picture.getAuxiliaryPicture(type);
// 获取辅助图信息
let auxinfo: image.AuxiliaryPictureInfo = auxPicture.getAuxiliaryPictureInfo();
console.info('GetAuxiliaryPictureInfo Type: ' + auxinfo.auxiliaryPictureType +
' height: ' + auxinfo.size.height + ' width: ' + auxinfo.size.width +
' rowStride: ' +  auxinfo.rowStride +  ' pixelFormat: ' + auxinfo.pixelFormat +
' colorSpace: ' +  auxinfo.colorSpace);
// 将辅助图数据读到ArrayBuffer
auxPicture.readPixelsToBuffer().then((pixelsBuffer: ArrayBuffer) => {
console.info('Read pixels to buffer success.');
}).catch((error: BusinessError) => {
console.error('Read pixels to buffer failed error.code: ' + JSON.stringify(error.code) + ' ,error.message:' + JSON.stringify(error.message));
});
auxPicture.release();
```
14.  释放picture。
```typescript
picture.release();
```
-  方法一：获取沙箱路径。具体请参考获取应用文件路径。应用沙箱的介绍及如何向应用沙箱推送文件，请参考文件管理。
```typescript
const context : Context = getContext(this);
const filePath : string = context.cacheDir + '/test.jpg';
```
-  方法二：通过沙箱路径获取图片的文件描述符。具体请参考file.fs API参考文档。 该方法需要先导入@kit.CoreFileKit模块。 然后调用fileIo.openSync()获取文件描述符。
```typescript
import { fileIo } from '@kit.CoreFileKit';
```
-  方法三：通过资源管理器获取资源文件的ArrayBuffer。具体请参考ResourceManager API参考文档。 不同模型获取资源管理器的方式不同，获取资源管理器后，再调用resourceMgr.getRawFileContent()获取资源文件的ArrayBuffer。
```typescript
const context : Context = getContext(this);
// 获取resourceManager资源管理器
const resourceMgr : resourceManager.ResourceManager = context.resourceManager;
```
-  方法四：通过资源管理器获取资源文件的RawFileDescriptor。具体请参考ResourceManager API参考文档。 不同模型获取资源管理器的方式不同，获取资源管理器后，再调用resourceMgr.getRawFd()获取资源文件的RawFileDescriptor。
```typescript
const context : Context = getContext(this);
// 获取resourceManager资源管理器
const resourceMgr : resourceManager.ResourceManager = context.resourceManager;
```
-  方法一：通过沙箱路径创建ImageSource。沙箱路径可以通过步骤2的方法一获取。
```typescript
// path为已获得的沙箱路径
const imageSource : image.ImageSource = image.createImageSource(filePath);
```
-  方法二：通过文件描述符fd创建ImageSource。文件描述符可以通过步骤2的方法二获取。
```typescript
// fd为已获得的文件描述符
const imageSource : image.ImageSource = image.createImageSource(fd);
```
-  方法三：通过缓冲区数组创建ImageSource。缓冲区数组可以通过步骤2的方法三获取。
```typescript
const imageSource : image.ImageSource = image.createImageSource(buffer);
```
-  方法四：通过资源文件的RawFileDescriptor创建ImageSource。RawFileDescriptor可以通过步骤2的方法四获取。
```typescript
const imageSource : image.ImageSource = image.createImageSource(rawFileDescriptor);
```
开发示例-解码资源文件中的图片
1.  获取resourceManager资源管理。
```typescript
const context : Context = getContext(this);
// 获取resourceManager资源管理
const resourceMgr : resourceManager.ResourceManager = context.resourceManager;
```
2.  创建ImageSource。 通过rawfile文件夹下test.jpg的ArrayBuffer创建。 通过rawfile文件夹下test.jpg的RawFileDescriptor创建。
```typescript
resourceMgr.getRawFileContent('test.jpg').then((fileData : Uint8Array) => {
console.log("Succeeded in getting RawFileContent")
// 获取图片的ArrayBuffer
const buffer = fileData.buffer.slice(0);
const imageSource : image.ImageSource = image.createImageSource(buffer);
}).catch((err : BusinessError) => {
console.error("Failed to get RawFileContent")
});
```
3.  通过rawfile文件夹下test.jpg的ArrayBuffer创建。
```typescript
resourceMgr.getRawFileContent('test.jpg').then((fileData : Uint8Array) => {
console.log("Succeeded in getting RawFileContent")
// 获取图片的ArrayBuffer
const buffer = fileData.buffer.slice(0);
const imageSource : image.ImageSource = image.createImageSource(buffer);
}).catch((err : BusinessError) => {
console.error("Failed to get RawFileContent")
});
```
4.  通过rawfile文件夹下test.jpg的RawFileDescriptor创建。
```typescript
resourceMgr.getRawFd('test.jpg').then((rawFileDescriptor : resourceManager.RawFileDescriptor) => {
console.log("Succeeded in getting RawFd")
const imageSource : image.ImageSource = image.createImageSource(rawFileDescriptor);
}).catch((err : BusinessError) => {
console.error("Failed to get RawFd")
});
```
5.  创建picture。
```typescript
let options: image.DecodingOptionsForPicture = {
desiredAuxiliaryPictures: [image.AuxiliaryPictureType.GAINMAP] // GAINMAP为需要解码的辅助图类型
};
imageSource.createPicture(options).then((picture: image.Picture) => {
console.log("Create picture succeeded.")
}).catch((err : BusinessError) => {
console.error("Create picture failed.")
});
```
6.  对picture进行操作，如获取辅助图等。对于picture和辅助图的操作具体请参考Image API参考文档。
```typescript
// 获取辅助图对象
let type: image.AuxiliaryPictureType = image.AuxiliaryPictureType.GAINMAP;
let auxPicture: image.AuxiliaryPicture | null = picture.getAuxiliaryPicture(type);
// 获取辅助图信息
let auxinfo: image.AuxiliaryPictureInfo = auxPicture.getAuxiliaryPictureInfo();
console.info('GetAuxiliaryPictureInfo Type: ' + auxinfo.auxiliaryPictureType +
' height: ' + auxinfo.size.height + ' width: ' + auxinfo.size.width +
' rowStride: ' +  auxinfo.rowStride +  ' pixelFormat: ' + auxinfo.pixelFormat +
' colorSpace: ' +  auxinfo.colorSpace);
// 将辅助图数据写入ArrayBuffer
auxPicture.readPixelsToBuffer().then((pixelsBuffer: ArrayBuffer) => {
console.info('Read pixels to buffer success.');
}).catch((error: BusinessError) => {
console.error('Read pixels to buffer failed error.code: ' + JSON.stringify(error.code) + ' ,error.message:' + JSON.stringify(error.message));
});
auxPicture.release();
```
7.  释放picture。
```typescript
picture.release();
```
-  通过rawfile文件夹下test.jpg的ArrayBuffer创建。
```typescript
resourceMgr.getRawFileContent('test.jpg').then((fileData : Uint8Array) => {
console.log("Succeeded in getting RawFileContent")
// 获取图片的ArrayBuffer
const buffer = fileData.buffer.slice(0);
const imageSource : image.ImageSource = image.createImageSource(buffer);
}).catch((err : BusinessError) => {
console.error("Failed to get RawFileContent")
});
```
-  通过rawfile文件夹下test.jpg的RawFileDescriptor创建。
```typescript
resourceMgr.getRawFd('test.jpg').then((rawFileDescriptor : resourceManager.RawFileDescriptor) => {
console.log("Succeeded in getting RawFd")
const imageSource : image.ImageSource = image.createImageSource(rawFileDescriptor);
}).catch((err : BusinessError) => {
console.error("Failed to get RawFd")
});
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-transformation-V14
爬取时间: 2025-04-28 20:24:43
来源: Huawei Developer
图片处理指对PixelMap进行相关的操作，如获取图片信息、裁剪、缩放、偏移、旋转、翻转、设置透明度、读写像素数据等。图片处理主要包括图像变换、位图操作，本文介绍图像变换。
开发步骤
图像变换相关API的详细介绍请参见API参考。
1.  完成图片解码，获取PixelMap对象。
2.  获取图片信息。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
// 获取图片大小
pixelMap.getImageInfo().then( (info : image.ImageInfo) => {
console.info('info.width = ' + info.size.width);
console.info('info.height = ' + info.size.height);
}).catch((err : BusinessError) => {
console.error("Failed to obtain the image pixel map information.And the error is: " + err);
});
```
3.  进行图像变换操作。 原图： 裁剪 缩放 偏移 旋转 翻转 透明度
```typescript
// x：裁剪起始点横坐标0
// y：裁剪起始点纵坐标0
// height：裁剪高度400，方向为从上往下（裁剪后的图片高度为400）
// width：裁剪宽度400，方向为从左到右（裁剪后的图片宽度为400）
pixelMap.crop({x: 0, y: 0, size: { height: 400, width: 400 } });
```
4.  裁剪
```typescript
// x：裁剪起始点横坐标0
// y：裁剪起始点纵坐标0
// height：裁剪高度400，方向为从上往下（裁剪后的图片高度为400）
// width：裁剪宽度400，方向为从左到右（裁剪后的图片宽度为400）
pixelMap.crop({x: 0, y: 0, size: { height: 400, width: 400 } });
```
5.  缩放
```typescript
// 宽为原来的0.5
// 高为原来的0.5
pixelMap.scale(0.5, 0.5);
```
6.  偏移
```typescript
// 向下偏移100
// 向右偏移100
pixelMap.translate(100, 100);
```
7.  旋转
```typescript
// 顺时针旋转90°
pixelMap.rotate(90);
```
8.  翻转
```typescript
// 垂直翻转
pixelMap.flip(false, true);
```
9.  透明度
```typescript
// 透明度0.5
pixelMap.opacity(0.5);
```
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165949.58846549164009527449404599011215:50001231000000:2800:71388A49BE8731943B1654AF7F402CF8D1C17E824C32D9F1E8C0D49BEDEB079A.jpeg)
-  裁剪
```typescript
// x：裁剪起始点横坐标0
// y：裁剪起始点纵坐标0
// height：裁剪高度400，方向为从上往下（裁剪后的图片高度为400）
// width：裁剪宽度400，方向为从左到右（裁剪后的图片宽度为400）
pixelMap.crop({x: 0, y: 0, size: { height: 400, width: 400 } });
```
-  缩放
```typescript
// 宽为原来的0.5
// 高为原来的0.5
pixelMap.scale(0.5, 0.5);
```
-  偏移
```typescript
// 向下偏移100
// 向右偏移100
pixelMap.translate(100, 100);
```
-  旋转
```typescript
// 顺时针旋转90°
pixelMap.rotate(90);
```
-  翻转
```typescript
// 垂直翻转
pixelMap.flip(false, true);
```
-  透明度
```typescript
// 透明度0.5
pixelMap.opacity(0.5);
```
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165950.97561317457200707190504960482320:50001231000000:2800:C6A222343CA9E3155102F829FA83AB704081F47A89A18771F2A1C15EA2BCA79F.jpeg)
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165950.60830960665429347774748812815925:50001231000000:2800:D50BCB1C3650FE8B463467B0E2BFAA57549FB07ABC8833FF411501509FFF05EB.jpeg)
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165950.34172087979099866343179507850968:50001231000000:2800:562229F4EBABFE89BD6678BC3F9A66A55A4C56E61AC9C8255E4B1E3D58F8C18E.jpeg)
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165950.10740999930833601461518973017379:50001231000000:2800:E95DFA83C464EC784D899F2079923C3702EFBC81C96A3068A6A00A7E13C78513.jpeg)
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165950.29157278262507212913792637580487:50001231000000:2800:F71545918D39998BBD5AB065561383758D98C4518EDE0D5F3A520C312130C5CB.jpeg)
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165950.78109938330983259524004465470743:50001231000000:2800:089B2F9B70F732330D10D2F61644D5CF8DAFD19069BB24F6981ADAB848EDDAC3.jpeg)
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165950.74116385519740602597306658862504:50001231000000:2800:9746B7A004A426ABD22750578B9F02F508A63C658197DDCD12E2D5C53EFF7C4E.png)
示例代码

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-pixelmap-operation-V14
爬取时间: 2025-04-28 20:24:56
来源: Huawei Developer
当需要对目标图片中的部分区域进行处理时，可以使用位图操作功能。此功能常用于图片美化等操作。
如下图所示，一张图片中，将指定的矩形区域像素数据读取出来，进行修改后，再写回原图片对应区域。
图1位图操作示意图
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165950.57499496773192621957851190977585:50001231000000:2800:1102BA53A47390012F7B70B102E3E913158AC1F30DE1454FE0C6FCABA9560B45.png)
开发步骤
位图操作相关API的详细介绍请参见API参考。
1.  完成图片解码，获取PixelMap位图对象。
2.  从PixelMap位图对象中获取信息。
```typescript
import { image } from '@kit.ImageKit';
// 获取图像像素的总字节数
let pixelBytesNumber : number = pixelMap.getPixelBytesNumber();
// 获取图像像素每行字节数
let rowBytes : number = pixelMap.getBytesNumberPerRow();
// 获取当前图像像素密度。像素密度是指每英寸图片所拥有的像素数量。像素密度越大，图片越精细。
let density : number = pixelMap.getDensity();
```
3.  读取并修改目标区域像素数据，写回原图。 建议readPixelsToBuffer和writeBufferToPixels成对使用，readPixels和writePixels成对使用，避免因图像像素格式不一致，造成PixelMap图像出现异常。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
// 场景一：读取并修改整张图片数据
// 按照PixelMap的像素格式，读取PixelMap的图像像素数据，并写入缓冲区中。
const buffer = new ArrayBuffer(pixelBytesNumber);
pixelMap.readPixelsToBuffer(buffer).then(() => {
console.info('Succeeded in reading image pixel data.');
}).catch((error : BusinessError) => {
console.error('Failed to read image pixel data. The error is: ' + error);
})
// 按照PixelMap的像素格式，读取缓冲区中的图像像素数据，并写入PixelMap。
pixelMap.writeBufferToPixels(buffer).then(() => {
console.info('Succeeded in writing image pixel data.');
}).catch((error : BusinessError) => {
console.error('Failed to write image pixel data. The error is: ' + error);
})
// 场景二：读取并修改指定区域内的图片数据
// 固定按照BGRA_8888格式，读取PixelMap指定区域内的图像像素数据，并写入PositionArea.pixels缓冲区中，该区域由PositionArea.region指定。
const area : image.PositionArea = {
pixels: new ArrayBuffer(8),
offset: 0,
stride: 8,
region: { size: { height: 1, width: 2 }, x: 0, y: 0 }
}
pixelMap.readPixels(area).then(() => {
console.info('Succeeded in reading the image data in the area.');
}).catch((error : BusinessError) => {
console.error('Failed to read the image data in the area. The error is: ' + error);
})
// 固定按照BGRA_8888格式，读取PositionArea.pixels缓冲区中的图像像素数据，并写入PixelMap指定区域内，该区域由PositionArea.region指定。
pixelMap.writePixels(area).then(() => {
console.info('Succeeded in writing the image data in the area.');
}).catch((error : BusinessError) => {
console.error('Failed to write the image data in the area. The error is: ' + error);
})
```
开发示例-复制（深拷贝）新的PixelMap
1.  完成图片解码，获取PixelMap位图对象。
2.  复制（深拷贝）一个新的PixelMap。 创建新PixelMap时，必须将srcPixelFormat指定为原PixelMap的像素格式，否则新PixelMap的图像会出现异常。
```typescript
/**
*  复制（深拷贝）一个新的PixelMap
*
* @param pixelMap - 被复制的PixelMap。
* @param desiredPixelFormat - 新PixelMap的像素格式。如果不指定，则仍使用原PixelMap的像素格式。
* @returns 新PixelMap。
**/
clonePixelMap(pixelMap: PixelMap, desiredPixelFormat?: image.PixelMapFormat): PixelMap {
// 获取当前PixelMap的图片信息。
const imageInfo = pixelMap.getImageInfoSync();
// 读取当前PixelMap的图像像素数据，并按照当前PixelMap的像素格式写入缓冲区数组。
const buffer = new ArrayBuffer(pixelMap.getPixelBytesNumber());
pixelMap.readPixelsToBufferSync(buffer);
// 根据当前PixelMap的图片信息，生成初始化选项。
const options: image.InitializationOptions = {
// 当前PixelMap的像素格式。
srcPixelFormat: imageInfo.pixelFormat,
// 新PixelMap的像素格式。
pixelFormat: desiredPixelFormat ?? imageInfo.pixelFormat,
// 当前PixelMap的尺寸大小。
size: imageInfo.size
};
// 根据初始化选项和缓冲区数组，生成新PixelMap。
return image.createPixelMapSync(buffer, options);
}
```
示例代码

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-encoding-V14
爬取时间: 2025-04-28 20:25:10
来源: Huawei Developer
图片编码指将PixelMap编码成不同格式的存档图片，当前支持打包为JPEG、WebP、png和 HEIF(不同硬件设备支持情况不同) 格式，用于后续处理，如保存、传输等。
开发步骤
图片编码相关API的详细介绍请参见：图片编码接口说明。
图片编码进文件流
1.  创建图像编码ImagePacker对象。
```typescript
// 导入相关模块包
import { image } from '@kit.ImageKit';
const imagePackerApi = image.createImagePacker();
```
2.  设置编码输出流和编码参数。 format为图像的编码格式；quality为图像质量，范围从0-100，100为最佳质量。 根据MIME标准，标准编码格式为image/jpeg。当使用image编码时，PackingOption.format设置为image/jpeg，image编码后的文件扩展名可设为.jpg或.jpeg，可在支持image/jpeg解码的平台上使用。 编码为hdr内容(需要资源本身为hdr，支持jpeg格式)。
```typescript
let packOpts : image.PackingOption = { format:"image/jpeg", quality:98 };
```
3.  format为图像的编码格式；quality为图像质量，范围从0-100，100为最佳质量。 根据MIME标准，标准编码格式为image/jpeg。当使用image编码时，PackingOption.format设置为image/jpeg，image编码后的文件扩展名可设为.jpg或.jpeg，可在支持image/jpeg解码的平台上使用。
```typescript
let packOpts : image.PackingOption = { format:"image/jpeg", quality:98 };
```
4.  编码为hdr内容(需要资源本身为hdr，支持jpeg格式)。
```typescript
packOpts.desiredDynamicRange = image.PackingDynamicRange.AUTO;
```
5.  创建PixelMap对象或创建ImageSource对象。
6.  进行图片编码，并保存编码后的图片。 方法一：通过PixelMap进行编码。 方法二：通过imageSource进行编码。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
imagePackerApi.packing(pixelMap, packOpts).then( (data : ArrayBuffer) => {
// data 为打包获取到的文件流，写入文件保存即可得到一张图片
}).catch((error : BusinessError) => {
console.error('Failed to pack the image. And the error is: ' + error);
})
```
-  format为图像的编码格式；quality为图像质量，范围从0-100，100为最佳质量。 根据MIME标准，标准编码格式为image/jpeg。当使用image编码时，PackingOption.format设置为image/jpeg，image编码后的文件扩展名可设为.jpg或.jpeg，可在支持image/jpeg解码的平台上使用。
```typescript
let packOpts : image.PackingOption = { format:"image/jpeg", quality:98 };
```
-  编码为hdr内容(需要资源本身为hdr，支持jpeg格式)。
```typescript
packOpts.desiredDynamicRange = image.PackingDynamicRange.AUTO;
```
图片编码进文件
在编码时，开发者可以传入对应的文件路径，编码后的内存数据将直接写入文件。
方法一：通过PixelMap编码进文件。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
import { fileIo as fs } from '@kit.CoreFileKit';
const context : Context = getContext(this);
const path : string = context.cacheDir + "/pixel_map.jpg";
let file = fs.openSync(path, fs.OpenMode.CREATE | fs.OpenMode.READ_WRITE);
imagePackerApi.packToFile(pixelMap, file.fd, packOpts).then(() => {
// 直接打包进文件
}).catch((error : BusinessError) => {
console.error('Failed to pack the image. And the error is: ' + error);
}).finally(()=>{
fs.closeSync(file.fd);
})
```
方法二：通过imageSource编码进文件。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
import { fileIo as fs } from '@kit.CoreFileKit';
const context : Context = getContext(this);
const filePath : string = context.cacheDir + "/image_source.jpg";
let file = fs.openSync(filePath, fs.OpenMode.CREATE | fs.OpenMode.READ_WRITE);
imagePackerApi.packToFile(imageSource, file.fd, packOpts).then(() => {
// 直接打包进文件
}).catch((error : BusinessError) => {
console.error('Failed to pack the image. And the error is: ' + error);
}).finally(()=>{
fs.closeSync(file.fd);
})
```
图片编码保存进图库
可以将图片编码保存到应用沙箱，然后使用媒体文件管理相关接口保存媒体库资源。
示例代码

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-picture-encoding-V14
爬取时间: 2025-04-28 20:25:23
来源: Huawei Developer
图片编码指将Picture多图对象编码成不同格式的存档图片（当前仅支持打包为JPEG 和 HEIF 格式），用于后续处理，如保存、传输等。
开发步骤
图片编码相关API的详细介绍请参见：图片编码接口说明。
图片编码进文件流
1.  创建图像编码ImagePacker对象。
```typescript
// 导入相关模块包
import { image } from '@kit.ImageKit';
const imagePackerApi = image.createImagePacker();
```
2.  设置编码输出流和编码参数。 format为图像的编码格式；quality为图像质量，范围从0-100，100为最佳质量 根据MIME标准，标准编码格式为image/jpeg。当使用image编码时，PackingOption.format设置为image/jpeg，image编码后的文件扩展名可设为.jpg或.jpeg，可在支持image/jpeg解码的平台上使用。
```typescript
let packOpts: image.PackingOption = {
format: "image/jpeg",
quality: 98,
bufferSize: 10,
desiredDynamicRange: image.PackingDynamicRange.AUTO,
needsPackProperties: true
};
```
3.  进行图片编码，并保存编码后的图片。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
imagePackerApi.packing(picture, packOpts).then( (data : ArrayBuffer) => {
console.info('Succeeded in packing the image.'+ data);
}).catch((error : BusinessError) => {
console.error('Failed to pack the image. And the error is: ' + error);
})
```
图片编码进文件
在编码时，开发者可以传入对应的文件路径，编码后的内存数据将直接写入文件。
```typescript
const context : Context = getContext(this);
const path : string = context.cacheDir + "/picture.jpg";
let file = fileIo.openSync(path, fileIo.OpenMode.CREATE | fileIo.OpenMode.READ_WRITE);
imagePackerApi.packToFile(picture, file.fd, packOpts).then(() => {
console.info('Succeeded in packing the image to file.');
}).catch((error : BusinessError) => {
console.error('Failed to pack the image. And the error is: ' + error);
})
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-tool-V14
爬取时间: 2025-04-28 20:25:37
来源: Huawei Developer
图片工具当前主要提供图片EXIF信息的读取与编辑能力。
EXIF（Exchangeable image file format）是专门为数码相机的照片设定的文件格式，可以记录数码照片的属性信息和拍摄数据。当前仅支持JPEG格式图片。
在图库等应用中，需要查看或修改数码照片的EXIF信息。由于摄像机的手动镜头的参数无法自动写入到EXIF信息中或者因为相机断电等原因经常会导致拍摄时间出错，这时候就需要手动修改错误的EXIF数据，即可使用本功能。
HarmonyOS目前仅支持对部分EXIF信息的查看和修改，具体支持的范围请参见：Exif信息。
开发步骤
EXIF信息的读取与编辑相关API的详细介绍请参见API参考。
1.  获取图片，创建图片源ImageSource。
```typescript
// 导入相关模块包
import { image } from '@kit.ImageKit';
// 获取沙箱路径创建ImageSource
const fd : number = 0; // 获取需要被处理的图片的fd
const imageSourceApi : image.ImageSource = image.createImageSource(fd);
```
2.  读取、编辑EXIF信息。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
// 读取EXIF信息，BitsPerSample为每个像素比特数
let options : image.ImagePropertyOptions = { index: 0, defaultValue: '9999' }
imageSourceApi.getImageProperty(image.PropertyKey.BITS_PER_SAMPLE, options).then((data : string) => {
console.log('Succeeded in getting the value of the specified attribute key of the image.');
}).catch((error : BusinessError) => {
console.error('Failed to get the value of the specified attribute key of the image.');
})
// 编辑EXIF信息
imageSourceApi.modifyImageProperty(image.PropertyKey.IMAGE_WIDTH, "120").then(() => {
imageSourceApi.getImageProperty(image.PropertyKey.IMAGE_WIDTH).then((width : string) => {
console.info('The new imageWidth is ' + width);
}).catch((error : BusinessError) => {
console.error('Failed to get the Image Width.');
})
}).catch((error : BusinessError) => {
console.error('Failed to modify the Image Width');
})
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-native-V14
爬取时间: 2025-04-28 20:25:51
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-source-c-V14
爬取时间: 2025-04-28 20:27:16
来源: Huawei Developer
创建图片源，获取位图的宽、高信息，以及释放图片源实例。
开发步骤
添加链接库
在进行应用开发之前，开发者需要打开native工程的src/main/cpp/CMakeLists.txt，在target_link_libraries依赖中添加libimage_source.so 以及日志依赖libhilog_ndk.z.so。
Native接口调用
具体接口说明请参考API文档。
在hello.cpp中实现C API接口调用逻辑，示例代码如下：
解码接口使用示例
在创建ImageSource实例后，进行指定属性值的获取和修改、通过解码参数创建PixelMap对象、获取图像帧数等操作。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-source-picture-c-V14
爬取时间: 2025-04-28 20:27:30
来源: Huawei Developer
创建图片源，解码多图对象，以及释放图片源实例。
开发步骤
添加链接库
在进行应用开发之前，开发者需要打开native工程的src/main/cpp/CMakeLists.txt，在target_link_libraries依赖中添libimage_source.so 以及日志依赖libhilog_ndk.z.so。
Native接口调用
具体接口说明请参考API文档。
在hello.cpp中实现C API接口调用逻辑，示例代码如下：
解码接口使用示例
在创建ImageSource实例后，进行指定属性值的获取和修改，通过解码参数创建PixelMap对象，获取图像帧数等操作。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-receiver-c-V14
爬取时间: 2025-04-28 20:27:44
来源: Huawei Developer
图像接收类，用于获取组件surface id、接收最新的图片和读取下一张图片、释放ImageReceiver实例。结合camera API实现的相机预览示例代码可参考C/C++预览流二次处理示例。
开发步骤
添加依赖
在进行应用开发之前，开发者需要打开native工程的src/main/cpp/CMakeLists.txt，在target_link_libraries依赖中添加libohimage.so、libimage_receiver.so、libnative_image.so以及日志依赖libhilog_ndk.z.so。
Native接口调用
具体接口说明请参考API文档。
在hello.cpp中实现C Native API接口调用逻辑，示例代码如下：

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/pixelmap-c-V14
爬取时间: 2025-04-28 20:27:57
来源: Huawei Developer
创建位图，获取位图的宽、高、pixelFormat、alphaType、rowStride信息、对位图进行操作以及释放位图实例。
开发步骤
添加链接库
在进行应用开发之前，开发者需要打开native工程的src/main/cpp/CMakeLists.txt，在target_link_libraries依赖中添加libpixelmap.so以及日志依赖libhilog_ndk.z.so。
Native接口调用
具体接口说明请参考API文档。
在hello.cpp中实现C API接口调用逻辑，示例代码如下：
位图接口使用示例
在初始化参数后创建Pixelmap实例，进行图片像素数据的读写，对图片进行缩放、位置变换、反转、旋转、裁剪等操作。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-info-c-V14
爬取时间: 2025-04-28 20:28:11
来源: Huawei Developer
图像信息类，用于设置和读取图像的矩形大小、组件信息和像素信息。
开发步骤
添加依赖
在进行应用开发之前，开发者需要打开native工程的src/main/cpp/CMakeLists.txt，在target_link_libraries依赖中添加libohimage.so，libimage_receiver.so，libnative_image.so 以及日志依赖libhilog_ndk.z.so。
Native接口调用
具体接口说明请参考API文档。
在hello.cpp中实现C Native API接口调用逻辑，示例代码如下：

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-packer-c-V14
爬取时间: 2025-04-28 20:28:25
来源: Huawei Developer
图像打包类，用于创建以及释放ImagePacker实例。
开发步骤
添加链接库
在进行应用开发之前，开发者需要打开native工程的src/main/cpp/CMakeLists.txt，在target_link_libraries依赖中添加libimage_packer.so 以及日志依赖libhilog_ndk.z.so。
Native接口调用
具体接口说明请参考API文档。
在hello.cpp中实现C API接口调用逻辑，示例代码如下：
编码接口使用示例
在创建ImagePacker实例，指定打包参数后将ImageSource或Pixelmap图片源打包至文件或者缓冲区。
根据MIME标准，标准编码格式为image/jpeg。当使用image编码时，打包参数中的编码格式image_MimeType设置为image/jpeg，image编码后的文件扩展名可设为.jpg或.jpeg，可在支持image/jpeg解码的平台上使用。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-packer-picture-c-V14
爬取时间: 2025-04-28 20:28:39
来源: Huawei Developer
图像打包类，用于创建以及释放ImagePacker实例，并编码多图对象。
开发步骤
添加链接库
在进行应用开发之前，开发者需要打开native工程的src/main/cpp/CMakeLists.txt，在target_link_libraries依赖中添libimage_packer.so 以及日志依赖libhilog_ndk.z.so。
Native接口调用
具体接口说明请参考API文档。
在hello.cpp中实现C API接口调用逻辑，示例代码如下：
编码接口使用示例
在创建ImagePacker实例，指定打包参数后将Picture多图对象打包至文件或者缓冲区。
根据MIME标准，标准编码格式为image/jpeg。当使用image编码时，打包参数中的编码格式image_MimeType设置为image/jpeg，image编码后的文件扩展名可设为.jpg或.jpeg，可在支持image/jpeg解码的平台上使用。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-effect-guidelines-V14
爬取时间: 2025-04-28 20:28:52
来源: Huawei Developer
场景介绍
ImageEffect提供了一系列接口用于图像的编辑。开发者可以通过ImageEffect接口处理不同图像输入类型Pixelmap、NativeWindow、NativeBuffer或Uri，获得滤镜处理效果。
针对ImageEffect，常见的开发场景如下：
接口说明
详细的接口说明请参考ImageEffect。
开发步骤
添加动态链接库
CMakeLists.txt中添加以下lib。
根据处理的图像类型添加对应动态链接库：Pixelmap(libpixelmap.so)、NativeWindow(libnative_window.so)、NativeBuffer(libnative_buffer.so)
添加头文件
通过ImageEffect提供的接口生效图像效果
1.  创建ImageEffect实例。
2.  添加EffectFilter滤镜。
3.  设置处理数据。 场景一：设置 OH_PixelmapNative 输入类型。 OH_PixelmapNative的具体使用方法请参考Pixelmap开发指导。 场景二：设置 OH_NativeBuffer 输入类型。 OH_NativeBuffer的具体使用方法请参考NativeBuffer开发指导。 场景三：设置 URI 输入类型。 场景四：设置 OHNativeWindow 输入类型。 以相机预览场景为例来说明OHNativeWindow输入场景。XComponent组件为相机预览流提供的SurfaceId，可在native c++层将SurfaceId转换成OHNativeWindow，下面提供一份代码示例。 XComponent模块的具体使用方法请参考XComponent组件参考。 NativeWindow模块的具体使用方法请参考OHNativeWindow。 Camera的具体使用方法请参考Camera预览参考。 (1) 在xxx.ets中添加一个XComponent组件。 (2) imageEffect.getSurfaceId的native c++层具体实现。
4.  启动效果器。
5.  停止生效效果（可选，仅在输入Surface场景下才有效）。
6.  序列化效果器（可选）。
7.  销毁效果器实例。
自定义滤镜
以下步骤描述了如何实现并注册自定义滤镜接口：
1.  定义 ImageEffect_FilterDelegate。 其中Render接口的实现分两种场景。 场景一：自定义算法可以直接修改info中的像素数据（比如：亮度调节滤镜）。 场景二：自定义算法不能直接修改info中的像素数据（比如：裁剪滤镜）。
2.  生成自定义滤镜信息。
3.  将 ImageEffect_FilterDelegate 注册到效果器。
EffectFilter快速实现单个滤镜的处理效果
1.  创建滤镜。
2.  设置滤镜参数。
3.  生效滤镜。
4.  销毁滤镜实例。
查询能力
-  根据滤镜名查询滤镜信息。
-  根据条件查询满足条件的滤镜。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-processing-V14
爬取时间: 2025-04-28 20:29:06
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-scaling-V14
爬取时间: 2025-04-28 20:29:19
来源: Huawei Developer
本模块提供图片细节增强的的C API接口，通过调用本模块，可以实现图片内容的清晰度增强及缩放功能，处理后的数据可以用于送显和编码保存。
典型应用场景如：图片解码获取图片buffer > 图片超分 > 显示。
约束与限制
1.  档位 输入分辨率要求 （单位：像素） 输出分辨率要求 （单位：像素） 说明 NONE 宽：[32,3000] 高：[32,3000] 宽：[32,3000] 高：[32,3000] 仅适用于缩放场景，支持改变宽高比例，无清晰度增强效果。 LOW 宽：[32,3000] 高：[32,3000] 宽：[32,3000] 高：[32,3000] 仅适用于缩放场景，支持改变宽高比例。 缩放时会对图像进行低质量的清晰度增强，处理速度较快。 该档位为默认设置。 MEDIUM 宽：[32,3000] 高：[32,3000] 宽：[32,3000] 高：[32,3000] 仅适用于缩放场景，支持改变宽高比例。 缩放时会对图像进行中等质量的清晰度增强，处理速度适中。 HIGH 宽：[512,2000] 高：[512,2000] 宽：[512,2000] 高：[512,2000] 适用于缩放及清晰度增强场景，支持改变宽高比例。 缩放时会对图像进行高质量的清晰度增强，处理速度相对较慢。
| 档位  | 输入分辨率要求 （单位：像素）  | 输出分辨率要求 （单位：像素）  | 说明  |
| --- | --- | --- | --- |
| NONE  | 宽：[32,3000] 高：[32,3000]  | 宽：[32,3000] 高：[32,3000]  | 仅适用于缩放场景，支持改变宽高比例，无清晰度增强效果。  |
| LOW  | 宽：[32,3000] 高：[32,3000]  | 宽：[32,3000] 高：[32,3000]  | 仅适用于缩放场景，支持改变宽高比例。 缩放时会对图像进行低质量的清晰度增强，处理速度较快。 该档位为默认设置。  |
| MEDIUM  | 宽：[32,3000] 高：[32,3000]  | 宽：[32,3000] 高：[32,3000]  | 仅适用于缩放场景，支持改变宽高比例。 缩放时会对图像进行中等质量的清晰度增强，处理速度适中。  |
| HIGH  | 宽：[512,2000] 高：[512,2000]  | 宽：[512,2000] 高：[512,2000]  | 适用于缩放及清晰度增强场景，支持改变宽高比例。 缩放时会对图像进行高质量的清晰度增强，处理速度相对较慢。  |
开发指导
在 CMake 脚本中链接动态库
开发步骤
1.  应用可以通过图片处理引擎模块类型来创建图片细节增强模块。示例中的变量说明如下：

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-dynamic-metadata-generation-V14
爬取时间: 2025-04-28 20:29:33
来源: Huawei Developer
调用者可以调用本模块提供的C API接口，实现HDR图片动态元数据生成。
该能力常用于图片编辑中，如下图所示：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165951.00504210269524131888837101767163:50001231000000:2800:D3F357FEA2DEDFDB2E5209310DF72910DC4CE38F4F0D9F5C70DDE7009CE15494.png)
规格说明
支持的数据输入格式：
| 输入ColorSpaceName   | 输入HdrMetadataType  | 输入PIXEL_FORMAT  |
| --- | --- | --- |
| BT2020_PQ_LIMIT / BT2020_PQ  | HDR_METADATA_TYPE_ALTERNATE  | PIXEL_FORMAT_YCBCR_P010 / PIXEL_FORMAT_YCRCB_P010 / PIXEL_FORMAT_RGBA_1010102  |
| BT2020_HLG_LIMIT / BT2020_HLG  |
| BT2020_PQ_LIMIT / BT2020_PQ  | HDR_METADATA_TYPE_BASE  |
| BT2020_HLG_LIMIT / BT2020_HLG  |
| BT2020_PQ_LIMIT / BT2020_PQ  | HDR_METADATA_TYPE_NONE  |
| BT2020_HLG_LIMIT / BT2020_HLG  |
输入ColorSpaceName
输入HdrMetadataType
输入PIXEL_FORMAT
BT2020_PQ_LIMIT / BT2020_PQ
HDR_METADATA_TYPE_ALTERNATE
PIXEL_FORMAT_YCBCR_P010 / PIXEL_FORMAT_YCRCB_P010 / PIXEL_FORMAT_RGBA_1010102
BT2020_HLG_LIMIT / BT2020_HLG
BT2020_PQ_LIMIT / BT2020_PQ
HDR_METADATA_TYPE_BASE
BT2020_HLG_LIMIT / BT2020_HLG
BT2020_PQ_LIMIT / BT2020_PQ
HDR_METADATA_TYPE_NONE
BT2020_HLG_LIMIT / BT2020_HLG
分辨率规格：
| 最小分辨率（单位：像素）  | 最大分辨率（单位：像素）  |
| --- | --- |
| 32*32  | 8880*8880  |
最小分辨率（单位：像素）
最大分辨率（单位：像素）
32*32
8880*8880
开发指导
在 CMake 脚本中链接动态库
ArkTS侧调用的开发步骤
Native侧调用的开发步骤
1.  一般在进程内第一次使用时调用，可提前完成部分耗时操作。
2.  instance：图片处理模块实例。 IMAGE_PROCESSING_TYPE_METADATA_GENERATION：图片元数据生成类型。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-csc-V14
爬取时间: 2025-04-28 20:29:47
来源: Huawei Developer
调用者可以调用本模块提供的C API接口，实现HDR2SDR、SDR2HDR、SDR2SDR的图片色彩空间转换。
该能力常用于图片编辑中，如下图所示：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165951.09777059780039990969731273193509:50001231000000:2800:A328AC8A45A6D4ACB4BFD862442273A9119C4B7EE19EF684CA86B139B82CB04F.png)
规格说明
支持的数据输入格式：
-  输入ColorSpaceName 输入HdrMetadataType 输入PIXEL_FORMAT 输出ColorSpaceName 输出 HdrMetadataType 输出PIXEL_FORMAT BT2020_HLG_LIMIT / BT2020_HLG HDR_METADATA_TYPE_ALTERNATE PIXEL_FORMAT_YCBCR_P010 / PIXEL_FORMAT_YCRCB_P010 / PIXEL_FORMAT_RGBA_1010102 SRGB_LIMIT/SRGB HDR_METADATA_TYPE_BASE PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888 BT2020_PQ_LIMIT / BT2020_PQ HDR_METADATA_TYPE_ALTERNATE YCBCR_P010, YCRCB_P010, RGBA_1010102 SRGB_LIMIT/SRGB HDR_METADATA_TYPE_BASE PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888 BT2020_HLG_LIMIT / BT2020_HLG HDR_METADATA_TYPE_NONE YCBCR_P010, YCRCB_P010, RGBA_1010102 SRGB_LIMIT/SRGB HDR_METADATA_TYPE_BASE PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888 BT2020_PQ_LIMIT / BT2020_PQ HDR_METADATA_TYPE_NONE YCBCR_P010, YCRCB_P010, RGBA_1010102 SRGB_LIMIT/SRGB HDR_METADATA_TYPE_BASE PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888 BT2020_HLG_LIMIT / BT2020_HLG HDR_METADATA_TYPE_ALTERNATE YCBCR_P010, YCRCB_P010, RGBA_1010102 DISPLAY_P3_LIMIT/DISPLAY_P3 HDR_METADATA_TYPE_BASE PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888 BT2020_PQ_LIMIT / BT2020_PQ HDR_METADATA_TYPE_ALTERNATE YCBCR_P010, YCRCB_P010, RGBA_1010102 DISPLAY_P3_LIMIT/DISPLAY_P3 HDR_METADATA_TYPE_BASE PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888 BT2020_HLG_LIMIT / BT2020_HLG HDR_METADATA_TYPE_NONE YCBCR_P010, YCRCB_P010, RGBA_1010102 DISPLAY_P3_LIMIT/DISPLAY_P3 HDR_METADATA_TYPE_BASE PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888 BT2020_PQ_LIMIT / BT2020_PQ HDR_METADATA_TYPE_NONE YCBCR_P010, YCRCB_P010, RGBA_1010102 DISPLAY_P3_LIMIT/DISPLAY_P3 HDR_METADATA_TYPE_BASE PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888 BT2020_HLG_LIMIT / BT2020_HLG HDR_METADATA_TYPE_ALTERNATE YCBCR_P010, YCRCB_P010, RGBA_1010102 SRGB_LIMIT/SRGB HDR_METADATA_TYPE_NONE PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888 BT2020_PQ_LIMIT / BT2020_PQ HDR_METADATA_TYPE_ALTERNATE YCBCR_P010, YCRCB_P010, RGBA_1010102 SRGB_LIMIT/SRGB HDR_METADATA_TYPE_NONE PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888 BT2020_HLG_LIMIT / BT2020_HLG HDR_METADATA_TYPE_NONE YCBCR_P010, YCRCB_P010, RGBA_1010102 SRGB_LIMIT/SRGB HDR_METADATA_TYPE_NONE PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888 BT2020_PQ_LIMIT / BT2020_PQ HDR_METADATA_TYPE_NONE YCBCR_P010, YCRCB_P010, RGBA_1010102 SRGB_LIMIT/SRGB HDR_METADATA_TYPE_NONE PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888 BT2020_HLG_LIMIT / BT2020_HLG HDR_METADATA_TYPE_ALTERNATE YCBCR_P010, YCRCB_P010, RGBA_1010102 DISPLAY_P3_LIMIT/DISPLAY_P3 HDR_METADATA_TYPE_NONE PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888 BT2020_PQ_LIMIT / BT2020_PQ HDR_METADATA_TYPE_ALTERNATE YCBCR_P010, YCRCB_P010, RGBA_1010102 DISPLAY_P3_LIMIT/DISPLAY_P3 HDR_METADATA_TYPE_NONE PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888 BT2020_HLG_LIMIT / BT2020_HLG HDR_METADATA_TYPE_NONE YCBCR_P010, YCRCB_P010, RGBA_1010102 DISPLAY_P3_LIMIT/DISPLAY_P3 HDR_METADATA_TYPE_NONE PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888 BT2020_PQ_LIMIT / BT2020_PQ HDR_METADATA_TYPE_NONE YCBCR_P010, YCRCB_P010, RGBA_1010102 DISPLAY_P3_LIMIT/DISPLAY_P3 HDR_METADATA_TYPE_NONE PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888
-  输入ColorSpaceName 输入HdrMetadataType 输入PIXEL_FORMAT 输出ColorSpaceName 输出 HdrMetadataType 输出PIXEL_FORMAT SRGB_LIMIT HDR_METADATA_TYPE_NONE RGBA_8888 BT2020_HLG HDR_METADATA_TYPE_ALTERNATE YCBCR_P010/ RGBA_1010102 DISPLAY_P3_LIMIT HDR_METADATA_TYPE_NONE RGBA_8888 BT2020_HLG HDR_METADATA_TYPE_ALTERNATE YCBCR_P010/ RGBA_1010102 SRGB HDR_METADATA_TYPE_NONE RGBA_8888 BT2020_HLG HDR_METADATA_TYPE_ALTERNATE YCBCR_P010/ RGBA_1010102 DISPLAY_P3 HDR_METADATA_TYPE_NONE RGBA_8888 BT2020_HLG HDR_METADATA_TYPE_ALTERNATE YCBCR_P010/ RGBA_1010102 SRGB HDR_METADATA_TYPE_NONE RGBA_8888 BT2020_HLG HDR_METADATA_TYPE_NONE YCBCR_P010/ RGBA_1010102 DISPLAY_P3 HDR_METADATA_TYPE_NONE RGBA_8888 BT2020_HLG HDR_METADATA_TYPE_NONE YCBCR_P010/ RGBA_1010102 ADOBE_RGB_1998 HDR_METADATA_TYPE_NONE RGBA_8888, BGRA_8888 BT2020_HLG HDR_METADATA_TYPE_NONE YCBCR_P010/ RGBA_1010102 SRGB HDR_METADATA_TYPE_NONE RGBA_8888, BGRA_8888 BT2020_PQ HDR_METADATA_TYPE_NONE YCBCR_P010/ RGBA_1010102 DISPLAY_P3 HDR_METADATA_TYPE_NONE RGBA_8888, BGRA_8888 BT2020_PQ HDR_METADATA_TYPE_NONE YCBCR_P010/ RGBA_1010102 ADOBE_RGB_1998 HDR_METADATA_TYPE_NONE RGBA_8888, BGRA_8888 BT2020_PQ HDR_METADATA_TYPE_NONE YCBCR_P010/ RGBA_1010102
-  输入ColorSpaceName 输入HdrMetadataType 输入PIXEL_FORMAT 输出ColorSpaceName 输出 HdrMetadataType 输出PIXEL_FORMAT SRGB HDR_METADATA_TYPE_NONE RGBA_8888, BGRA_8888 SRGB HDR_METADATA_TYPE_NONE RGBA_8888 DISPLAY_P3 HDR_METADATA_TYPE_NONE RGBA_8888, BGRA_8888 SRGB HDR_METADATA_TYPE_NONE RGBA_8888 ADOBE_RGB_1998 HDR_METADATA_TYPE_NONE RGBA_8888, BGRA_8888 SRGB HDR_METADATA_TYPE_NONE RGBA_8888 SRGB HDR_METADATA_TYPE_NONE RGBA_8888, BGRA_8888 DISPLAY_P3 HDR_METADATA_TYPE_NONE RGBA_8888 DISPLAY_P3 HDR_METADATA_TYPE_NONE RGBA_8888, BGRA_8888 DISPLAY_P3 HDR_METADATA_TYPE_NONE RGBA_8888 ADOBE_RGB_1998 HDR_METADATA_TYPE_NONE RGBA_8888, BGRA_8888 DISPLAY_P3 HDR_METADATA_TYPE_NONE RGBA_8888
| 输入ColorSpaceName   | 输入HdrMetadataType  | 输入PIXEL_FORMAT  | 输出ColorSpaceName  | 输出 HdrMetadataType   | 输出PIXEL_FORMAT   |
| --- | --- | --- | --- | --- | --- |
| BT2020_HLG_LIMIT / BT2020_HLG  | HDR_METADATA_TYPE_ALTERNATE  | PIXEL_FORMAT_YCBCR_P010 / PIXEL_FORMAT_YCRCB_P010 / PIXEL_FORMAT_RGBA_1010102  | SRGB_LIMIT/SRGB  | HDR_METADATA_TYPE_BASE  | PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888  |
| BT2020_PQ_LIMIT / BT2020_PQ  | HDR_METADATA_TYPE_ALTERNATE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | SRGB_LIMIT/SRGB  | HDR_METADATA_TYPE_BASE  | PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888  |
| BT2020_HLG_LIMIT / BT2020_HLG  | HDR_METADATA_TYPE_NONE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | SRGB_LIMIT/SRGB  | HDR_METADATA_TYPE_BASE  | PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888  |
| BT2020_PQ_LIMIT / BT2020_PQ  | HDR_METADATA_TYPE_NONE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | SRGB_LIMIT/SRGB  | HDR_METADATA_TYPE_BASE  | PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888  |
| BT2020_HLG_LIMIT / BT2020_HLG  | HDR_METADATA_TYPE_ALTERNATE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | DISPLAY_P3_LIMIT/DISPLAY_P3  | HDR_METADATA_TYPE_BASE  | PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888  |
| BT2020_PQ_LIMIT / BT2020_PQ  | HDR_METADATA_TYPE_ALTERNATE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | DISPLAY_P3_LIMIT/DISPLAY_P3  | HDR_METADATA_TYPE_BASE  | PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888  |
| BT2020_HLG_LIMIT / BT2020_HLG  | HDR_METADATA_TYPE_NONE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | DISPLAY_P3_LIMIT/DISPLAY_P3  | HDR_METADATA_TYPE_BASE  | PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888  |
| BT2020_PQ_LIMIT / BT2020_PQ  | HDR_METADATA_TYPE_NONE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | DISPLAY_P3_LIMIT/DISPLAY_P3  | HDR_METADATA_TYPE_BASE  | PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888  |
| BT2020_HLG_LIMIT / BT2020_HLG  | HDR_METADATA_TYPE_ALTERNATE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | SRGB_LIMIT/SRGB  | HDR_METADATA_TYPE_NONE  | PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888  |
| BT2020_PQ_LIMIT / BT2020_PQ  | HDR_METADATA_TYPE_ALTERNATE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | SRGB_LIMIT/SRGB  | HDR_METADATA_TYPE_NONE  | PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888  |
| BT2020_HLG_LIMIT / BT2020_HLG  | HDR_METADATA_TYPE_NONE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | SRGB_LIMIT/SRGB  | HDR_METADATA_TYPE_NONE  | PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888  |
| BT2020_PQ_LIMIT / BT2020_PQ  | HDR_METADATA_TYPE_NONE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | SRGB_LIMIT/SRGB  | HDR_METADATA_TYPE_NONE  | PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888  |
| BT2020_HLG_LIMIT / BT2020_HLG  | HDR_METADATA_TYPE_ALTERNATE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | DISPLAY_P3_LIMIT/DISPLAY_P3  | HDR_METADATA_TYPE_NONE  | PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888  |
| BT2020_PQ_LIMIT / BT2020_PQ  | HDR_METADATA_TYPE_ALTERNATE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | DISPLAY_P3_LIMIT/DISPLAY_P3  | HDR_METADATA_TYPE_NONE  | PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888  |
| BT2020_HLG_LIMIT / BT2020_HLG  | HDR_METADATA_TYPE_NONE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | DISPLAY_P3_LIMIT/DISPLAY_P3  | HDR_METADATA_TYPE_NONE  | PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888  |
| BT2020_PQ_LIMIT / BT2020_PQ  | HDR_METADATA_TYPE_NONE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | DISPLAY_P3_LIMIT/DISPLAY_P3  | HDR_METADATA_TYPE_NONE  | PIXEL_FORMAT_RGBA_8888 / PIXEL_FORMAT_BGRA_8888  |
| 输入ColorSpaceName   | 输入HdrMetadataType  | 输入PIXEL_FORMAT  | 输出ColorSpaceName  | 输出 HdrMetadataType   | 输出PIXEL_FORMAT   |
| --- | --- | --- | --- | --- | --- |
| SRGB_LIMIT  | HDR_METADATA_TYPE_NONE  | RGBA_8888  | BT2020_HLG  | HDR_METADATA_TYPE_ALTERNATE  | YCBCR_P010/ RGBA_1010102  |
| DISPLAY_P3_LIMIT  | HDR_METADATA_TYPE_NONE  | RGBA_8888  | BT2020_HLG  | HDR_METADATA_TYPE_ALTERNATE  | YCBCR_P010/ RGBA_1010102  |
| SRGB  | HDR_METADATA_TYPE_NONE  | RGBA_8888  | BT2020_HLG  | HDR_METADATA_TYPE_ALTERNATE  | YCBCR_P010/ RGBA_1010102  |
| DISPLAY_P3  | HDR_METADATA_TYPE_NONE  | RGBA_8888  | BT2020_HLG  | HDR_METADATA_TYPE_ALTERNATE  | YCBCR_P010/ RGBA_1010102  |
| SRGB  | HDR_METADATA_TYPE_NONE  | RGBA_8888  | BT2020_HLG  | HDR_METADATA_TYPE_NONE  | YCBCR_P010/ RGBA_1010102  |
| DISPLAY_P3  | HDR_METADATA_TYPE_NONE  | RGBA_8888  | BT2020_HLG  | HDR_METADATA_TYPE_NONE  | YCBCR_P010/ RGBA_1010102  |
| ADOBE_RGB_1998  | HDR_METADATA_TYPE_NONE  | RGBA_8888, BGRA_8888  | BT2020_HLG  | HDR_METADATA_TYPE_NONE  | YCBCR_P010/ RGBA_1010102  |
| SRGB  | HDR_METADATA_TYPE_NONE  | RGBA_8888, BGRA_8888  | BT2020_PQ  | HDR_METADATA_TYPE_NONE  | YCBCR_P010/ RGBA_1010102  |
| DISPLAY_P3  | HDR_METADATA_TYPE_NONE  | RGBA_8888, BGRA_8888  | BT2020_PQ  | HDR_METADATA_TYPE_NONE  | YCBCR_P010/ RGBA_1010102  |
| ADOBE_RGB_1998  | HDR_METADATA_TYPE_NONE  | RGBA_8888, BGRA_8888  | BT2020_PQ  | HDR_METADATA_TYPE_NONE  | YCBCR_P010/ RGBA_1010102  |
| 输入ColorSpaceName   | 输入HdrMetadataType  | 输入PIXEL_FORMAT  | 输出ColorSpaceName  | 输出 HdrMetadataType   | 输出PIXEL_FORMAT   |
| --- | --- | --- | --- | --- | --- |
| SRGB  | HDR_METADATA_TYPE_NONE  | RGBA_8888, BGRA_8888  | SRGB  | HDR_METADATA_TYPE_NONE  | RGBA_8888  |
| DISPLAY_P3  | HDR_METADATA_TYPE_NONE  | RGBA_8888, BGRA_8888  | SRGB  | HDR_METADATA_TYPE_NONE  | RGBA_8888  |
| ADOBE_RGB_1998  | HDR_METADATA_TYPE_NONE  | RGBA_8888, BGRA_8888  | SRGB  | HDR_METADATA_TYPE_NONE  | RGBA_8888  |
| SRGB  | HDR_METADATA_TYPE_NONE  | RGBA_8888, BGRA_8888  | DISPLAY_P3  | HDR_METADATA_TYPE_NONE  | RGBA_8888  |
| DISPLAY_P3  | HDR_METADATA_TYPE_NONE  | RGBA_8888, BGRA_8888  | DISPLAY_P3  | HDR_METADATA_TYPE_NONE  | RGBA_8888  |
| ADOBE_RGB_1998  | HDR_METADATA_TYPE_NONE  | RGBA_8888, BGRA_8888  | DISPLAY_P3  | HDR_METADATA_TYPE_NONE  | RGBA_8888  |
分辨率规格：
| 最小分辨率（单位：像素）  | 最大分辨率（单位：像素）  |
| --- | --- |
| 32*32  | 8880*8880  |
最小分辨率（单位：像素）
最大分辨率（单位：像素）
32*32
8880*8880
开发指导
在 CMake 脚本中链接动态库
ArkTS侧调用的开发步骤
Native侧调用的开发步骤
1.  一般在进程内第一次使用时调用，可提前完成部分耗时操作。
2.  应用可以通过图片处理引擎模块类型来创建图片色彩空间转换模块。示例中的变量说明如下：

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/hdr-single-to-dual-V14
爬取时间: 2025-04-28 20:30:01
来源: Huawei Developer
调用者可以调用本模块提供的C API接口，实现Decompose单层HDR图片转双层HDR图片。
该能力常用于图片分享中，如下图所示：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165952.93126482601654295732019593929231:50001231000000:2800:B4CDD490DDE59F91DDD36880848A1D4DF7D04EB0B55E9BD2E50B2DCBE88EB58B.png)
规格说明
支持的数据输入格式：
使用图片单层HDR转双层HDR转换算法Decompose。
| 输入ColorSpaceName   | 输入HdrMetadataType  | 输入PIXEL_FORMAT  | 输出ColorSpaceName  | 输出 HdrMetadataType   | 输出PIXEL_FORMAT   |
| --- | --- | --- | --- | --- | --- |
| BT2020_HLG_LIMIT/BT2020_HLG  | HDR_METADATA_TYPE_ALTERNATE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | SRGB_LIMIT/SRGB  | HDR_METADATA_TYPE_BASE  | RGBA_8888/ BGRA_8888  |
| BT2020_PQ_LIMIT/BT2020_PQ  | HDR_METADATA_TYPE_ALTERNATE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | SRGB_LIMIT/SRGB  | HDR_METADATA_TYPE_BASE  | RGBA_8888/ BGRA_8888  |
| BT2020_HLG_LIMIT/BT2020_HLG  | HDR_METADATA_TYPE_NONE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | SRGB_LIMIT/SRGB  | HDR_METADATA_TYPE_BASE  | RGBA_8888/ BGRA_8888  |
| BT2020_PQ_LIMIT/BT2020_PQ  | HDR_METADATA_TYPE_NONE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | SRGB_LIMIT/SRGB  | HDR_METADATA_TYPE_BASE  | RGBA_8888/ BGRA_8888  |
| BT2020_HLG_LIMIT/BT2020_HLG  | HDR_METADATA_TYPE_ALTERNATE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | DISPLAY_P3_LIMIT/DISPLAY_P3  | HDR_METADATA_TYPE_BASE  | RGBA_8888/ BGRA_8888  |
| BT2020_PQ_LIMIT/BT2020_PQ  | HDR_METADATA_TYPE_ALTERNATE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | DISPLAY_P3_LIMIT/DISPLAY_P3  | HDR_METADATA_TYPE_BASE  | RGBA_8888/ BGRA_8888  |
| BT2020_HLG_LIMIT/BT2020_HLG  | HDR_METADATA_TYPE_NONE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | DISPLAY_P3_LIMIT/DISPLAY_P3  | HDR_METADATA_TYPE_BASE  | RGBA_8888/ BGRA_8888  |
| BT2020_PQ_LIMIT/BT2020_PQ  | HDR_METADATA_TYPE_NONE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | DISPLAY_P3_LIMIT/DISPLAY_P3  | HDR_METADATA_TYPE_BASE  | RGBA_8888/ BGRA_8888  |
| BT2020_HLG_LIMIT/BT2020_HLG  | HDR_METADATA_TYPE_ALTERNATE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | SRGB_LIMIT/SRGB  | HDR_METADATA_TYPE_NONE  | RGBA_8888/ BGRA_8888  |
| BT2020_PQ_LIMIT/BT2020_PQ  | HDR_METADATA_TYPE_ALTERNATE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | SRGB_LIMIT/SRGB  | HDR_METADATA_TYPE_NONE  | RGBA_8888/ BGRA_8888  |
| BT2020_HLG_LIMIT/BT2020_HLG  | HDR_METADATA_TYPE_NONE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | SRGB_LIMIT/SRGB  | HDR_METADATA_TYPE_NONE  | RGBA_8888/ BGRA_8888  |
| BT2020_PQ_LIMIT/BT2020_PQ  | HDR_METADATA_TYPE_NONE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | SRGB_LIMIT/SRGB  | HDR_METADATA_TYPE_NONE  | RGBA_8888/ BGRA_8888  |
| BT2020_HLG_LIMIT/BT2020_HLG  | HDR_METADATA_TYPE_ALTERNATE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | DISPLAY_P3_LIMIT/DISPLAY_P3  | HDR_METADATA_TYPE_NONE  | RGBA_8888/ BGRA_8888  |
| BT2020_PQ_LIMIT/BT2020_PQ  | HDR_METADATA_TYPE_ALTERNATE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | DISPLAY_P3_LIMIT/DISPLAY_P3  | HDR_METADATA_TYPE_NONE  | RGBA_8888/ BGRA_8888  |
| BT2020_HLG_LIMIT/BT2020_HLG  | HDR_METADATA_TYPE_NONE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | DISPLAY_P3_LIMIT/DISPLAY_P3  | HDR_METADATA_TYPE_NONE  | RGBA_8888/ BGRA_8888  |
| BT2020_PQ_LIMIT/BT2020_PQ  | HDR_METADATA_TYPE_NONE  | YCBCR_P010, YCRCB_P010, RGBA_1010102  | DISPLAY_P3_LIMIT/DISPLAY_P3  | HDR_METADATA_TYPE_NONE  | RGBA_8888/ BGRA_8888  |
输入ColorSpaceName
输入HdrMetadataType
输入PIXEL_FORMAT
输出ColorSpaceName
输出
HdrMetadataType
输出PIXEL_FORMAT
BT2020_HLG_LIMIT/BT2020_HLG
HDR_METADATA_TYPE_ALTERNATE
YCBCR_P010,
YCRCB_P010,
RGBA_1010102
SRGB_LIMIT/SRGB
HDR_METADATA_TYPE_BASE
RGBA_8888/
BGRA_8888
BT2020_PQ_LIMIT/BT2020_PQ
HDR_METADATA_TYPE_ALTERNATE
YCBCR_P010,
YCRCB_P010,
RGBA_1010102
SRGB_LIMIT/SRGB
HDR_METADATA_TYPE_BASE
RGBA_8888/
BGRA_8888
BT2020_HLG_LIMIT/BT2020_HLG
HDR_METADATA_TYPE_NONE
YCBCR_P010,
YCRCB_P010,
RGBA_1010102
SRGB_LIMIT/SRGB
HDR_METADATA_TYPE_BASE
RGBA_8888/
BGRA_8888
BT2020_PQ_LIMIT/BT2020_PQ
HDR_METADATA_TYPE_NONE
YCBCR_P010,
YCRCB_P010,
RGBA_1010102
SRGB_LIMIT/SRGB
HDR_METADATA_TYPE_BASE
RGBA_8888/
BGRA_8888
BT2020_HLG_LIMIT/BT2020_HLG
HDR_METADATA_TYPE_ALTERNATE
YCBCR_P010,
YCRCB_P010,
RGBA_1010102
DISPLAY_P3_LIMIT/DISPLAY_P3
HDR_METADATA_TYPE_BASE
RGBA_8888/
BGRA_8888
BT2020_PQ_LIMIT/BT2020_PQ
HDR_METADATA_TYPE_ALTERNATE
YCBCR_P010,
YCRCB_P010,
RGBA_1010102
DISPLAY_P3_LIMIT/DISPLAY_P3
HDR_METADATA_TYPE_BASE
RGBA_8888/
BGRA_8888
BT2020_HLG_LIMIT/BT2020_HLG
HDR_METADATA_TYPE_NONE
YCBCR_P010,
YCRCB_P010,
RGBA_1010102
DISPLAY_P3_LIMIT/DISPLAY_P3
HDR_METADATA_TYPE_BASE
RGBA_8888/
BGRA_8888
BT2020_PQ_LIMIT/BT2020_PQ
HDR_METADATA_TYPE_NONE
YCBCR_P010,
YCRCB_P010,
RGBA_1010102
DISPLAY_P3_LIMIT/DISPLAY_P3
HDR_METADATA_TYPE_BASE
RGBA_8888/
BGRA_8888
BT2020_HLG_LIMIT/BT2020_HLG
HDR_METADATA_TYPE_ALTERNATE
YCBCR_P010,
YCRCB_P010,
RGBA_1010102
SRGB_LIMIT/SRGB
HDR_METADATA_TYPE_NONE
RGBA_8888/
BGRA_8888
BT2020_PQ_LIMIT/BT2020_PQ
HDR_METADATA_TYPE_ALTERNATE
YCBCR_P010,
YCRCB_P010,
RGBA_1010102
SRGB_LIMIT/SRGB
HDR_METADATA_TYPE_NONE
RGBA_8888/
BGRA_8888
BT2020_HLG_LIMIT/BT2020_HLG
HDR_METADATA_TYPE_NONE
YCBCR_P010,
YCRCB_P010,
RGBA_1010102
SRGB_LIMIT/SRGB
HDR_METADATA_TYPE_NONE
RGBA_8888/
BGRA_8888
BT2020_PQ_LIMIT/BT2020_PQ
HDR_METADATA_TYPE_NONE
YCBCR_P010,
YCRCB_P010,
RGBA_1010102
SRGB_LIMIT/SRGB
HDR_METADATA_TYPE_NONE
RGBA_8888/
BGRA_8888
BT2020_HLG_LIMIT/BT2020_HLG
HDR_METADATA_TYPE_ALTERNATE
YCBCR_P010,
YCRCB_P010,
RGBA_1010102
DISPLAY_P3_LIMIT/DISPLAY_P3
HDR_METADATA_TYPE_NONE
RGBA_8888/
BGRA_8888
BT2020_PQ_LIMIT/BT2020_PQ
HDR_METADATA_TYPE_ALTERNATE
YCBCR_P010,
YCRCB_P010,
RGBA_1010102
DISPLAY_P3_LIMIT/DISPLAY_P3
HDR_METADATA_TYPE_NONE
RGBA_8888/
BGRA_8888
BT2020_HLG_LIMIT/BT2020_HLG
HDR_METADATA_TYPE_NONE
YCBCR_P010,
YCRCB_P010,
RGBA_1010102
DISPLAY_P3_LIMIT/DISPLAY_P3
HDR_METADATA_TYPE_NONE
RGBA_8888/
BGRA_8888
BT2020_PQ_LIMIT/BT2020_PQ
HDR_METADATA_TYPE_NONE
YCBCR_P010,
YCRCB_P010,
RGBA_1010102
DISPLAY_P3_LIMIT/DISPLAY_P3
HDR_METADATA_TYPE_NONE
RGBA_8888/
BGRA_8888
分辨率规格：
| 最小分辨率（单位：像素）  | 最大分辨率（单位：像素）  |
| --- | --- |
| 32*32  | 8880*8880  |
最小分辨率（单位：像素）
最大分辨率（单位：像素）
32*32
8880*8880
开发指导
在 CMake 脚本中链接动态库
ArkTS侧调用的开发步骤
Native侧调用的开发步骤
1.  应用可以通过图片处理引擎模块类型来创建图片HDR单层转双层模块。示例中的变量说明如下：

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/hdr-dual-to-single-V14
爬取时间: 2025-04-28 20:30:15
来源: Huawei Developer
调用者可以调用本模块提供的C API接口，实现Compose双层HDR图片转单层HDR图片。
该能力常用于图片分享中，如下图所示：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165952.07385054142678337454314207576171:50001231000000:2800:BC319DEA2C4C67AD8A9CDC9CF953B278CF5EF49F2BBBB5DFC30C5099D8E8ADB0.png)
规格说明
支持的数据输入格式：
使用图片单层HDR转双层HDR转换算法Compose。
| 输入ColorSpaceName   | 输入HdrMetadataType  | 输入PIXEL_FORMAT  | 输出ColorSpaceName  | 输出 HdrMetadataType   | 输出PIXEL_FORMAT   |
| --- | --- | --- | --- | --- | --- |
| DISPLAY_P3_LIMIT/DISPLAY_P3  | HDR_METADATA_TYPE_BASE  | RGBA_8888  | BT2020_HLG  | HDR_METADATA_TYPE_ALTERNATE  | YCBCR_P010/ RGBA_1010102  |
| SRGB_LIMIT/SRGB  | HDR_METADATA_TYPE_BASE  | RGBA_8888  | BT2020_HLG  | HDR_METADATA_TYPE_ALTERNATE  | YCBCR_P010/ RGBA_1010102  |
| DISPLAY_P3_LIMIT/DISPLAY_P3  | HDR_METADATA_TYPE_BASE  | RGBA_8888  | BT2020_PQ  | HDR_METADATA_TYPE_ALTERNATE  | YCBCR_P010/ RGBA_1010102  |
| SRGB_LIMIT/SRGB  | HDR_METADATA_TYPE_BASE  | RGBA_8888  | BT2020_PQ  | HDR_METADATA_TYPE_ALTERNATE  | YCBCR_P010/ RGBA_1010102  |
输入ColorSpaceName
输入HdrMetadataType
输入PIXEL_FORMAT
输出ColorSpaceName
输出
HdrMetadataType
输出PIXEL_FORMAT
DISPLAY_P3_LIMIT/DISPLAY_P3
HDR_METADATA_TYPE_BASE
RGBA_8888
BT2020_HLG
HDR_METADATA_TYPE_ALTERNATE
YCBCR_P010/
RGBA_1010102
SRGB_LIMIT/SRGB
HDR_METADATA_TYPE_BASE
RGBA_8888
BT2020_HLG
HDR_METADATA_TYPE_ALTERNATE
YCBCR_P010/
RGBA_1010102
DISPLAY_P3_LIMIT/DISPLAY_P3
HDR_METADATA_TYPE_BASE
RGBA_8888
BT2020_PQ
HDR_METADATA_TYPE_ALTERNATE
YCBCR_P010/
RGBA_1010102
SRGB_LIMIT/SRGB
HDR_METADATA_TYPE_BASE
RGBA_8888
BT2020_PQ
HDR_METADATA_TYPE_ALTERNATE
YCBCR_P010/
RGBA_1010102
分辨率规格：
| 最小分辨率（单位：像素）  | 最大分辨率（单位：像素）  |
| --- | --- |
| 32*32  | 8880*8880  |
最小分辨率（单位：像素）
最大分辨率（单位：像素）
32*32
8880*8880
开发指导
在 CMake 脚本中链接动态库
ArkTS侧调用的开发步骤
Native侧调用的开发步骤
1.  应用可以通过图片处理引擎模块类型来创建图片HDR双层转单层模块。示例中的变量说明如下：

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-native-js-objects-V14
爬取时间: 2025-04-28 20:30:28
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-decoding-native-V14
爬取时间: 2025-04-28 20:30:42
来源: Huawei Developer
图片解码指将所支持格式的存档图片解码成统一的PixelMap，以便在应用或系统中进行图片显示或图片处理。当前支持的存档图片格式包括JPEG、PNG、GIF、WebP、BMP、SVG、ICO、DNG、HEIF(不同硬件设备支持情况不同)。
开发步骤
图片解码相关API的详细介绍请参见：图片解码接口文档。
添加依赖
在进行应用开发之前，开发者需要打开native工程的src/main/cpp/CMakeLists.txt，在target_link_libraries依赖中添加libace_napi.z.so、libpixelmap_ndk.z.so、libimage_source_ndk.z.so、librawfile.z.so以及日志依赖libhilog_ndk.z.so。
添加接口映射
打开src/main/cpp/hello.cpp文件，在Init函数中添加getSyncPixelMap函数接口映射，作用是以同步的方式生成PixelMap，具体代码如下：
JS侧调用
1.  打开src\main\cpp\types\libentry\index.d.ts（其中libentry根据工程名生成），导入如下引用文件：
2.  准备图片资源文件，本示例文件名为example.jpg，导入到src\main\resources\rawfile\ 路径下。
3.  打开src\main\ets\pages\index.ets，导入"libentry.so（根据工程名生成）"，调用Native接口，传入JS的资源对象。示例如下：
Native接口调用
具体接口说明请参考API文档。
在hello.cpp文件中获取JS的资源对象，并转为Native的资源对象，即可调用Native接口，调用方式示例代码如下：
添加引用文件
图片框架支持增量式解码，使用方法如下：

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-receiver-native-V14
爬取时间: 2025-04-28 20:30:56
来源: Huawei Developer
图像接收类，用于获取组件surface id，接收最新的图片和读取下一张图片，以及释放ImageReceiver实例。
开发步骤
添加依赖
在进行应用开发之前，开发者需要打开native工程的src/main/cpp/CMakeLists.txt，在target_link_libraries依赖中添加libace_napi.z.so、libimage_ndk.z.so、libimage_receiver_ndk.z.so、libnative_image.so以及日志依赖libhilog_ndk.z.so。
添加接口映射
打开src/main/cpp/hello.cpp文件，在Init函数中添加接口映射如下：
添加权限申请
此处通过camera图片获取输入数据，需要申请权限ohos.permission.CAMERA，申请方式请参考向用户申请授权。
JS侧调用
1.  打开src\main\cpp\types\libentry\index.d.ts（其中libentry根据工程名生成），导入如下引用文件：
2.  打开src\main\ets\pages\index.ets，导入"libentry.so（根据工程名生成）"，调用Native接口，传入JS的资源对象。示例如下：
Native接口调用
具体接口说明请参考API文档。
在hello.cpp文件中获取JS的资源对象，并转为Native的资源对象，即可调用Native接口，调用方式示例代码如下：
添加引用文件

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-transformation-native-V14
爬取时间: 2025-04-28 20:31:09
来源: Huawei Developer
开发者可以通过本指导了解如何使用Native Image的接口。
开发步骤
添加依赖
在进行应用开发之前，开发者需要打开native工程的src/main/cpp/CMakeLists.txt，在target_link_libraries依赖中添加image的libace_napi.z.so、libpixelmap_ndk.z.so以及日志依赖libhilog_ndk.z.so。
添加接口映射
打开src/main/cpp/hello.cpp文件，在Init函数中添加接口映射如下：
Native接口调用
具体接口说明请参考API文档。
在hello.cpp文件中获取JS的资源对象，并转为Native的资源对象，即可调用Native接口，调用方式示例代码如下：
打开src/main/cpp/hello.cpp，添加引用文件。
1.  获取PixelMap的信息，并记录信息到OhosPixelMapInfo结构中。
2.  获取PixelMap对象数据的内存地址，并锁定该内存。
3.  释放PixelMap对象数据的内存锁。
JS侧调用
1.  打开src\main\cpp\types\libentry\index.d.ts(其中libentry根据工程名生成)，导入如下引用文件：
2.  打开src\main\ets\pages\index.ets, 导入"libentry.so"(根据工程名生成)；调用Native接口，传入JS的资源对象。示例如下：

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-pixelmap-operation-native-V14
爬取时间: 2025-04-28 20:31:23
来源: Huawei Developer
开发者可以通过本指导了解如何使用Native Image的接口。
开发步骤
添加依赖
在进行应用开发之前，开发者需要打开native工程的src/main/cpp/CMakeLists.txt，在target_link_libraries依赖中添加image的libace_napi.z.so、libpixelmap_ndk.z.so以及日志依赖libhilog_ndk.z.so。
添加接口映射
打开src/main/cpp/hello.cpp文件，在Init函数中添加接口映射如下：
Native接口调用
具体接口说明请参考API文档。
在hello.cpp文件中获取JS的资源对象，并转为Native的资源对象，即可调用Native接口，调用方式示例代码如下：
添加引用文件
1.  创建一个PixelMap对象。
2.  根据Alpha通道的信息，来生成一个仅包含Alpha通道信息的PixelMap对象。
3.  对PixelMap数据进行处理。
JS侧调用
1.  打开src\main\cpp\types\libentry\index.d.ts（其中libentry根据工程名生成），导入如下引用文件：
2.  打开src\main\ets\pages\index.ets, 导入"libentry.so"(根据工程名生成)，调用Native接口，传入JS的资源对象。示例如下：

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/image-encoding-native-V14
爬取时间: 2025-04-28 20:31:36
来源: Huawei Developer
开发者可以调用本模块的Native API接口，完成图片编码，即将PixelMap压缩成不同格式的存档图片。
当前支持编码为JPEG、WebP、PNG和 HEIF(不同硬件设备支持情况不同)格式。
适用场景
-  图片编码转换。 通过传入图片源ImageSource，封装成想要的格式。
-  图片编辑。 编辑PixelMap后导出图片文件的场景，需要编码成对应图片格式文件。
开发指导
详细的API说明请参考ImagePacker API参考。
参考以下示例代码，完成图片编码的全流程，包括：创建编码器、初始化资源、编码过程、销毁资源。
在应用开发过程中，开发者应按一定顺序调用方法，执行对应操作，否则系统可能会抛出异常或生成其他未定义的行为。具体顺序可参考下列开发步骤及对应说明。
如下为图片编码调用关系图：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165952.24781710831468869772248499953414:50001231000000:2800:34D1B90BEEC8C64DDC6255A84F82F1E3A3BDFFF8856E6F5CACC18E0E375CE883.png)
在 CMake 脚本中链接动态库
开发步骤
1.  引入编码器头文件：image_packer_mdk.h。
2.  创建编码器实例对象。 应用需要napi_env来创建编码器。
3.  初始化资源。 通过OH_ImagePacker_InitNative来初始化编码器原生实例对象。
4.  编码。 编码接口入参包括： 上述过程中获取的实例对象（ImagePacker_Native）。 需要编码的图像源（napi_value）, PixelMap或ImageSource（未调用过CreatePixelMap）的实例对象均可。 编码参数：包括编码格式与编码质量。 根据MIME标准，标准编码格式为image/jpeg。当使用image编码时，编码参数中的编码格式format设置为image/jpeg，image编码后的文件扩展名可设为.jpg或.jpeg，可在支持image/jpeg解码的平台上使用。 编码接口可按输出方式分为向缓存区（内存）输出和向文件输出两种接口，入参均为上述内容。 应用可根据输出的不同需求选择编码接口。 例如向缓存区（内存）输出： 例如向文件输出：
5.  上述过程中获取的实例对象（ImagePacker_Native）。
6.  需要编码的图像源（napi_value）, PixelMap或ImageSource（未调用过CreatePixelMap）的实例对象均可。
7.  编码参数：包括编码格式与编码质量。 根据MIME标准，标准编码格式为image/jpeg。当使用image编码时，编码参数中的编码格式format设置为image/jpeg，image编码后的文件扩展名可设为.jpg或.jpeg，可在支持image/jpeg解码的平台上使用。
8.  销毁编码器实例，释放资源。 资源不能重复销毁。
-  上述过程中获取的实例对象（ImagePacker_Native）。
-  需要编码的图像源（napi_value）, PixelMap或ImageSource（未调用过CreatePixelMap）的实例对象均可。
-  编码参数：包括编码格式与编码质量。 根据MIME标准，标准编码格式为image/jpeg。当使用image编码时，编码参数中的编码格式format设置为image/jpeg，image编码后的文件扩展名可设为.jpg或.jpeg，可在支持image/jpeg解码的平台上使用。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/media-kit-V14
爬取时间: 2025-04-28 20:31:50
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/media-kit-intro-V14
爬取时间: 2025-04-28 20:32:04
来源: Huawei Developer
Media Kit（媒体服务）用于开发音视频播放或录制的各类功能。在Media Kit的开发指导中，将详细介绍音视频多个模块的开发方式，指导开发者如何使用系统提供的音视频API实现对应功能。比如使用SoundPool实现简单的提示音，当设备接收到新消息时，会发出短促的“滴滴”声；使用AVPlayer实现音乐播放器，循环播放一首音乐。
Media Kit提供的模块有：
亮点/特征
-  使用轻量媒体引擎 使用较少的系统资源（线程、内存），可支持音视频播放/录制，支持pipeline灵活拼装，支持插件化扩展source/demuxer/codec。
-  支持HDR视频 系统原生数据结构与接口支持hdr vivid的采集与播放，方便三方应用在业务中使用系统的HDR能力，为用户带来更炫彩的体验。
-  支持音频池 针对开发中常用的短促音效播放场景，如相机快门音效、系统通知音效等，应用可调用SoundPool，实现一次加载，多次低时延播放。
开发说明
本开发指导仅针对音视频播放或录制本身，由media模块提供相关能力，不涉及UI界面、图形处理、媒体存储或其他相关领域功能。
在开发音乐、视频播放功能之前，建议了解流媒体播放的相关概念包括但不限于：
-  播放过程：网络协议 > 容器格式 > 音视频编解码 > 图形/音频渲染
-  网络协议：比如HLS、HTTP-FLV、HTTP/HTTPS
-  容器格式：比如mp4、mkv、mpeg-ts
-  编码格式：比如h264/h265
详细流媒体开发流程请参考流媒体播放开发指导。
AVPlayer
AVPlayer主要工作是将Audio/Video媒体资源（比如mp4/mp3/mkv/mpeg-ts等）转码为可供渲染的图像和可听见的音频模拟信号，并通过输出设备进行播放。
AVPlayer提供功能完善一体化播放能力，应用只需要提供流媒体来源，不负责数据解析和解码就可达成播放效果。
音频播放
当使用AVPlayer开发音乐应用播放音频时，AVPlayer与外部模块的交互关系如图所示。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250328150000.99176907039232155896088701332324:50001231000000:2800:AB4D02F929C775E952C4F28EEF1C2CC29D39D638FBB7CA53493A1DE741DDB3E6.png)
音乐类应用通过调用JS接口层提供的AVPlayer接口实现相应功能时，框架层会通过播放服务（Player Framework）将资源解析成音频数据流（PCM），音频数据流经过软件解码后输出至音频服务（Audio Framework），由音频服务输出至音频驱动渲染，实现音频播放功能。完整的音频播放需要应用、Player Framework、Audio Framework、音频HDI共同实现。
上图中，数字标注表示需要数据与外部模块的传递。
1.  音乐应用将媒体资源传递给AVPlayer接口。
2.  Player Framework将音频PCM数据流输出给Audio Framework，再由Audio Framework输出给音频HDI。
视频播放
当使用AVPlayer开发视频应用播放视频时，AVPlayer与外部模块的交互关系如图所示。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250328150000.62616468516409627036484660929375:50001231000000:2800:26531351E82A5F12E989F356703E295F4C6116596D1D041E17611363ED555E0F.png)
应用通过调用JS接口层提供的AVPlayer接口实现相应功能时，框架层会通过播放服务（Player Framework）解析成单独的音频数据流和视频数据流，音频数据流经过软件解码后输出至音频服务（Audio Framework），再至硬件接口层的音频HDI，实现音频播放功能。视频数据流经过硬件（推荐）/软件解码后输出至图形渲染服务（Graphic Framework），再输出至硬件接口层的显示HDI，完成图形渲染。
完整的视频播放需要：应用、XComponent、Player Framework、Graphic Framework、Audio Framework、显示HDI和音频HDI共同实现。
图中的数字标注表示需要数据与外部模块的传递。
1.  应用从XComponent组件获取窗口SurfaceID，获取方式参考XComponent。
2.  应用把媒体资源、SurfaceID传递给AVPlayer接口。
3.  Player Framework把视频ES数据流输出给解码HDI，解码获得视频帧（NV12/NV21/RGBA）。
4.  Player Framework把音频PCM数据流输出给Audio Framework，Audio Framework输出给音频HDI。
5.  Player Framework把视频帧（NV12/NV21/RGBA）输出给Graphic Framework，Graphic Framework输出给显示HDI。
支持的格式与协议
推荐使用以下主流的播放格式，音视频容器、音视频编码属于内容创作者所掌握的专业领域，不建议应用开发者自制码流进行测试，以免产生无法播放、卡顿、花屏等兼容性问题。若发生此类问题不会影响系统，退出播放即可。
支持的协议如下：
| 协议类型 | 协议描述 |
| --- | --- |
| 本地点播 | 协议格式：支持file descriptor，禁止file path |
| 网络点播 | 协议格式：支持http/https/hls/dash |
| 网络直播 | 协议格式：支持hls/http-flv |
支持的音频播放格式如下：
| 音频容器规格 | 规格描述 |
| --- | --- |
| m4a | 音频格式：AAC |
| aac | 音频格式：AAC |
| mp3 | 音频格式：MP3 |
| ogg | 音频格式：VORBIS |
| wav | 音频格式：PCM |
| amr | 音频格式：AMR |
支持的视频播放格式和主流分辨率如下：
| 视频容器规格 | 规格描述 | 分辨率 |
| --- | --- | --- |
| mp4 | 视频格式：H26510+/H264 音频格式：AAC/MP3 | 主流分辨率，如4K/1080P/720P/480P/270P |
| mkv | 视频格式：H26510+/H264 音频格式：AAC/MP3 | 主流分辨率，如4K/1080P/720P/480P/270P |
| ts | 视频格式：H26510+/H264 音频格式：AAC/MP3 | 主流分辨率，如4K/1080P/720P/480P/270P |
视频格式：H26510+/H264
音频格式：AAC/MP3
视频格式：H26510+/H264
音频格式：AAC/MP3
视频格式：H26510+/H264
音频格式：AAC/MP3
支持的字幕格式如下：
| 字幕容器规格 | 支持的协议 | 支持的加载方式 |
| --- | --- | --- |
| srt | 本地点播(fd)/网络点播(http/https/hls/dash) | 外挂字幕 |
| vtt | 本地点播(fd)/网络点播(http/https/hls/dash) | 外挂字幕 |
| webvtt | 网络点播(dash协议) | 内置字幕 |
当dash协议存在内置字幕时，不支持添加外挂字幕。
SoundPool
SoundPool主要工作是将音频媒体资源（比如mp3/m4a/wav等）转码为音频模拟信号，并通过输出设备进行播放。
SoundPool提供短音频的播放能力，应用只需要提供音频资源来源，不负责数据解析和解码就可达成播放效果。
当使用SoundPool开发应用播放音频时，SoundPool与外部模块的交互关系如图所示。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250328150000.80593054715319966600614592798563:50001231000000:2800:525AF9C3B55277AF98690E9F94AC515372892FEA8D5C99C771EBB9D95A447A79.png)
音乐类应用通过调用JS接口层提供的SoundPool接口实现相应功能时，框架层会通过播放服务（Player Framework）将资源解析成音频数据流（PCM），音频数据流经过软件解码后输出至音频服务（Audio Framework），由音频服务输出至音频驱动渲染，实现音频播放功能。完整的音频播放需要应用、Player Framework、Audio Framework、音频HDI共同实现。
图中的数字标注表示需要数据与外部模块的传递。
1.  音乐应用将媒体资源传递给SoundPool接口。
2.  Player Framework将音频PCM数据流输出给Audio Framework，再由Audio Framework输出给音频HDI。
支持的格式与协议
推荐使用以下主流的播放格式，音视容器、音频编码属于内容创作者所掌握的专业领域，不建议应用开发者自制码流进行测试，以免产生无法播放、卡顿等兼容性问题。若发生此类问题不会影响系统，退出播放即可。
支持的协议如下：
| 协议类型 | 协议描述 |
| --- | --- |
| 本地点播 | 协议格式：支持file descriptor，禁止file path |
支持的音频播放格式如下：
| 音频容器规格 | 规格描述 |
| --- | --- |
| m4a | 音频格式：AAC |
| aac | 音频格式：AAC |
| mp3 | 音频格式：MP3 |
| ogg | 音频格式：VORBIS |
| wav | 音频格式：PCM |
AVRecorder
AVRecorder主要工作是捕获音频信号，接收视频信号，完成音视频编码并保存到文件中，帮助开发者轻松实现音视频录制功能，包括开始录制、暂停录制、恢复录制、停止录制、释放资源等功能控制。它允许调用者指定录制的编码格式、封装格式、文件路径等参数。
当使用AVRecorder开发应用录制视频时，AVRecorder与外部模块的交互关系如图所示。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250328150000.61660107609413833978479219726828:50001231000000:2800:11E1F3749326F4495C0F03308114AD68FD7E83EB910E4C5B31A89C268433FCE3.png)
-  音频录制：应用通过调用JS接口层提供的AVRecorder接口实现音频录制时，框架层会通过录制服务（Player Framework），调用音频服务（Audio Framework）通过音频HDI捕获音频数据，通过软件编码封装后保存至文件中，实现音频录制功能。
-  视频录制：应用通过调用JS接口层提供的AVRecorder接口实现视频录制时，先通过Camera接口调用相机服务（Camera Framework）通过视频HDI捕获图像数据送至框架层的录制服务，录制服务将图像数据通过视频编码HDI编码，再将编码后的图像数据封装至文件中，实现视频录制功能。
通过音视频录制组合，可分别实现纯音频录制、纯视频录制、音视频录制。
图中的数字标注表示需要数据与外部模块的传递。
1.  应用通过AVRecorder接口从录制服务获取SurfaceID。
2.  应用将SurfaceID设置给相机服务，相机服务可以通过SurfaceID获取到Surface。相机服务通过视频HDI捕获图像数据送至框架层的录制服务。
3.  相机服务通过Surface将视频数据传递给录制服务。
4.  录制服务通过视频编码HDI模块将视频数据编码。
5.  录制服务将音频参数设置给音频服务，并从音频服务获取到音频数据。
支持的格式
支持的音频源如下：
| 音频源类型 | 说明 |
| --- | --- |
| mic | 系统麦克风作为音频源输入。 |
支持的视频源如下：
| 视频源类型 | 说明 |
| --- | --- |
| surface_yuv | 输入surface中携带的是raw data。 |
| surface_es | 输入surface中携带的是ES data。 |
支持的音视频编码格式如下：
| 音视频编码格式 | 说明 |
| --- | --- |
| audio/mp4a-latm | 音频/mp4a-latm类型 |
| video/hevc | 视频/hevc类型 |
| video/avc | 视频/avc类型 |
| audio/mpeg | 音频/mpeg类型 |
| audio/g711mu | 音频/g711-mulaw类型 |
支持的输出文件格式如下：
| 输出文件格式 | 说明 |
| --- | --- |
| mp4 | 视频的容器格式，MP4。 |
| m4a | 音频的容器格式，M4A。 |
| mp3 | 音频的容器格式，MP3。 |
| wav | 音频的容器格式，wav。 |
AVScreenCapture
AVScreenCapture主要工作是捕获音频信号、视频信号，并通过音视频编码将屏幕信息保存到文件中，帮助开发者轻松实现屏幕录制功能，主要包括录屏存文件和录屏取码流两套接口，它允许调用者指定屏幕录制的编码格式、封装格式和文件路径等参数。
当使用AVScreenCapture开发应用录制屏幕时，AVScreenCapture与外部模块的交互关系如图所示。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250328150001.56854590264349128836327798557407:50001231000000:2800:BF355A626A5E13DD2ECCC1DD93B89CD7FCA6CCEC8CC05C6287F1EF00E4806B76.png)
支持的格式
支持的音频源如下：
| 音频源类型 | 说明 |
| --- | --- |
| MIC | 系统麦克风作为音频源输入。 |
| ALL_PLAYBACK | 系统内录使用作为音频源输入。 |
支持的视频源如下：
| 视频源类型 | 说明 |
| --- | --- |
| SURFACE_RGBA | 输出Buffer是rgba data |
支持的音频编码格式如下：
| 音频编码格式 | 说明 |
| --- | --- |
| AAC_LC | AAC_LC类型 |
支持的视频编码格式如下：
| 视频编码格式 | 说明 |
| --- | --- |
| H264 | H264类型 |
支持的输出文件格式如下：
| 输出文件格式 | 说明 |
| --- | --- |
| mp4 | 视频的容器格式，MP4。 |
| m4a | 纯音频的容器格式，M4A。 |
AVMetadataExtractor
AVMetadataExtractor 主要用于获取音视频元数据。通过使用 AVMetadataExtractor，开发者可以从原始媒体资源中提取出丰富的元数据信息。以音频资源为例，我们可以获取到关于该音频的标题、艺术家、专辑名称、时长等详细信息。视频资源的元数据获取流程与音频类似，由于视频没有专辑封面，所以无法获取视频资源的专辑封面。
获取音频资源的元数据的全流程包含：创建AVMetadataExtractor，设置资源，获取元数据，获取专辑封面（可选），销毁资源。
支持的格式
支持的音视频源参考媒体数据解析。
AVImageGenerator
AVImageGenerator 主要用于获取视频缩略图。通过使用 AVImageGenerator，开发者可以实现从原始媒体资源中获取视频指定时间的视频帧。
支持的格式
支持的视频源参考视频解码。
AVTranscoder
AVTranscoder主要用于将已压缩编码的视频文件按照指定参数转换为另一种格式的视频。
支持的格式
当前版本AVTranscoder提供以下转码服务：
支持修改源视频文件的编码参数（格式、码率）和封装格式。源视频的音视频编码和封装格式为系统AVCodec支持的解码和解封装格式，目标视频的音视频编码和封装格式为系统AVCodec支持的编码和封装格式。
-  支持将HDR VIVID视频转换为SDR视频，以及SDR视频的转码。
-  支持转码时降低视频分辨率。 原视频分辨率不高于4K，且目标视频分辨率不低于240p。 目标视频宽、高不能大于源视频宽、高，且不能设置为奇数，详情请参考设置正确的视频宽高。
-  支持的源视频格式：
-  支持的目标视频格式：
-  支持的轨道数：

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/media-kit-dev--arkts-V14
爬取时间: 2025-04-28 20:32:18
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/media-playback-arkts-V14
爬取时间: 2025-04-28 20:32:32
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/using-avplayer-for-playback-V14
爬取时间: 2025-04-28 20:32:45
来源: Huawei Developer
使用AVPlayer可以实现端到端播放原始媒体资源，本开发指导将以完整地播放一首音乐作为示例，向开发者讲解AVPlayer音频播放相关功能。如需播放PCM音频数据，请使用AudioRenderer。
播放的全流程包含：创建AVPlayer，设置播放资源，设置播放参数（音量/倍速/焦点模式），播放控制（播放/暂停/跳转/停止），重置，销毁资源。
在进行应用开发的过程中，开发者可以通过AVPlayer的state属性主动获取当前状态或使用on('stateChange')方法监听状态变化。如果应用在音频播放器处于错误状态时执行操作，系统可能会抛出异常或生成其他未定义的行为。
图1播放状态变化示意图
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165953.99955305765916580634462980926369:50001231000000:2800:05C4D674A1894FC638734BBA56D6B3B32D83680845CC8F51B9FDF37AEBBE3C1B.png)
状态的详细说明请参考AVPlayerState。当播放处于prepared / playing / paused / completed状态时，播放引擎处于工作状态，这需要占用系统较多的运行内存。当客户端暂时不使用播放器时，调用reset()或release()回收内存资源，做好资源利用。
开发建议
当前指导仅介绍如何实现媒体资源播放，在应用开发过程中可能会涉及后台播放、播放冲突等情况，请根据实际需要参考以下说明。
开发步骤及注意事项
详细的API说明请参考AVPlayer API参考。
1.  创建实例createAVPlayer()，AVPlayer初始化idle状态。
2.  设置业务需要的监听事件，搭配全流程场景使用。支持的监听事件包括： 响应API调用，监听seek()请求完成情况。 当使用seek()跳转到指定播放位置后，如果seek操作成功，将上报该事件。 响应API调用，监听setSpeed()请求完成情况。 当使用setSpeed()设置播放倍速后，如果setSpeed操作成功，将上报该事件。 响应API调用，监听setVolume()请求完成情况。 当使用setVolume()调节播放音量后，如果setVolume操作成功，将上报该事件。 监听音频焦点切换信息，搭配属性audioInterruptMode使用。 如果当前设备存在多个音频正在播放，音频焦点被切换（即播放其他媒体如通话等）时将上报该事件，应用可以及时处理。
3.  设置资源：设置属性url，AVPlayer进入initialized状态。 下面代码示例中的url仅作示意使用，开发者需根据实际情况，确认资源有效性并设置： 如果使用本地资源播放，必须确认资源文件可用，并使用应用沙箱路径访问对应资源，参考获取应用文件路径。应用沙箱的介绍及如何向应用沙箱推送文件，请参考文件管理。 如果使用网络播放路径，需声明权限：ohos.permission.INTERNET。 如果使用ResourceManager.getRawFd打开HAP资源文件描述符，使用方法可参考ResourceManager API参考。 需要使用支持的播放格式与协议。 此外，如果需要设置音频渲染信息，则只允许在initialized状态下，第一次调用prepare()之前设置，以便音频渲染器信息在之后生效。若媒体源包含视频，则usage默认值为STREAM_USAGE_MOVIE，否则usage默认值为STREAM_USAGE_MUSIC。rendererFlags默认值为0。若默认usage不满足需求，则须主动配置audio.AudioRendererInfo。
4.  如果使用本地资源播放，必须确认资源文件可用，并使用应用沙箱路径访问对应资源，参考获取应用文件路径。应用沙箱的介绍及如何向应用沙箱推送文件，请参考文件管理。
5.  如果使用网络播放路径，需声明权限：ohos.permission.INTERNET。
6.  如果使用ResourceManager.getRawFd打开HAP资源文件描述符，使用方法可参考ResourceManager API参考。
7.  需要使用支持的播放格式与协议。
8.  准备播放：调用prepare()，AVPlayer进入prepared状态，此时可以获取duration，设置音量。
9.  音频播控：播放play()，暂停pause()，跳转seek()，停止stop() 等操作。
10.  （可选）更换资源：调用reset()重置资源，AVPlayer重新进入idle状态，允许更换资源url。
11.  退出播放：调用release()销毁实例，AVPlayer进入released状态，退出播放。
| 事件类型 | 说明 |
| --- | --- |
| stateChange | 必要事件，监听播放器的state属性改变。 |
| error | 必要事件，监听播放器的错误信息。 |
| durationUpdate | 用于进度条，监听进度条长度，刷新资源时长。 |
| timeUpdate | 用于进度条，监听进度条当前位置，刷新当前时间。 |
| seekDone | 响应API调用，监听seek()请求完成情况。 当使用seek()跳转到指定播放位置后，如果seek操作成功，将上报该事件。 |
| speedDone | 响应API调用，监听setSpeed()请求完成情况。 当使用setSpeed()设置播放倍速后，如果setSpeed操作成功，将上报该事件。 |
| volumeChange | 响应API调用，监听setVolume()请求完成情况。 当使用setVolume()调节播放音量后，如果setVolume操作成功，将上报该事件。 |
| bufferingUpdate | 用于网络播放，监听网络播放缓冲信息，用于上报缓冲百分比以及缓存播放进度。 |
| audioInterrupt | 监听音频焦点切换信息，搭配属性audioInterruptMode使用。 如果当前设备存在多个音频正在播放，音频焦点被切换（即播放其他媒体如通话等）时将上报该事件，应用可以及时处理。 |
-  如果使用本地资源播放，必须确认资源文件可用，并使用应用沙箱路径访问对应资源，参考获取应用文件路径。应用沙箱的介绍及如何向应用沙箱推送文件，请参考文件管理。
-  如果使用网络播放路径，需声明权限：ohos.permission.INTERNET。
-  如果使用ResourceManager.getRawFd打开HAP资源文件描述符，使用方法可参考ResourceManager API参考。
-  需要使用支持的播放格式与协议。
完整示例
参考以下示例，完整地播放一首音乐，实现起播后3s暂停，暂停3s重新播放的效果。
```typescript
import { media } from '@kit.MediaKit';
import { fileIo as fs } from '@kit.CoreFileKit';
import { common } from '@kit.AbilityKit';
import { BusinessError } from '@kit.BasicServicesKit';
import { audio } from '@kit.AudioKit';
export class AVPlayerDemo {
private count: number = 0;
private isSeek: boolean = true; // 用于区分模式是否支持seek操作
private fileSize: number = -1;
private fd: number = 0;
// 注册avplayer回调函数
setAVPlayerCallback(avPlayer: media.AVPlayer) {
// seek操作结果回调函数
avPlayer.on('seekDone', (seekDoneTime: number) => {
console.info(`AVPlayer seek succeeded, seek time is ${seekDoneTime}`);
});
// error回调监听函数,当avPlayer在操作过程中出现错误时调用 reset接口触发重置流程
avPlayer.on('error', (err: BusinessError) => {
console.error(`Invoke avPlayer failed, code is ${err.code}, message is ${err.message}`);
avPlayer.reset(); // 调用reset重置资源，触发idle状态
});
// 状态机变化回调函数
avPlayer.on('stateChange', async (state: string, reason: media.StateChangeReason) => {
switch (state) {
case 'idle': // 成功调用reset接口后触发该状态机上报
console.info('AVPlayer state idle called.');
avPlayer.release(); // 调用release接口销毁实例对象
break;
case 'initialized': // avplayer 设置播放源后触发该状态上报
console.info('AVPlayer state initialized called.');
avPlayer.audioRendererInfo = {
usage: audio.StreamUsage.STREAM_USAGE_MUSIC,
rendererFlags: 0
};
avPlayer.prepare();
break;
case 'prepared': // prepare调用成功后上报该状态机
console.info('AVPlayer state prepared called.');
avPlayer.play(); // 调用播放接口开始播放
break;
case 'playing': // play成功调用后触发该状态机上报
console.info('AVPlayer state playing called.');
if (this.count !== 0) {
if (this.isSeek) {
console.info('AVPlayer start to seek.');
avPlayer.seek(avPlayer.duration); //seek到音频末尾
} else {
// 当播放模式不支持seek操作时继续播放到结尾
console.info('AVPlayer wait to play end.');
}
} else {
setTimeout(() => {
console.info('AVPlayer playing wait to pause');
avPlayer.pause(); // 播放3s后调用暂停接口暂停播放
}, 3000);
}
this.count++;
break;
case 'paused': // pause成功调用后触发该状态机上报
console.info('AVPlayer state paused called.');
setTimeout(() => {
console.info('AVPlayer paused wait to play again');
avPlayer.play(); // 暂停3s后再次调用播放接口开始播放
}, 3000);
break;
case 'completed': // 播放结束后触发该状态机上报
console.info('AVPlayer state completed called.');
avPlayer.stop(); //调用播放结束接口
break;
case 'stopped': // stop接口成功调用后触发该状态机上报
console.info('AVPlayer state stopped called.');
avPlayer.reset(); // 调用reset接口初始化avplayer状态
break;
case 'released':
console.info('AVPlayer state released called.');
break;
default:
console.info('AVPlayer state unknown called.');
break;
}
});
}
// 以下demo为使用fs文件系统打开沙箱地址获取媒体文件地址并通过url属性进行播放示例
async avPlayerUrlDemo() {
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
// 创建状态机变化回调函数
this.setAVPlayerCallback(avPlayer);
let fdPath = 'fd://';
// 通过UIAbilityContext获取沙箱地址filesDir，以Stage模型为例
let context = getContext(this) as common.UIAbilityContext;
let pathDir = context.filesDir;
let path = pathDir + '/01.mp3';
// 打开相应的资源文件地址获取fd，并为url赋值触发initialized状态机上报
let file = await fs.open(path);
fdPath = fdPath + '' + file.fd;
this.isSeek = true; // 支持seek操作
avPlayer.url = fdPath;
}
// 以下demo为使用资源管理接口获取打包在HAP内的媒体资源文件并通过fdSrc属性进行播放示例
async avPlayerFdSrcDemo() {
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
// 创建状态机变化回调函数
this.setAVPlayerCallback(avPlayer);
// 通过UIAbilityContext的resourceManager成员的getRawFd接口获取媒体资源播放地址
// 返回类型为{fd,offset,length},fd为HAP包fd地址，offset为媒体资源偏移量，length为播放长度
let context = getContext(this) as common.UIAbilityContext;
let fileDescriptor = await context.resourceManager.getRawFd('01.mp3');
let avFileDescriptor: media.AVFileDescriptor =
{ fd: fileDescriptor.fd, offset: fileDescriptor.offset, length: fileDescriptor.length };
this.isSeek = true; // 支持seek操作
// 为fdSrc赋值触发initialized状态机上报
avPlayer.fdSrc = avFileDescriptor;
}
// 以下demo为使用fs文件系统打开沙箱地址获取媒体文件地址并通过dataSrc属性进行播放(seek模式)示例
async avPlayerDataSrcSeekDemo() {
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
// 创建状态机变化回调函数
this.setAVPlayerCallback(avPlayer);
// dataSrc播放模式的的播放源地址，当播放为Seek模式时fileSize为播放文件的具体大小，下面会对fileSize赋值
let src: media.AVDataSrcDescriptor = {
fileSize: -1,
callback: (buf: ArrayBuffer, length: number, pos: number | undefined) => {
let num = 0;
if (buf == undefined || length == undefined || pos == undefined) {
return -1;
}
num = fs.readSync(this.fd, buf, { offset: pos, length: length });
if (num > 0 && (this.fileSize >= pos)) {
return num;
}
return -1;
}
};
let context = getContext(this) as common.UIAbilityContext;
// 通过UIAbilityContext获取沙箱地址filesDir，以Stage模型为例
let pathDir = context.filesDir;
let path = pathDir  + '/01.mp3';
await fs.open(path).then((file: fs.File) => {
this.fd = file.fd;
});
// 获取播放文件的大小
this.fileSize = fs.statSync(path).size;
src.fileSize = this.fileSize;
this.isSeek = true; // 支持seek操作
avPlayer.dataSrc = src;
}
// 以下demo为使用fs文件系统打开沙箱地址获取媒体文件地址并通过dataSrc属性进行播放(No seek模式)示例
async avPlayerDataSrcNoSeekDemo() {
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
// 创建状态机变化回调函数
this.setAVPlayerCallback(avPlayer);
let context = getContext(this) as common.UIAbilityContext;
let src: media.AVDataSrcDescriptor = {
fileSize: -1,
callback: (buf: ArrayBuffer, length: number) => {
let num = 0;
if (buf == undefined || length == undefined) {
return -1;
}
num = fs.readSync(this.fd, buf);
if (num > 0) {
return num;
}
return -1;
}
};
// 通过UIAbilityContext获取沙箱地址filesDir，以Stage模型为例
let pathDir = context.filesDir;
let path = pathDir  + '/01.mp3';
await fs.open(path).then((file: fs.File) => {
this.fd = file.fd;
});
this.isSeek = false; // 不支持seek操作
avPlayer.dataSrc = src;
}
// 以下demo为通过url设置网络地址来实现播放直播码流的demo
async avPlayerLiveDemo() {
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
// 创建状态机变化回调函数
this.setAVPlayerCallback(avPlayer);
this.isSeek = false; // 不支持seek操作
avPlayer.url = 'http://xxx.xxx.xxx.xxx:xx/xx/index.m3u8';
}
}
```
示例代码

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/video-playback-V14
爬取时间: 2025-04-28 20:32:59
来源: Huawei Developer
当前提供两种视频播放开发的方案：
-  AVPlayer：功能较完善的音视频播放ArkTS/JS API，集成了流媒体和本地资源解析，媒体资源解封装，视频解码和渲染功能，适用于对媒体资源进行端到端播放的场景，可直接播放mp4、mkv等格式的视频文件。
-  Video组件：封装了视频播放的基础能力，需要设置数据源以及基础信息即可播放视频，但相对扩展能力较弱。Video组件由ArkUI提供能力，相关指导请参考UI开发文档-Video组件。
本开发指导将介绍如何使用AVPlayer开发视频播放功能，以完整地播放一个视频作为示例，实现端到端播放原始媒体资源。
播放的全流程包含：创建AVPlayer，设置播放资源和窗口，设置播放参数（音量/倍速/缩放模式），播放控制（播放/暂停/跳转/停止），重置，销毁资源。在进行应用开发的过程中，开发者可以通过AVPlayer的state属性主动获取当前状态或使用on('stateChange')方法监听状态变化。如果应用在视频播放器处于错误状态时执行操作，系统可能会抛出异常或生成其他未定义的行为。
图1播放状态变化示意图
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165953.60793816811539203827014118052888:50001231000000:2800:9C680CABAA55A36EF43B0BF3D00C5288513DCC440959B32B022F021DFE825B68.png)
状态的详细说明请参考AVPlayerState。当播放处于prepared / playing / paused / completed状态时，播放引擎处于工作状态，这需要占用系统较多的运行内存。当客户端暂时不使用播放器时，调用reset()或release()回收内存资源，做好资源利用。
开发建议
当前指导仅介绍如何实现媒体资源播放，在应用开发过程中可能会涉及后台播放、播放冲突等情况，请根据实际需要参考以下说明。
开发步骤及注意事项
详细的API说明请参考AVPlayer API参考。
1.  调用createAVPlayer()创建AVPlayer实例，初始化进入idle状态。
2.  设置业务需要的监听事件，搭配全流程场景使用。支持的监听事件包括： 响应API调用，监听seek()请求完成情况。 当使用seek()跳转到指定播放位置后，如果seek操作成功，将上报该事件。 响应API调用，监听setSpeed()请求完成情况。 当使用setSpeed()设置播放倍速后，如果setSpeed操作成功，将上报该事件。 响应API调用，监听setVolume()请求完成情况。 当使用setVolume()调节播放音量后，如果setVolume操作成功，将上报该事件。 响应API调用，用于HLS协议流，监听setBitrate()请求完成情况。 当使用setBitrate()指定播放比特率后，如果setBitrate操作成功，将上报该事件。 用于视频播放，监听视频播放首帧渲染时间。 当AVPlayer首次起播进入playing状态后，等到首帧视频画面被渲染到显示画面时，将上报该事件。应用通常可以利用此事件上报，进行视频封面移除，达成封面与视频画面的顺利衔接。 监听音频焦点切换信息，搭配属性audioInterruptMode使用。 如果当前设备存在多个媒体正在播放，音频焦点被切换（即播放其他媒体如通话等）时将上报该事件，应用可以及时处理。
3.  设置资源：设置属性url，AVPlayer进入initialized状态。 下面代码示例中的url仅作示意使用，开发者需根据实际情况，确认资源有效性并设置： 如果使用本地资源播放，必须确认资源文件可用，并使用应用沙箱路径访问对应资源，参考获取应用文件路径。应用沙箱的介绍及如何向应用沙箱推送文件，请参考文件管理。 如果使用网络播放路径，需声明权限：ohos.permission.INTERNET。 如果使用ResourceManager.getRawFd打开HAP资源文件描述符，使用方法可参考ResourceManager API参考。 需要使用支持的播放格式与协议。
4.  如果使用本地资源播放，必须确认资源文件可用，并使用应用沙箱路径访问对应资源，参考获取应用文件路径。应用沙箱的介绍及如何向应用沙箱推送文件，请参考文件管理。
5.  如果使用网络播放路径，需声明权限：ohos.permission.INTERNET。
6.  如果使用ResourceManager.getRawFd打开HAP资源文件描述符，使用方法可参考ResourceManager API参考。
7.  需要使用支持的播放格式与协议。
8.  设置窗口：获取并设置属性SurfaceID，用于设置显示画面。 应用需要从XComponent组件获取surfaceID，获取方式请参考XComponent。
9.  准备播放：调用prepare()，AVPlayer进入prepared状态，此时可以获取duration，设置缩放模式、音量等。
10.  视频播控：播放play()，暂停pause()，跳转seek()，停止stop() 等操作。
11.  （可选）更换资源：调用reset()重置资源，AVPlayer重新进入idle状态，允许更换资源url。
12.  退出播放：调用release()销毁实例，AVPlayer进入released状态，退出播放。
| 事件类型 | 说明 |
| --- | --- |
| stateChange | 必要事件，监听播放器的state属性改变。 |
| error | 必要事件，监听播放器的错误信息。 |
| durationUpdate | 用于进度条，监听进度条长度，刷新资源时长。 |
| timeUpdate | 用于进度条，监听进度条当前位置，刷新当前时间。 |
| seekDone | 响应API调用，监听seek()请求完成情况。 当使用seek()跳转到指定播放位置后，如果seek操作成功，将上报该事件。 |
| speedDone | 响应API调用，监听setSpeed()请求完成情况。 当使用setSpeed()设置播放倍速后，如果setSpeed操作成功，将上报该事件。 |
| volumeChange | 响应API调用，监听setVolume()请求完成情况。 当使用setVolume()调节播放音量后，如果setVolume操作成功，将上报该事件。 |
| bitrateDone | 响应API调用，用于HLS协议流，监听setBitrate()请求完成情况。 当使用setBitrate()指定播放比特率后，如果setBitrate操作成功，将上报该事件。 |
| availableBitrates | 用于HLS协议流，监听HLS资源的可选bitrates，用于setBitrate()。 |
| bufferingUpdate | 用于网络播放，监听网络播放缓冲信息。 |
| startRenderFrame | 用于视频播放，监听视频播放首帧渲染时间。 当AVPlayer首次起播进入playing状态后，等到首帧视频画面被渲染到显示画面时，将上报该事件。应用通常可以利用此事件上报，进行视频封面移除，达成封面与视频画面的顺利衔接。 |
| videoSizeChange | 用于视频播放，监听视频播放的宽高信息，可用于调整窗口大小、比例。 |
| audioInterrupt | 监听音频焦点切换信息，搭配属性audioInterruptMode使用。 如果当前设备存在多个媒体正在播放，音频焦点被切换（即播放其他媒体如通话等）时将上报该事件，应用可以及时处理。 |
-  如果使用本地资源播放，必须确认资源文件可用，并使用应用沙箱路径访问对应资源，参考获取应用文件路径。应用沙箱的介绍及如何向应用沙箱推送文件，请参考文件管理。
-  如果使用网络播放路径，需声明权限：ohos.permission.INTERNET。
-  如果使用ResourceManager.getRawFd打开HAP资源文件描述符，使用方法可参考ResourceManager API参考。
-  需要使用支持的播放格式与协议。
完整示例
```typescript
import { media } from '@kit.MediaKit';
import { fileIo as fs } from '@kit.CoreFileKit';
import { common } from '@kit.AbilityKit';
import { BusinessError } from '@kit.BasicServicesKit';
export class AVPlayerDemo {
private count: number = 0;
private surfaceID: string = ''; // surfaceID用于播放画面显示，具体的值需要通过Xcomponent接口获取，相关文档链接见上面Xcomponent创建方法
private isSeek: boolean = true; // 用于区分模式是否支持seek操作
private fileSize: number = -1;
private fd: number = 0;
constructor(surfaceID: string) {
this.surfaceID = surfaceID;
}
// 注册avplayer回调函数
setAVPlayerCallback(avPlayer: media.AVPlayer) {
// startRenderFrame首帧渲染回调函数
avPlayer.on('startRenderFrame', () => {
console.info(`AVPlayer start render frame`);
});
// seek操作结果回调函数
avPlayer.on('seekDone', (seekDoneTime: number) => {
console.info(`AVPlayer seek succeeded, seek time is ${seekDoneTime}`);
});
// error回调监听函数,当avPlayer在操作过程中出现错误时调用reset接口触发重置流程
avPlayer.on('error', (err: BusinessError) => {
console.error(`Invoke avPlayer failed, code is ${err.code}, message is ${err.message}`);
avPlayer.reset(); // 调用reset重置资源，触发idle状态
});
// 状态机变化回调函数
avPlayer.on('stateChange', async (state: string, reason: media.StateChangeReason) => {
switch (state) {
case 'idle': // 成功调用reset接口后触发该状态机上报
console.info('AVPlayer state idle called.');
avPlayer.release(); // 调用release接口销毁实例对象
break;
case 'initialized': // avplayer 设置播放源后触发该状态上报
console.info('AVPlayer state initialized called.');
avPlayer.surfaceId = this.surfaceID; // 设置显示画面，当播放的资源为纯音频时无需设置
avPlayer.prepare();
break;
case 'prepared': // prepare调用成功后上报该状态机
console.info('AVPlayer state prepared called.');
avPlayer.play(); // 调用播放接口开始播放
break;
case 'playing': // play成功调用后触发该状态机上报
console.info('AVPlayer state playing called.');
if (this.count !== 0) {
if (this.isSeek) {
console.info('AVPlayer start to seek.');
avPlayer.seek(avPlayer.duration); //seek到视频末尾
} else {
// 当播放模式不支持seek操作时继续播放到结尾
console.info('AVPlayer wait to play end.');
}
} else {
avPlayer.pause(); // 调用暂停接口暂停播放
}
this.count++;
break;
case 'paused': // pause成功调用后触发该状态机上报
console.info('AVPlayer state paused called.');
avPlayer.play(); // 再次播放接口开始播放
break;
case 'completed': // 播放结束后触发该状态机上报
console.info('AVPlayer state completed called.');
avPlayer.stop(); //调用播放结束接口
break;
case 'stopped': // stop接口成功调用后触发该状态机上报
console.info('AVPlayer state stopped called.');
avPlayer.reset(); // 调用reset接口初始化avplayer状态
break;
case 'released':
console.info('AVPlayer state released called.');
break;
default:
console.info('AVPlayer state unknown called.');
break;
}
});
}
// 以下demo为使用fs文件系统打开沙箱地址获取媒体文件地址并通过url属性进行播放示例
async avPlayerUrlDemo() {
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
// 创建状态机变化回调函数
this.setAVPlayerCallback(avPlayer);
let fdPath = 'fd://';
let context = getContext(this) as common.UIAbilityContext;
// 通过UIAbilityContext获取沙箱地址filesDir，以Stage模型为例
let pathDir = context.filesDir;
let path = pathDir + '/H264_AAC.mp4';
// 打开相应的资源文件地址获取fd，并为url赋值触发initialized状态机上报
let file = await fs.open(path);
fdPath = fdPath + '' + file.fd;
this.isSeek = true; // 支持seek操作
avPlayer.url = fdPath;
}
// 以下demo为使用资源管理接口获取打包在HAP内的媒体资源文件并通过fdSrc属性进行播放示例
async avPlayerFdSrcDemo() {
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
// 创建状态机变化回调函数
this.setAVPlayerCallback(avPlayer);
// 通过UIAbilityContext的resourceManager成员的getRawFd接口获取媒体资源播放地址
// 返回类型为{fd,offset,length},fd为HAP包fd地址，offset为媒体资源偏移量，length为播放长度
let context = getContext(this) as common.UIAbilityContext;
let fileDescriptor = await context.resourceManager.getRawFd('H264_AAC.mp4');
let avFileDescriptor: media.AVFileDescriptor =
{ fd: fileDescriptor.fd, offset: fileDescriptor.offset, length: fileDescriptor.length };
this.isSeek = true; // 支持seek操作
// 为fdSrc赋值触发initialized状态机上报
avPlayer.fdSrc = avFileDescriptor;
}
// 以下demo为使用fs文件系统打开沙箱地址获取媒体文件地址并通过dataSrc属性进行播放(seek模式)示例
async avPlayerDataSrcSeekDemo() {
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
// 创建状态机变化回调函数
this.setAVPlayerCallback(avPlayer);
// dataSrc播放模式的的播放源地址，当播放为Seek模式时fileSize为播放文件的具体大小，下面会对fileSize赋值
let src: media.AVDataSrcDescriptor = {
fileSize: -1,
callback: (buf: ArrayBuffer, length: number, pos: number | undefined) => {
let num = 0;
if (buf == undefined || length == undefined || pos == undefined) {
return -1;
}
num = fs.readSync(this.fd, buf, { offset: pos, length: length });
if (num > 0 && (this.fileSize >= pos)) {
return num;
}
return -1;
}
};
let context = getContext(this) as common.UIAbilityContext;
// 通过UIAbilityContext获取沙箱地址filesDir，以Stage模型为例
let pathDir = context.filesDir;
let path = pathDir + '/H264_AAC.mp4';
await fs.open(path).then((file: fs.File) => {
this.fd = file.fd;
});
// 获取播放文件的大小
this.fileSize = fs.statSync(path).size;
src.fileSize = this.fileSize;
this.isSeek = true; // 支持seek操作
avPlayer.dataSrc = src;
}
// 以下demo为使用fs文件系统打开沙箱地址获取媒体文件地址并通过dataSrc属性进行播放(No seek模式)示例
async avPlayerDataSrcNoSeekDemo() {
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
// 创建状态机变化回调函数
this.setAVPlayerCallback(avPlayer);
let context = getContext(this) as common.UIAbilityContext;
let src: media.AVDataSrcDescriptor = {
fileSize: -1,
callback: (buf: ArrayBuffer, length: number) => {
let num = 0;
if (buf == undefined || length == undefined) {
return -1;
}
num = fs.readSync(this.fd, buf);
if (num > 0) {
return num;
}
return -1;
}
};
// 通过UIAbilityContext获取沙箱地址filesDir，以Stage模型为例
let pathDir = context.filesDir;
let path = pathDir + '/H264_AAC.mp4';
await fs.open(path).then((file: fs.File) => {
this.fd = file.fd;
});
this.isSeek = false; // 不支持seek操作
avPlayer.dataSrc = src;
}
// 以下demo为通过url设置网络地址来实现播放直播码流的demo
async avPlayerLiveDemo() {
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
// 创建状态机变化回调函数
this.setAVPlayerCallback(avPlayer);
this.isSeek = false; // 不支持seek操作
avPlayer.url = 'http://xxx.xxx.xxx.xxx:xx/xx/index.m3u8'; // 播放hls网络直播码流
}
// 以下demo为通过setMediaSource设置自定义头域及媒体播放优选参数实现初始播放参数设置
async preDownloadDemo() {
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
let mediaSource : media.MediaSource = media.createMediaSourceWithUrl("http://xxx",  {"User-Agent" : "User-Agent-Value"});
let playbackStrategy : media.PlaybackStrategy = {preferredWidth: 1, preferredHeight: 2, preferredBufferDuration: 3, preferredHdr: false};
// 设置媒体来源和播放策略
avPlayer.setMediaSource(mediaSource, playbackStrategy);
}
// 以下demo为通过selectTrack设置音频轨道，通过deselectTrack取消上次设置的音频轨道并恢复到视频默认音频轨道。
async multiTrackDemo() {
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
let audioTrackIndex: Object = 0;
avPlayer.getTrackDescription((error: BusinessError, arrList: Array<media.MediaDescription>) => {
if (arrList != null) {
for (let i = 0; i < arrList.length; i++) {
if (i != 0) {
// 获取音频轨道列表
audioTrackIndex = arrList[i][media.MediaDescriptionKey.MD_KEY_TRACK_INDEX];
}
}
} else {
console.error(`audio getTrackDescription fail, error:${error}`);
}
});
// 选择其中一个音频轨道
avPlayer.selectTrack(parseInt(audioTrackIndex.toString()));
// 取消选择上次选中的音频轨道，并恢复到默认音频轨道。
avPlayer.deselectTrack(parseInt(audioTrackIndex.toString()));
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/playback-url-setting-method-V14
爬取时间: 2025-04-28 20:33:13
来源: Huawei Developer
本开发指导将介绍如何使用AVPlayer开发播放功能，在不同的场景下如何设置URL。
当前指导仅介绍播放URL设置方法，其他场景及完整示例代码，请参考视频播放。
当前开发指导将提供以下设置播放URL的方法：
流媒体播放场景下设置URL
情况一：播放HTTP/HTTPS媒体资源
```typescript
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
// 设置对应的播放url
avPlayer.url = 'https://xxx.xxx.xxx.mp4';
```
情况二：HLS媒体资源播放(点播/直播)
```typescript
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
// 设置对应的播放url
avPlayer.url = 'https://xxx.xxx.xxx.xxx:xx/xx/index.m3u8';
```
情况三：设置HTTP请求头信息播放
当服务器需要校验HTTP请求头信息时，可通过createMediaSourceWithUrl设置HTTP请求头信息。
```typescript
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
// 创建mediaSource实例对象，设置媒体来源，定制HTTP请求，如需要，可以键值对的形式设置User-Agent、Cookie、Referer等字段
let mediaSource : media.MediaSource = media.createMediaSourceWithUrl("https://xxx.xxx.xxx.xxx:xx/xx/index.m3u8",  {"User-Agent" : "User-Agent-Value", "Cookie" : "Cookie-Value", "Referer" : "Referer-Value"});
// 设置播放策略，设置缓冲区数据量为20s
let playbackStrategy : media.PlaybackStrategy = {preferredBufferDuration: 20};
// 为avPlayer设置媒体来源和播放策略
avPlayer.setMediaSource(mediaSource, playbackStrategy);
```
情况四：通过本地Raw文件中的m3u8文件播放在线流媒体资源
当应用需要通过解析本地Raw文件中的m3u8文件，播放在线流媒体资源时，可以通过resourceManager.getRawFd获取文件描述符，将其拼接成fdUrl，并通过setMimeType设置MIME类型为APPLICATION_M3U8。
```typescript
import { resourceManager } from '@kit.LocalizationKit';
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
// 获取context实例
let context = getContext(this) as common.UIAbilityContext;
let mgr = context.resourceManager;
// 设置本地m3u8文件名
let m3u8FileName : string = "xxx.m3u8";
// 通过本地m3u8文件名，获取文件描述符
let fileDescriptor = await mgr.getRawFd(m3u8FileName);
// 用文件描述符构造本地m3u8的URL
let fd : string = fileDescriptor.fd.toString();
let offset : string = fileDescriptor.offset.toString();
let length : string = fileDescriptor.length.toString();
let fdUrl : string = "fd://" + fd + "?offset=" + offset + "&size=" + length;
// 按需设置HTTP请求头
let headers : Record<string,string> = {"User-Agent" : "User-Agent-Value", "Cookie" : "Cookie-Value"};
// 通过本地m3u8的URL和HTTP请求头构造mediaSource媒体来源
let mediaSource : media.MediaSource = media.createMediaSourceWithUrl(fdUrl, headers);
// 设置媒体MIME类型为APPLICATION_M3U8
let mimeType : media.AVMimeTypes = media.AVMimeTypes.APPLICATION_M3U8;
mediaSource.setMimeType(mimeType);
// 设置播放策略，设置缓冲区数据量为20s
let playbackStrategy : media.PlaybackStrategy = {preferredBufferDuration: 20};
// 为avPlayer设置媒体来源和播放策略
avPlayer.setMediaSource(mediaSource, playbackStrategy);
```
情况五：通过应用沙箱中的m3u8文件播放在线流媒体资源
当应用需要通过解析应用沙箱中的的m3u8文件，播放在线流媒体资源时，可以通过fs.openSync获取文件句柄，将其拼接成fdUrl，并通过setMimeType设置MIME类型为APPLICATION_M3U8。
```typescript
import { fileIo as fs } from '@kit.CoreFileKit';
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
// 获取context实例
let context = getContext(this) as common.UIAbilityContext;
let mgr = context.resourceManager;
// 设置本地m3u8文件名
let m3u8FileName : string = "xxx.m3u8";
// 设置本地m3u8沙箱路径
let filePath = "/data/storage/el1/bundle/${m3u8FileName}";
// 通过fs.openSync获取文件句柄
let file = fs.openSync(filePath, fs.OpenMode.READ_ONLY);
let fd : string = file.fd.toString();
// 用文件句柄构造本地m3u8的URL
let fdUrl : string = "fd://" + fd + "?offset=" + "0" + "&size=" + "0";
// 按需设置HTTP请求头
let headers : Record<string,string> = {"User-Agent" : "User-Agent-Value", "Cookie" : "Cookie-Value"};
// 通过本地m3u8的URL和HTTP请求头构造mediaSource媒体来源
let mediaSource : media.MediaSource = media.createMediaSourceWithUrl(fdUrl, headers);
// 设置媒体MIME类型为APPLICATION_M3U8
let mimeType : media.AVMimeTypes = media.AVMimeTypes.APPLICATION_M3U8;
mediaSource.setMimeType(mimeType);
// 设置播放策略，设置缓冲区数据量为20s
let playbackStrategy : media.PlaybackStrategy = {preferredBufferDuration: 20};
// 为avPlayer设置媒体来源和播放策略
avPlayer.setMediaSource(mediaSource, playbackStrategy);
```
本地raw文件播放场景下设置URL
情况一：应用沙箱文件播放
```typescript
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
let fdPath = 'fd://';
// 通过UIAbilityContext获取沙箱地址filesDir，以Stage模型为例
let context = getContext(this) as common.UIAbilityContext;
let pathDir = context.filesDir;
let path = '/data/storage/el1/bundle/01.mp3';
// 打开相应的资源文件地址获取fd，并为url赋值触发initialized状态机上报
let file = await fs.open(path);
fdPath = fdPath + '' + file.fd;
avPlayer.url = fdPath;
```
情况二：本地文件播放
```typescript
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
// 通过UIAbilityContext的resourceManager成员的getRawFd接口获取媒体资源播放地址
// 返回类型为{fd,offset,length},fd为HAP包fd地址，offset为媒体资源偏移量，length为播放长度
let context = getContext(this) as common.UIAbilityContext;
let fileDescriptor = await context.resourceManager.getRawFd('01.mp3');
let avFileDescriptor: media.AVFileDescriptor =
{ fd: fileDescriptor.fd, offset: fileDescriptor.offset, length: fileDescriptor.length };
// 为fdSrc赋值触发initialized状态机上报
avPlayer.fdSrc = avFileDescriptor;
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/streaming-media-playback-development-guide-V14
爬取时间: 2025-04-28 20:33:26
来源: Huawei Developer
本开发指导将介绍如何使用AVPlayer开发流媒体播放功能，以完整地播放一个流媒体视频作为示例，实现端到端播放流媒体资源。
当前指导仅介绍如何实现流媒体播放功能，本地音视频播放等其他场景，请参考视频播放。
流媒体支持的格式
| 流媒体协议类型 | 典型链接 | 网络点播 | 网络直播 | 内容保护 |
| --- | --- | --- | --- | --- |
| HLS | https://xxxx/index.m3u8 | 支持 | 支持 | 支持，详见DRM Kit。 |
| DASH | https://xxxx.mpd | 支持 | - | 支持，详见DRM Kit。 |
| HTTP/HTTPS | https://xxxx.mp4 | 支持 | - | - |
| HTTP-FLV | https://xxxx.flv | 支持 | 支持 | - |
开发步骤
创建AVPlayer，设置播放资源和窗口，设置播放参数（音量/倍速/缩放模式），播放控制（播放/暂停/跳转/停止），重置，销毁资源。在进行应用开发的过程中，开发者可以通过AVPlayer的state属性主动获取当前状态或使用on('stateChange')方法监听状态变化。如果应用在视频播放器处于错误状态时执行操作，系统可能会抛出异常或生成其他未定义的行为。状态的详细说明请参考AVPlayerState。具体的开发步骤如下：
1.  创建实例createAVPlayer()，AVPlayer初始化idle状态。
2.  设置业务需要的监听事件，搭配全流程场景使用。支持的监听事件包括： 响应API调用，监听seek()请求完成情况。 当使用seek()跳转到指定播放位置后，如果seek操作成功，将上报该事件。 响应API调用，监听setSpeed()请求完成情况。 当使用setSpeed()设置播放倍速后，如果setSpeed操作成功，将上报该事件。 响应API调用，监听setVolume()请求完成情况。 当使用setVolume()调节播放音量后，如果setVolume操作成功，将上报该事件。 监听音频焦点切换信息，搭配属性audioInterruptMode使用。 如果当前设备存在多个音频正在播放，音频焦点被切换（即播放其他媒体如通话等）时将上报该事件，应用可以及时处理。
3.  设置资源：使用AVPlayer设置播放URL，AVPlayer进入initialized状态。 下面代码示例中的url仅作示意使用，开发者需根据实际情况，确认资源有效性并设置： 使用网络播放路径，需声明权限：ohos.permission.INTERNET。 需要使用支持的播放格式与协议。
4.  使用网络播放路径，需声明权限：ohos.permission.INTERNET。
5.  需要使用支持的播放格式与协议。
6.  设置窗口：获取并设置属性SurfaceID，用于设置显示画面。 应用需要从XComponent组件获取surfaceID，获取方式请参考XComponent。
7.  准备播放：调用prepare()，AVPlayer进入prepared状态，此时可以获取duration，设置缩放模式、音量等。
8.  视频播控：播放play()，暂停pause()，跳转seek()，停止stop() 等操作。
9.  （可选）更换资源：调用reset()重置资源，AVPlayer重新进入idle状态，允许更换资源url。
10.  退出播放：调用release()销毁实例，AVPlayer进入released状态，退出播放。
| 事件类型 | 说明 |
| --- | --- |
| stateChange | 必要事件，监听播放器的state属性改变。 |
| error | 必要事件，监听播放器的错误信息。 |
| durationUpdate | 用于进度条，监听进度条长度，刷新资源时长。 |
| timeUpdate | 用于进度条，监听进度条当前位置，刷新当前时间。 |
| seekDone | 响应API调用，监听seek()请求完成情况。 当使用seek()跳转到指定播放位置后，如果seek操作成功，将上报该事件。 |
| speedDone | 响应API调用，监听setSpeed()请求完成情况。 当使用setSpeed()设置播放倍速后，如果setSpeed操作成功，将上报该事件。 |
| volumeChange | 响应API调用，监听setVolume()请求完成情况。 当使用setVolume()调节播放音量后，如果setVolume操作成功，将上报该事件。 |
| bufferingUpdate | 用于网络播放，监听网络播放缓冲信息，用于上报缓冲百分比以及缓存播放进度。 |
| audioInterrupt | 监听音频焦点切换信息，搭配属性audioInterruptMode使用。 如果当前设备存在多个音频正在播放，音频焦点被切换（即播放其他媒体如通话等）时将上报该事件，应用可以及时处理。 |
-  使用网络播放路径，需声明权限：ohos.permission.INTERNET。
-  需要使用支持的播放格式与协议。
注意事项
播放流媒体的标准流程如上述开发步骤所示，但使用不同的流媒体格式在实际开发的过程中还是会存在一定差异，本节将详细描述不同流媒体格式业务的差异，包括设置视频起播策略、切换音视频轨道等。
流媒体缓冲状态
当下载速率低于片源的码率时，可能会出现卡顿，此时播放器检测到缓冲区数据不足，会先缓冲一些数据再播放，避免连续卡顿。一次卡顿对应的缓冲事件上报过程为：BUFFERING_START-> BUFFERING_PERCENT 0 -> ... -> BUFFERING_PERCENT 100 -> BUFFERING_END。而CACHED_DURATION无论是卡顿过程中还是播放过程中，都会持续上报，直至下载至资源末尾。详见BufferingInfoType缓冲事件类型枚举。
监听当前bufferingUpdate缓冲状态示例代码：
```typescript
avPlayer.on('bufferingUpdate', (infoType : media.BufferingInfoType, value : number) => {
console.info(`AVPlayer bufferingUpdate, infoType is ${infoType}, value is ${value}.`);
})
```
HLS切换码率
当前流媒体HLS协议流支持多码率播放，默认情况下，播放器会根据网络下载速度选择合适的码率。
1.  通过on('availableBitrates')监听当前HLS协议流可用的码率，若监听的码率列表长度为0，则不支持设置指定码率。
```typescript
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
// 监听当前HLS协议流可用的码率
avPlayer.on('availableBitrates', (bitrates: Array<number>) => {
console.info('availableBitrates called, and availableBitrates length is: ' + bitrates.length);
})
```
2.  通过setBitrate接口设置播放码率，若用户设置的码率不在可用码率中，播放器将从可用码率中选择最小且最接近的码率。该接口只能在prepared/playing/paused/completed状态下调用，可通过监听bitrateDone事件确认是否生效。
```typescript
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
// 监听码率设置是否生效
avPlayer.on('bitrateDone', (bitrate: number) => {
console.info('bitrateDone called, and bitrate value is: ' + bitrate);
})
// 设置播放码率
let bitrate: number = 96000;
avPlayer.setBitrate(bitrate);
```
DASH设置视频起播策略
为了保证在弱网环境下的播放体验，AVPlayer会默认选择最低的视频分辨率开始播放，随后依据网络状况自动调整。开发者可根据实际需求，自定义DASH视频的起播策略，包括设定视频的宽度、高度以及色彩格式等参数。
以调节视频起播分辨率为例，下述示例代码描述了设置视频宽度1920px、高度1080px起播。此时，AVPlayer会选择MPD资源中一路分辨率为1920x1080的视频资源进行播放。
```typescript
let mediaSource : media.MediaSource = media.createMediaSourceWithUrl("http://test.cn/dash/aaa.mpd",  {"User-Agent" : "User-Agent-Value"});
let playbackStrategy : media.PlaybackStrategy = {preferredWidth: 1920, preferredHeight: 1080};
avPlayer.setMediaSource(mediaSource, playbackStrategy);
```
DASH切换音视频轨道
DASH流媒体资源一般包含多路分辨率、码率、采样率、编码格式等参数各不相同的音频、视频和字幕资源。默认情况下，AVPlayer会依据网络状况自动切换不同码率的视频轨道。开发者可根据实际需求，自主选择指定的音视频轨道进行播放，此时自适应码率切换策略会失效。
1.  设置selectTrack生效的监听事件trackChange。
```typescript
avPlayer.on('trackChange', (index: number, isSelect: boolean) => {
console.info(`trackChange info, index: ${index}, isSelect: ${isSelect}`);
})
```
2.  调用getTrackDescription获取所有音视频轨道列表。开发者可根据实际需求，基于MediaDescription各字段信息，确定目标轨道索引。
```typescript
// 以获取1080p视频轨道索引为例
public videoTrackIndex: number;
avPlayer.getTrackDescription((error: BusinessError, arrList: Array<media.MediaDescription>) => {
if (arrList != null) {
for (let i = 0; i < arrList.length; i++) {
let propertyIndex: Object = arrList[i][media.MediaDescriptionKey.MD_KEY_TRACK_INDEX];
let propertyType: Object = arrList[i][media.MediaDescriptionKey.MD_KEY_TRACK_TYPE];
let propertyWidth: Object = arrList[i][media.MediaDescriptionKey.MD_KEY_WIDTH];
let propertyHeight: Object = arrList[i][media.MediaDescriptionKey.MD_KEY_HEIGHT];
if (propertyType == media.MediaType.MEDIA_TYPE_VID && propertyWidth == 1920 && propertyHeight == 1080) {
videoTrackIndex = parseInt(propertyIndex.toString()); // 获取1080p视频轨道索引
}
}
} else {
console.error(`getTrackDescription fail, error:${error}`);
}
});
```
3.  在音视频播放过程中调用selectTrack选择对应的音视频轨道，或者调用deselectTrack取消选择的音视频轨道。
```typescript
// 切换至目标视频轨道
avPlayer.selectTrack(videoTrackIndex);
// 取消选择目标视频轨道
// avPlayer.deselectTrack(videoTrackIndex);
```
异常场景说明
使用avPlayer播放流媒体过程中断网：流媒体模块会根据返回的错误码、服务器请求失败的响应时间、请求次数等因素综合处理。若错误码类型属于不进行请求重试的类型，会向应用上报对应的错误码。若错误码类型需要进行请求重试，会在30s内进行至多10次的请求重试。若请求重试次数超过10次，或重试总时长超过30秒，会上向应用上报对应的错误码。若请求重试成功，则继续播放。
完整示例
参考以下示例，完整地播放一个流媒体视频。
```typescript
import { media } from '@kit.MediaKit';
import { fileIo as fs } from '@kit.CoreFileKit';
import { common } from '@kit.AbilityKit';
import { BusinessError } from '@kit.BasicServicesKit';
export class AVPlayerDemo {
private count: number = 0;
private surfaceID: string = ''; // surfaceID用于播放画面显示，具体的值需要通过Xcomponent接口获取，相关文档链接见上面Xcomponent创建方法
private isSeek: boolean = true; // 用于区分模式是否支持seek操作
public audioTrackList: number[] = [];
public videoTrackList: number[] = [];
constructor(surfaceID: string) {
this.surfaceID = surfaceID;
}
// 注册avplayer回调函数
setAVPlayerCallback(avPlayer: media.AVPlayer) {
// startRenderFrame首帧渲染回调函数
avPlayer.on('startRenderFrame', () => {
console.info(`AVPlayer start render frame`);
});
// seek操作结果回调函数
avPlayer.on('seekDone', (seekDoneTime: number) => {
console.info(`AVPlayer seek succeeded, seek time is ${seekDoneTime}`);
})
// avPlayer.on('trackChange', (index: number, isSelect: boolean) => {
//   console.info(`AVPlayer track changed, track index: ${index}, isSelect: ${isSelect}`);
// })
// error回调监听函数,当avPlayer在操作过程中出现错误时调用 reset接口触发重置流程
avPlayer.on('error', (err: BusinessError) => {
console.error(`Invoke avPlayer failed, code is ${err.code}, message is ${err.message}`);
avPlayer.reset(); // 调用reset重置资源，触发idle状态
})
// 状态机变化回调函数
avPlayer.on('stateChange', async (state: string, reason: media.StateChangeReason) => {
switch (state) {
case 'idle': // 成功调用reset接口后触发该状态机上报
console.info('AVPlayer state idle called.');
avPlayer.release(); // 调用release接口销毁实例对象
break;
case 'initialized': // avplayer 设置播放源后触发该状态上报
console.info('AVPlayer state initialized called.');
avPlayer.surfaceId = this.surfaceID; // 设置显示画面，当播放的资源为纯音频时无需设置
avPlayer.prepare();
break;
case 'prepared': // prepare调用成功后上报该状态机
console.info('AVPlayer state prepared called.');
avPlayer.play(); // 调用播放接口开始播放
break;
case 'playing': // play成功调用后触发该状态机上报
console.info('AVPlayer state playing called.');
break;
case 'paused': // pause成功调用后触发该状态机上报
console.info('AVPlayer state paused called.');
break;
case 'completed': // 播放结束后触发该状态机上报
console.info('AVPlayer state completed called.');
avPlayer.stop(); //调用播放结束接口
break;
case 'stopped': // stop接口成功调用后触发该状态机上报
console.info('AVPlayer state stopped called.');
avPlayer.reset(); // 调用reset接口初始化avplayer状态
break;
case 'released':
console.info('AVPlayer state released called.');
break;
default:
console.info('AVPlayer state unknown called.');
break;
}
})
// 监听流媒体缓冲状态、缓冲百分比、已缓冲数据预估可播放时长
avPlayer.on('bufferingUpdate', (infoType : media.BufferingInfoType, value : number) => {
console.info(`AVPlayer bufferingUpdate, infoType is ${infoType}, value is ${value}.`);
})
}
// 以下demo为通过url设置网络地址来实现播放流媒体HLS点播视频
async avPlayerVodDemo() {
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
// 创建状态机变化回调函数
this.setAVPlayerCallback(avPlayer);
this.isSeek = true; // 点播支持seek操作
avPlayer.url = 'http://xxx.xxx.xxx.xxx:xx/xx/index.m3u8';
}
// 以下demo为通过url设置网络地址来实现播放流媒体HLS直播视频
async avPlayerLiveDemo() {
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
// 创建状态机变化回调函数
this.setAVPlayerCallback(avPlayer);
this.isSeek = false; // 直播不支持seek操作
avPlayer.url = 'http://xxx.xxx.xxx.xxx:xx/xx/index.m3u8';
}
// 以下demo为通过url设置网络地址来实现播放Dash流媒体视频
async avPlayerDashDemo() {
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
// 创建状态机变化回调函数
this.setAVPlayerCallback(avPlayer);
// 设置播放偏好策略
// let mediaSource : media.MediaSource = media.createMediaSourceWithUrl("http://test.cn/dash/aaa.mpd",  {"User-Agent" : "User-Agent-Value"});
// let playbackStrategy : media.PlaybackStrategy = {preferredWidth: 1, preferredHeight: 2, preferredBufferDuration: 3, preferredHdr: false};
// avPlayer.setMediaSource(mediaSource, playbackStrategy);
this.isSeek = true; // 表示支持seek操作
avPlayer.url = 'http://test.cn/dash/aaa.mpd'; //须替换为DASH资源实际地址
// 通过selectTrack设置音频/视频轨道，通过deselectTrack取消上次设置的音频/视频轨道并恢复到默认音频/视频轨道
avPlayer.getTrackDescription((error: BusinessError, arrList: Array<media.MediaDescription>) => {
if (arrList != null) {
for (let i = 0; i < arrList.length; i++) {
let propertyIndex: Object = arrList[i][media.MediaDescriptionKey.MD_KEY_TRACK_INDEX];
let propertyType: Object = arrList[i][media.MediaDescriptionKey.MD_KEY_TRACK_TYPE];
if (propertyType == 0) {
this.audioTrackList.push(parseInt(propertyIndex.toString())); // 获取音频轨道列表
} else if (propertyType == 1) {
this.videoTrackList.push(parseInt(propertyIndex.toString())); // 获取视频轨道列表
}
}
} else {
console.error(`getTrackDescription fail, error:${error}`);
}
});
// 选择其中一个视频轨道
// avPlayer.selectTrack(this.videoTrackList[0]);
// 取消选择的视频轨道
// avPlayer.deselectTrack(this.videoTrackList[0]);
}
// 以下demo为通过setMediaSource设置自定义头域及媒体播放优选参数实现初始播放参数设置，以流媒体Https点播为例
async preDownloadDemo() {
// 创建avPlayer实例对象
let avPlayer: media.AVPlayer = await media.createAVPlayer();
// 创建状态机变化回调函数
this.setAVPlayerCallback(avPlayer);
this.isSeek = true; // 点播支持seek操作
// 创建mediaSource实例对象，设置媒体来源，定制HTTP请求，如需要，可以键值对的形式设置User-Agent、Cookie、Referer等字段
let mediaSource : media.MediaSource = media.createMediaSourceWithUrl("https://xxx.xxx",  {"User-Agent" : "User-Agent-Value", "Cookie" : "Cookie-Value", "Referer" : "Referer-Value"});
// 设置播放策略，设置缓冲区数据量为20s
let playbackStrategy : media.PlaybackStrategy = {preferredBufferDuration: 20};
// 为avPlayer设置媒体来源和播放策略
avPlayer.setMediaSource(mediaSource, playbackStrategy);
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/video-subtitle-V14
爬取时间: 2025-04-28 20:33:40
来源: Huawei Developer
当前仅支持视频播放前设置外挂字幕。
在进行应用开发的过程中，开发者可以通过AVPlayer的实例注册on('subtitleUpdate')方法监听字幕信息。
开发步骤及注意事项
详细的API说明请参考AVPlayer
1.  使用视频播放的AVPlayer实例设置外挂字幕资源。
```typescript
let context = getContext(this) as common.UIAbilityContext;
let fileDescriptor = await context.resourceManager.getRawFd('xxx.srt');
avPlayer.addSubtitleFromFd(fileDescriptor.fd, fileDescriptor.offset, fileDescriptor.length);
// 或者使用addSubtitleFromUrl接口
let fdUrl:string = "http://xxx.xxx.xxx.xxx:xx/xx/index.srt" ;
avPlayer.addSubtitleFromUrl(fdUrl);
```
2.  使用视频播放的AVPlayer实例注册字幕回调函数。
```typescript
avPlayer.on('subtitleUpdate', (info: media.SubtitleInfo) => {
if (!!info) {
let text = (!info.text) ? '' : info.text;
let startTime = (!info.startTime) ? 0 : info.startTime;
let duration = (!info.duration) ? 0 : info.duration;
console.info('subtitleUpdate info: text=' + text + ' startTime=' + startTime +' duration=' + duration);
} else {
console.info('subtitleUpdate info is null');
}
});
```
3.  (可选)当需要不显示字幕的时候，使用视频播放的AVPlayer实例注销字幕回调函数。
```typescript
avPlayer.off('subtitleUpdate');
```
完整示例
```typescript
import { media } from '@kit.MediaKit';
import { common } from '@kit.AbilityKit';
import { BusinessError } from '@kit.BasicServicesKit';
export class AVPlayerSubtitleDemo {
private avPlayer: media.AVPlayer | undefined = undefined;
// 注册avplayer回调函数
setAVPlayerCallback(avPlayer: media.AVPlayer) {
// error回调监听函数,当avPlayer在操作过程中出现错误时调用reset接口触发重置流程
avPlayer.on('error', (err: BusinessError) => {
console.error(`Invoke avPlayer failed, code is ${err.code}, message is ${err.message}`);
avPlayer.reset(); // 调用reset重置资源，触发idle状态
});
// 注册字幕回调函数
avPlayer.on('subtitleUpdate', (info: media.SubtitleInfo) => {
if (info) {
let text = (!info.text) ? '' : info.text;
let startTime = (!info.startTime) ? 0 : info.startTime;
let duration = (!info.duration) ? 0 : info.duration;
console.info('subtitleUpdate info: text=' + text + ' startTime=' + startTime +' duration=' + duration);
} else {
console.info('subtitleUpdate info is null');
}
});
}
// 以下demo为使用资源管理接口获取打包在HAP内的媒体资源文件并通过url属性设置
async avPlayerSubtitleUrlDemo() {
// 创建avPlayer实例对象
this.avPlayer = await media.createAVPlayer();
// 设置视频信息
// 创建回调函数
this.setAVPlayerCallback(this.avPlayer);
let fdUrl:string = "http://xxx.xxx.xxx.xxx:xx/xx/index.srt";
this.avPlayer.addSubtitleFromUrl(fdUrl);
}
// 以下demo为使用资源管理接口获取打包在HAP内的媒体资源文件并通过FromFd属性设置
async avPlayerSubtitleFromFdDemo() {
// 创建avPlayer实例对象
this.avPlayer = await media.createAVPlayer();
// 设置视频信息
// 创建回调函数
this.setAVPlayerCallback(this.avPlayer);
let context = getContext(this) as common.UIAbilityContext;
let fileDescriptor = await context.resourceManager.getRawFd('xxx.srt');
this.avPlayer.addSubtitleFromFd(fileDescriptor.fd, fileDescriptor.offset, fileDescriptor.length);
}
// 注销字幕回调函数
async avPlayerSubtitleOffDemo() {
if(this.avPlayer) {
this.avPlayer.off('subtitleUpdate');
}
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/using-soundpool-for-playback-V14
爬取时间: 2025-04-28 20:33:53
来源: Huawei Developer
使用SoundPool（音频池）提供的接口，可以实现低时延短音播放。
当应用开发时，经常需要使用一些急促简短的音效（如相机快门音效、系统通知音效等），此时建议调用SoundPool，实现一次加载，多次低时延播放。
SoundPool当前支持播放1MB以下的音频资源，大小超过1MB的长音频将截取1MB大小数据进行播放。
本开发指导将以SoundPool进行一次低时延播放音频的过程为例，向开发者讲解如何使用SoundPool。详细的API声明请参考SoundPool API参考。
过程包括：创建SoundPool实例，加载音频资源（包括资源的解封装与解码:解码格式参考音频解码支持），设置播放参数（循环模式/播放优先级等），播放控制（播放/停止），释放资源。
在应用开发过程中，开发者应通过监听方法检查当前播放状态并按照一定顺序调用接口，执行对应操作，否则系统可能会抛出异常或生成其他未定义的行为。具体顺序可参考下列开发步骤及对应说明。
使用SoundPool播放短音频时，涉及音频焦点管控策略的问题，请参考音频焦点指南。
开发步骤及注意事项
1.  调用createSoundPool方法创建SoundPool实例。
```typescript
import { media } from '@kit.MediaKit';
import { audio } from '@kit.AudioKit';
import { BusinessError } from '@kit.BasicServicesKit';
let soundPool: media.SoundPool;
// audioRenderInfo中的参数usage取值为STREAM_USAGE_UNKNOWN，STREAM_USAGE_MUSIC，STREAM_USAGE_MOVIE，
// STREAM_USAGE_AUDIOBOOK时，SoundPool播放短音时为混音模式，不会打断其他音频播放。
let audioRendererInfo: audio.AudioRendererInfo = {
usage : audio.StreamUsage.STREAM_USAGE_MUSIC,
rendererFlags : 0
};
media.createSoundPool(5, audioRendererInfo).then((soundpool_: media.SoundPool) => {
if (soundpool_ != null) {
soundPool = soundpool_;
console.info('create SoundPool success');
} else {
console.error('create SoundPool fail');
}
}).catch((error: BusinessError) => {
console.error(`soundpool catchCallback, error message:${error.message}`);
});
```
2.  调用on('loadComplete')方法，用于监听“资源加载完成”。
```typescript
soundPool.on('loadComplete', (soundId: number) => {
console.info('loadComplete, soundId: ' + soundId);
});
```
3.  调用on('playFinished')方法，用于监听“播放完成”。
```typescript
soundPool.on('playFinished', () => {
console.info("receive play finished message");
});
```
4.  调用on('error')方法，设置错误类型监听。
```typescript
soundPool.on('error', (error: BusinessError) => {
console.info('error happened,message is :' + error.message);
});
```
5.  调用load方法进行音频资源加载。 可以传入uri或fd加载资源，此处使用传入uri的方式为例，更多方法请参考API文档。 当系统加载完毕音频资源文件的时候，会通过loadComplete回调，通知用户资源加载完成，请在收到回调之后，再进行后续的play操作。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
import { fileIo as fs } from '@kit.CoreFileKit';
let soundID: number;
let uri: string;
async function load() {
await fs.open('/test_01.mp3', fs.OpenMode.READ_ONLY).then((file: fs.File) => {
console.info("file fd: " + file.fd);
uri = 'fd://' + (file.fd).toString()
}); // '/test_01.mp3' 作为样例，使用时需要传入文件对应路径。
soundPool.load(uri).then((soundId: number) => {
console.info('soundPool load uri success');
soundID = soundId;
}).catch((err: BusinessError) => {
console.error('soundPool load failed and catch error is ' + err.message);
})
}
```
6.  配置播放参数PlayParameters，并在收到loadComplete回调通知之后，调用play方法播放音频。多次调用play播放同一个soundID，只会播放一次。
```typescript
let soundID: number;
let streamID: number;
let playParameters: media.PlayParameters = {
loop: 0, // 循环0次
rate: 2, // 2倍速
leftVolume: 0.5, // range = 0.0-1.0
rightVolume: 0.5, // range = 0.0-1.0
priority: 0, // 最低优先级
};
soundPool.play(soundID, playParameters, (error: BusinessError, streamId: number) => {
if (error) {
console.info(`play sound Error: errCode is ${error.code}, errMessage is ${error.message}`)
} else {
streamID = streamId;
console.info('play success soundid:' + streamId);
}
});
```
7.  调用setLoop方法设置循环次数。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
let streamID: number;
soundPool.setLoop(streamID, 1).then(() => {
console.info('setLoop success streamID:' + streamID);
}).catch((err: BusinessError) => {
console.error('soundpool setLoop failed and catch error is ' + err.message);
});
```
8.  调用setPriority方法设置优先级。
```typescript
let streamID: number;
soundPool.setPriority(streamID, 1);
```
9.  调用setVolume方法设置音量。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
let streamID: number;
// 先调用play方法获取到对应资源的streamID
soundPool.setVolume(streamID, 0.5, 0.5).then(() => {
console.info('setVolume success');
}).catch((err: BusinessError) => {
console.error('soundpool setVolume failed and catch error is ' + err.message);
});
```
10.  调用stop方法终止指定流的播放。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
let streamID: number;
//先调用play方法给拿到对应的streamID
soundPool.stop(streamID).then(() => {
console.info('stop success');
}).catch((err: BusinessError) => {
console.error('soundpool load stop and catch error is ' + err.message);
});
```
11.  调用unload方法卸载音频资源。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
let soundID: number;
// 先调用load方法获取到对应资源的soundID
soundPool.unload(soundID).then(() => {
console.info('unload success');
}).catch((err: BusinessError) => {
console.error('soundpool unload failed and catch error is ' + err.message);
});
```
12.  调用off('loadComplete')方法注销加载完成监听。
```typescript
soundPool.off('loadComplete');
```
13.  调用off('playFinished')方法注销播放完成监听。
```typescript
soundPool.off('playFinished');
```
14.  调用off('error')方法注销错误错误类型监听。
```typescript
soundPool.off('error');
```
15.  调用release方法释放SoundPool实例。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
soundPool.release().then(() => {
console.info('release success');
}).catch((err: BusinessError) => {
console.error('soundpool release failed and catch error is ' + err.message);
});
```
完整示例
下面展示了使用SoundPool进行低时延播放的完整示例代码。
```typescript
import { audio } from '@kit.AudioKit';
import { media } from '@kit.MediaKit';
import { fileIo as fs } from '@kit.CoreFileKit';
import { BusinessError } from '@kit.BasicServicesKit';
let soundPool: media.SoundPool;
let streamId: number = 0;
let soundId: number = 0;
// audioRenderInfo中的参数usage取值为STREAM_USAGE_UNKNOWN，STREAM_USAGE_MUSIC，STREAM_USAGE_MOVIE，
// STREAM_USAGE_AUDIOBOOK时，SoundPool播放短音时为混音模式，不会打断其他音频播放。
let audioRendererInfo: audio.AudioRendererInfo = {
usage: audio.StreamUsage.STREAM_USAGE_MUSIC,
rendererFlags: 1
};
let playParameters: media.PlayParameters = {
loop: 3, // 循环4次
rate: audio.AudioRendererRate.RENDER_RATE_NORMAL, // 正常倍速
leftVolume: 0.5, // range = 0.0-1.0
rightVolume: 0.5, // range = 0.0-1.0
priority: 0, // 最低优先级
};
let uri: string = "";
async function create() {
//创建soundPool实例
soundPool = await media.createSoundPool(5, audioRendererInfo);
//注册监听
loadCallback();
finishPlayCallback();
setErrorCallback();
// 加载音频资源
await fs.open('/test_01.mp3', fs.OpenMode.READ_ONLY).then((file: fs.File) => {
console.info("file fd: " + file.fd);
uri = 'fd://' + (file.fd).toString()
}); // '/test_01.mp3' 作为样例，使用时需要传入文件对应路径。
soundId = await soundPool.load(uri);
}
function loadCallback() {
// 加载完成回调
soundPool.on('loadComplete', (soundId_: number) => {
console.info('loadComplete, soundId: ' + soundId_);
})
}
//设置播放完成监听
function finishPlayCallback() {
// 播放完成回调
soundPool.on('playFinished', () => {
console.info("receive play finished message");
// 可进行下次播放
})
}
//设置错误类型监听
function setErrorCallback() {
soundPool.on('error', (error: BusinessError) => {
console.info('error happened,message is :' + error.message);
})
}
async function PlaySoundPool() {
// 开始播放，这边play也可带播放播放的参数PlayParameters，请在音频资源加载完毕，即收到loadComplete回调之后再执行play操作
soundPool.play(soundId, playParameters, (error, streamID: number) => {
if (error) {
console.info(`play sound Error: errCode is ${error.code}, errMessage is ${error.message}`)
} else {
streamId = streamID;
console.info('play success soundid:' + streamId);
}
});
// 设置循环播放次数
await soundPool.setLoop(streamId, 2); // 播放3次
// 设置对应流的优先级
await soundPool.setPriority(streamId, 1);
// 设置音量
await soundPool.setVolume(streamId, 0.5, 0.5);
}
async function release() {
// 终止指定流的播放
await soundPool.stop(streamId);
// 卸载音频资源
await soundPool.unload(soundId);
//关闭监听
setOffCallback();
// 释放SoundPool
await soundPool.release();
}
//关闭监听
function setOffCallback() {
soundPool.off('loadComplete');
soundPool.off('playFinished');
soundPool.off('error');
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/media-recording-arkts-V14
爬取时间: 2025-04-28 20:34:07
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/using-avrecorder-for-recording-V14
爬取时间: 2025-04-28 20:34:21
来源: Huawei Developer
使用AVRecorder可以实现音频录制功能，本开发指导将以“开始录制-暂停录制-恢复录制-停止录制”的一次流程为示例，向开发者讲解AVRecorder音频录制相关功能。
在进行应用开发的过程中，开发者可以通过AVRecorder的state属性，主动获取当前状态或使用on('stateChange')方法监听状态变化。开发过程中应该严格遵循状态机要求，例如只能在started状态下调用pause()接口，只能在paused状态下调用resume()接口。
图1录制状态变化示意图
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165954.41837787923638342706732501772896:50001231000000:2800:DCD8E6647213FCCC3E12C6F4FE9C83B0E540BB492A7D59FD21AC8A741EDA6C50.png)
状态的详细说明请参考AVRecorderState。
申请权限
在开发此功能前，开发者应根据实际需求申请相关权限：
仅应用需要克隆、备份或同步用户公共目录的音频类文件时，可申请ohos.permission.READ_AUDIO、ohos.permission.WRITE_AUDIO权限来读写音频文件，申请方式请参考申请受控权限，通过AGC审核后才能使用。为避免应用的上架申请被驳回，开发者应优先使用Picker/控件等替代方案，仅少量符合特殊场景的应用被允许申请受限权限。
开发步骤及注意事项
详细的API说明请参考AVRecorder API参考。
1.  创建AVRecorder实例，实例创建完成进入idle状态。 需要在avRecorder完成赋值（即“avRecorder = recorder; ”运行完成）后，再进行剩余操作。
```typescript
import { media } from '@kit.MediaKit';
import { BusinessError } from '@kit.BasicServicesKit';
let avRecorder: media.AVRecorder;
media.createAVRecorder().then((recorder: media.AVRecorder) => {
avRecorder = recorder;
}, (error: BusinessError) => {
console.error(`createAVRecorder failed`);
})
```
2.  设置业务需要的监听事件，监听状态变化及错误上报。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
// 状态上报回调函数
this.avRecorder.on('stateChange', (state: media.AVRecorderState, reason: media.StateChangeReason) => {
console.log(`current state is ${state}`);
// 用户可以在此补充状态发生切换后想要进行的动作
})
// 错误上报回调函数
this.avRecorder.on('error', (err: BusinessError) => {
console.error(`avRecorder failed, code is ${err.code}, message is ${err.message}`);
})
```
3.  配置音频录制参数，调用prepare()接口，此时进入prepared状态。 配置参数需要注意： 配置参数之前需要确保完成对应权限的申请，请参考申请权限。 prepare接口的入参avConfig中仅设置音频相关的配置参数，如示例代码所示。 如果只需要录制音频，请不要设置视频相关配置参数；如果需要录制视频，可以参考视频录制开发指导进行开发。直接设置视频相关参数会导致后续步骤报错。 需要使用支持的录制规格。 录制输出的url地址（即示例里avConfig中的url），形式为fd://xx (fd number)。需要基础文件操作接口（Core File Kit的ohos.file.fs）实现应用文件访问能力，获取方式参考应用文件访问与管理。
```typescript
import { media } from '@kit.MediaKit';
import { BusinessError } from '@kit.BasicServicesKit';
import { fileIo as fs } from '@kit.CoreFileKit';
let avProfile: media.AVRecorderProfile = {
audioBitrate: 100000, // 音频比特率
audioChannels: 2, // 音频声道数
audioCodec: media.CodecMimeType.AUDIO_AAC, // 音频编码格式，当前支持ACC，MP3，G711MU
audioSampleRate: 48000, // 音频采样率
fileFormat: media.ContainerFormatType.CFT_MPEG_4A, // 封装格式，当前支持MP4，M4A，MP3，WAV
};
const context: Context = getContext(this); // 参考应用文件访问与管理
let filePath: string = context.filesDir + '/example.mp3';
let audioFile: fs.File = fs.openSync(filePath, fs.OpenMode.READ_WRITE | fs.OpenMode.CREATE);
let fileFd: number = this.audioFile.fd; // 获取文件fd
let avConfig: media.AVRecorderConfig = {
audioSourceType: media.AudioSourceType.AUDIO_SOURCE_TYPE_MIC, // 音频输入源，这里设置为麦克风
profile: avProfile,
url: 'fd://' + fileFd.toString(), // 参考应用文件访问与管理中的开发示例获取创建的音频文件fd填入此处
};
this.avRecorder.prepare(avConfig).then(() => {
console.log('Invoke prepare succeeded.');
}, (err: BusinessError) => {
console.error(`Invoke prepare failed, code is ${err.code}, message is ${err.message}`);
})
```
4.  配置参数之前需要确保完成对应权限的申请，请参考申请权限。
5.  prepare接口的入参avConfig中仅设置音频相关的配置参数，如示例代码所示。 如果只需要录制音频，请不要设置视频相关配置参数；如果需要录制视频，可以参考视频录制开发指导进行开发。直接设置视频相关参数会导致后续步骤报错。
6.  需要使用支持的录制规格。
7.  录制输出的url地址（即示例里avConfig中的url），形式为fd://xx (fd number)。需要基础文件操作接口（Core File Kit的ohos.file.fs）实现应用文件访问能力，获取方式参考应用文件访问与管理。
8.  开始录制，调用start()接口，此时进入started状态。
```typescript
// 开始录制
avRecorder.start();
```
9.  暂停录制，调用pause()接口，此时进入paused状态。
```typescript
// 暂停录制
avRecorder.pause();
```
10.  恢复录制，调用resume()接口，此时再次进入started状态。
```typescript
// 恢复录制
avRecorder.resume();
```
11.  停止录制，调用stop()接口，此时进入stopped状态。
```typescript
// 停止录制
avRecorder.stop();
```
12.  重置资源，调用reset()重新进入idle状态，允许重新配置录制参数。
```typescript
// 重置资源
avRecorder.reset();
```
13.  销毁实例，调用release()进入released状态，退出录制。
```typescript
// 销毁实例
avRecorder.release();
```
| 事件类型 | 说明 |
| --- | --- |
| stateChange | 必要事件，监听AVRecorder的state属性改变 |
| error | 必要事件，监听AVRecorder的错误信息 |
-  配置参数之前需要确保完成对应权限的申请，请参考申请权限。
-  prepare接口的入参avConfig中仅设置音频相关的配置参数，如示例代码所示。 如果只需要录制音频，请不要设置视频相关配置参数；如果需要录制视频，可以参考视频录制开发指导进行开发。直接设置视频相关参数会导致后续步骤报错。
-  需要使用支持的录制规格。
-  录制输出的url地址（即示例里avConfig中的url），形式为fd://xx (fd number)。需要基础文件操作接口（Core File Kit的ohos.file.fs）实现应用文件访问能力，获取方式参考应用文件访问与管理。
完整示例
参考以下示例，完成“开始录制-暂停录制-恢复录制-停止录制”的完整流程。
```typescript
import { media } from '@kit.MediaKit';
import { BusinessError } from '@kit.BasicServicesKit';
import { fileIo as fs } from '@kit.CoreFileKit';
export class AudioRecorderDemo {
private avRecorder: media.AVRecorder | undefined = undefined;
private avProfile: media.AVRecorderProfile = {
audioBitrate: 100000, // 音频比特率
audioChannels: 2, // 音频声道数
audioCodec: media.CodecMimeType.AUDIO_AAC, // 音频编码格式，当前支持ACC，MP3，G711MU
audioSampleRate: 48000, // 音频采样率
fileFormat: media.ContainerFormatType.CFT_MPEG_4A, // 封装格式，当前支持MP4，M4A，MP3，WAV
};
private avConfig: media.AVRecorderConfig = {
audioSourceType: media.AudioSourceType.AUDIO_SOURCE_TYPE_MIC, // 音频输入源，这里设置为麦克风
profile: this.avProfile,
url: 'fd://35', // 参考应用文件访问与管理开发示例新建并读写一个文件
};
private uriPath: string = '';
private filePath: string = '';
// 创建文件以及设置avConfig.url
async createAndSetFd(): Promise<void> {
const context: Context = getContext(this);
const path: string = context.filesDir + '/example.mp3'; // 文件沙箱路径，文件后缀名应与封装格式对应
const audioFile: fs.File = fs.openSync(path, fs.OpenMode.READ_WRITE | fs.OpenMode.CREATE);
this.avConfig.url = 'fd://' + audioFile.fd; // 更新url
this.filePath = path;
}
// 注册audioRecorder回调函数
setAudioRecorderCallback() {
if (this.avRecorder != undefined) {
// 状态机变化回调函数
this.avRecorder.on('stateChange', (state: media.AVRecorderState, reason: media.StateChangeReason) => {
console.log(`AudioRecorder current state is ${state}`);
})
// 错误上报回调函数
this.avRecorder.on('error', (err: BusinessError) => {
console.error(`AudioRecorder failed, code is ${err.code}, message is ${err.message}`);
})
}
}
// 开始录制对应的流程
async startRecordingProcess() {
if (this.avRecorder != undefined) {
await this.avRecorder.release();
this.avRecorder = undefined;
}
// 1.创建录制实例
this.avRecorder = await media.createAVRecorder();
this.setAudioRecorderCallback();
// 2.获取录制文件fd赋予avConfig里的url；参考FilePicker文档
// 3.配置录制参数完成准备工作
await this.avRecorder.prepare(this.avConfig);
// 4.开始录制
await this.avRecorder.start();
}
// 暂停录制对应的流程
async pauseRecordingProcess() {
if (this.avRecorder != undefined && this.avRecorder.state === 'started') { // 仅在started状态下调用pause为合理状态切换
await this.avRecorder.pause();
}
}
// 恢复录制对应的流程
async resumeRecordingProcess() {
if (this.avRecorder != undefined && this.avRecorder.state === 'paused') { // 仅在paused状态下调用resume为合理状态切换
await this.avRecorder.resume();
}
}
// 停止录制对应的流程
async stopRecordingProcess() {
if (this.avRecorder != undefined) {
// 1. 停止录制
if (this.avRecorder.state === 'started'
|| this.avRecorder.state === 'paused') { // 仅在started或者paused状态下调用stop为合理状态切换
await this.avRecorder.stop();
}
// 2.重置
await this.avRecorder.reset();
// 3.释放录制实例
await this.avRecorder.release();
this.avRecorder = undefined;
// 4.关闭录制文件fd
}
}
// 一个完整的【开始录制-暂停录制-恢复录制-停止录制】示例
async audioRecorderDemo() {
await this.startRecordingProcess(); // 开始录制
// 用户此处可以自行设置录制时长，例如通过设置休眠阻止代码执行
await this.pauseRecordingProcess(); //暂停录制
await this.resumeRecordingProcess(); // 恢复录制
await this.stopRecordingProcess(); // 停止录制
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/video-recording-V14
爬取时间: 2025-04-28 20:34:34
来源: Huawei Developer
当前仅支持AVRecorder开发视频录制，集成了音频捕获，音频编码，视频编码，音视频封装功能，适用于实现简单视频录制并直接得到视频本地文件的场景。
本开发指导将以“开始录制-暂停录制-恢复录制-停止录制”的一次流程为示例，向开发者讲解如何使用AVRecorder进行视频录制。
在进行应用开发的过程中，开发者可以通过AVRecorder的state属性主动获取当前状态，或使用on('stateChange')方法监听状态变化。开发过程中应该严格遵循状态机要求，例如只能在started状态下调用pause()接口，只能在paused状态下调用resume()接口。
图1录制状态变化示意图
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165954.42255574545590409045263071647851:50001231000000:2800:960E348D15543B9CCCA274771C6641B3D0FBF15866E04B4BEBB50F297FD7A393.png)
状态的详细说明请参考AVRecorderState。
申请权限
在开发此功能前，开发者应根据实际需求申请相关权限：
仅应用需要克隆、备份或同步用户公共目录的图片、视频类文件时，可申请ohos.permission.READ_IMAGEVIDEO、ohos.permission.WRITE_IMAGEVIDEO权限来读写音频文件，申请方式请参考申请受控权限，通过AGC审核后才能使用。为避免应用的上架申请被驳回，开发者应优先使用Picker/控件等替代方案，仅少量符合特殊场景的应用被允许申请受限权限。
开发步骤及注意事项
AVRecorder只负责视频数据的处理，需要与视频数据采集模块配合才能完成视频录制。视频数据采集模块需要通过Surface将视频数据传递给AVRecorder进行数据处理。当前常用的数据采集模块为相机模块，具体请参考相机-录像。
文件的创建与存储，请参考应用文件访问与管理，默认存储在应用的沙箱路径之下，如需存储至图库，请使用安全控件保存媒体资源对沙箱内文件进行存储。
AVRecorder详细的API说明请参考AVRecorder API参考。
1.  创建AVRecorder实例，实例创建完成进入idle状态。
```typescript
import { media } from '@kit.MediaKit';
import { BusinessError } from '@kit.BasicServicesKit';
let avRecorder: media.AVRecorder;
media.createAVRecorder().then((recorder: media.AVRecorder) => {
avRecorder = recorder;
}, (error: BusinessError) => {
console.error('createAVRecorder failed');
})
```
2.  设置业务需要的监听事件，监听状态变化及错误上报。
```typescript
import { media } from '@kit.MediaKit';
import { BusinessError } from '@kit.BasicServicesKit';
// 状态上报回调函数
this.avRecorder.on('stateChange', (state: media.AVRecorderState, reason: media.StateChangeReason) => {
console.info('current state is: ' + state);
})
// 错误上报回调函数
this.avRecorder.on('error', (err: BusinessError) => {
console.error('error happened, error message is ' + err);
})
```
3.  配置视频录制参数，调用prepare()接口，此时进入prepared状态。 配置参数需要注意： 配置参数之前需要确保完成对应权限的申请，请参考申请权限。 prepare接口的入参avConfig中仅设置视频相关的配置参数，如示例代码所示。 如果添加了音频参数，系统将认为是“音频+视频录制”。 需要使用支持的录制规格，视频比特率、分辨率、帧率以实际硬件设备支持的范围为准。 录制输出的url地址（即示例里avConfig中的url），形式为fd://xx (fd number)。需要调用基础文件操作接口（Core File Kit的ohos.file.fs）实现应用文件访问能力，获取方式参考应用文件访问与管理。
```typescript
import { media } from '@kit.MediaKit';
import { BusinessError } from '@kit.BasicServicesKit';
import { fileIo as fs } form '@kit.CoreFileKit';
let avProfile: media.AVRecorderProfile = {
fileFormat : media.ContainerFormatType.CFT_MPEG_4, // 视频文件封装格式，只支持MP4
videoBitrate : 200000, // 视频比特率
videoCodec : media.CodecMimeType.VIDEO_AVC, // 视频文件编码格式，支持avc格式
videoFrameWidth : 640,  // 视频分辨率的宽
videoFrameHeight : 480, // 视频分辨率的高
videoFrameRate : 30 // 视频帧率
};
const context: Context = getContext(this); // 参考应用文件访问与管理
let filePath: string = context.filesDir + '/example.mp4';
let videoFile: fs.File = fs.openSync(filePath, fs.OpenMode.READ_WRITE | fs.OpenMode.CREATE);
let fileFd = videoFile.fd; // 获取文件fd
let avConfig: media.AVRecorderConfig = {
videoSourceType : media.VideoSourceType.VIDEO_SOURCE_TYPE_SURFACE_YUV, // 视频源类型，支持YUV和ES两种格式
profile : avProfile,
url: 'fd://' + fileFd.toString(), // 参考应用文件访问与管理开发示例新建并读写一个视频文件
rotation : 0 // 视频旋转角度，默认为0不旋转，支持的值为0、90、180、270
};
this.avRecorder.prepare(avConfig).then(() => {
console.info('avRecorder prepare success');
}, (error: BusinessError) => {
console.error('avRecorder prepare failed');
})
```
4.  配置参数之前需要确保完成对应权限的申请，请参考申请权限。
5.  prepare接口的入参avConfig中仅设置视频相关的配置参数，如示例代码所示。 如果添加了音频参数，系统将认为是“音频+视频录制”。
6.  需要使用支持的录制规格，视频比特率、分辨率、帧率以实际硬件设备支持的范围为准。
7.  录制输出的url地址（即示例里avConfig中的url），形式为fd://xx (fd number)。需要调用基础文件操作接口（Core File Kit的ohos.file.fs）实现应用文件访问能力，获取方式参考应用文件访问与管理。
8.  获取视频录制需要的SurfaceID。 调用getInputSurface()接口，接口的返回值SurfaceID用于传递给视频数据输入源模块。常用的输入源模块为相机，以下示例代码中，采用相机作为视频输入源为例。 输入源模块通过SurfaceID可以获取到Surface，通过Surface可以将视频数据流传递给AVRecorder，由AVRecorder再进行视频数据的处理。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
this.avRecorder.getInputSurface().then((surfaceId: string) => {
console.info('avRecorder getInputSurface success');
}, (error: BusinessError) => {
console.error('avRecorder getInputSurface failed');
})
```
9.  初始化视频数据输入源。该步骤需要在输入源模块完成，以相机为例，需要创建录像输出流，包括创建Camera对象、获取相机列表、创建相机输入流等，相机详细步骤请参考相机-录像方案。
10.  开始录制，启动输入源输入视频数据，例如相机模块调用camera.VideoOutput.start接口启动相机录制。然后调用AVRecorder.start()接口，此时AVRecorder进入started状态。
11.  暂停录制，调用pause()接口，此时AVRecorder进入paused状态，同时暂停输入源输入数据。例如相机模块调用camera.VideoOutput.stop停止相机视频数据输入。
12.  恢复录制，调用resume()接口，此时再次进入started状态。
13.  停止录制，调用stop()接口，此时进入stopped状态，同时停止相机录制。
14.  重置资源，调用reset()重新进入idle状态，允许重新配置录制参数。
15.  销毁实例，调用release()进入released状态，退出录制，释放视频数据输入源相关资源，例如相机资源。
| 事件类型 | 说明 |
| --- | --- |
| stateChange | 必要事件，监听播放器的state属性改变 |
| error | 必要事件，监听播放器的错误信息 |
-  配置参数之前需要确保完成对应权限的申请，请参考申请权限。
-  prepare接口的入参avConfig中仅设置视频相关的配置参数，如示例代码所示。 如果添加了音频参数，系统将认为是“音频+视频录制”。
-  需要使用支持的录制规格，视频比特率、分辨率、帧率以实际硬件设备支持的范围为准。
-  录制输出的url地址（即示例里avConfig中的url），形式为fd://xx (fd number)。需要调用基础文件操作接口（Core File Kit的ohos.file.fs）实现应用文件访问能力，获取方式参考应用文件访问与管理。
完整示例
参考以下示例，完成“开始录制-暂停录制-恢复录制-停止录制”的完整流程。
```typescript
import { media } from '@kit.MediaKit';
import { BusinessError } from '@kit.BasicServicesKit';
import { fileIo as fs, fileUri } from '@kit.CoreFileKit';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
const TAG = 'VideoRecorderDemo:';
export class VideoRecorderDemo {
private context: Context;
constructor() {
this.context = getContext(this);
}
private avRecorder: media.AVRecorder | undefined = undefined;
private videoOutSurfaceId: string = "";
private avProfile: media.AVRecorderProfile = {
fileFormat : media.ContainerFormatType.CFT_MPEG_4, // 视频文件封装格式，只支持MP4
videoBitrate : 100000, // 视频比特率
videoCodec : media.CodecMimeType.VIDEO_AVC, // 视频文件编码格式，支持avc格式
videoFrameWidth : 640,  // 视频分辨率的宽
videoFrameHeight : 480, // 视频分辨率的高
videoFrameRate : 30 // 视频帧率
};
private avConfig: media.AVRecorderConfig = {
videoSourceType : media.VideoSourceType.VIDEO_SOURCE_TYPE_SURFACE_YUV, // 视频源类型，支持YUV和ES两种格式
profile : this.avProfile,
url : 'fd://35', //  参考应用文件访问与管理开发示例新建并读写一个文件
rotation : 0 // 视频旋转角度，默认为0不旋转，支持的值为0、90、180、270
};
private uriPath: string = ''; // 文件uri，可用于安全控件保存媒体资源
private filePath: string = ''; // 文件路径
private fileFd: number = 0;
// 创建文件以及设置avConfig.url
async createAndSetFd() {
const path: string = this.context.filesDir + '/example.mp4'; // 文件沙箱路径，文件后缀名应与封装格式对应
const videoFile: fs.File = fs.openSync(path, fs.OpenMode.READ_WRITE | fs.OpenMode.CREATE);
this.avConfig.url = 'fd://' + videoFile.fd; // 设置url
this.fileFd = videoFile.fd; // 文件fd
this.filePath = path;
}
// 注册avRecorder回调函数
setAvRecorderCallback() {
if (this.avRecorder != undefined) {
// 状态机变化回调函数
this.avRecorder.on('stateChange', (state: media.AVRecorderState, reason: media.StateChangeReason) => {
console.info(TAG + 'current state is: ' + state);
})
// 错误上报回调函数
this.avRecorder.on('error', (err: BusinessError) => {
console.error(TAG + 'error ocConstantSourceNode, error message is ' + err);
})
}
}
// 相机相关准备工作
async prepareCamera() {
// 具体实现查看相机资料
}
// 启动相机出流
async startCameraOutput() {
// 调用VideoOutput的start接口开始录像输出
}
// 停止相机出流
async stopCameraOutput() {
// 调用VideoOutput的stop接口停止录像输出
}
// 释放相机实例
async releaseCamera() {
// 释放相机准备阶段创建出的实例
}
// 开始录制对应的流程
async startRecordingProcess() {
if (this.avRecorder === undefined) {
// 1.创建录制实例；
this.avRecorder = await media.createAVRecorder();
this.setAvRecorderCallback();
}
// 2. 获取录制文件fd；获取到的值传递给avConfig里的url，实现略
// 3.配置录制参数完成准备工作
await this.avRecorder.prepare(this.avConfig);
this.videoOutSurfaceId = await this.avRecorder.getInputSurface();
// 4.完成相机相关准备工作
await this.prepareCamera();
// 5.启动相机出流
await this.startCameraOutput();
// 6. 启动录制
await this.avRecorder.start();
}
// 暂停录制对应的流程
async pauseRecordingProcess() {
if (this.avRecorder != undefined && this.avRecorder.state === 'started') { // 仅在started状态下调用pause为合理状态切换
await this.avRecorder.pause();
await this.stopCameraOutput(); // 停止相机出流
}
}
// 恢复录制对应的流程
async resumeRecordingProcess() {
if (this.avRecorder != undefined && this.avRecorder.state === 'paused') { // 仅在paused状态下调用resume为合理状态切换
await this.startCameraOutput();  // 启动相机出流
await this.avRecorder.resume();
}
}
async stopRecordingProcess() {
if (this.avRecorder != undefined) {
// 1. 停止录制
if (this.avRecorder.state === 'started'
|| this.avRecorder.state === 'paused' ) { // 仅在started或者paused状态下调用stop为合理状态切换
await this.avRecorder.stop();
await this.stopCameraOutput();
}
// 2.重置
await this.avRecorder.reset();
// 3.释放录制实例
await this.avRecorder.release();
// 4.文件录制完成后，关闭fd,实现略
await fs.close(this.fileFd);
// 5.释放相机相关实例
await this.releaseCamera();
}
}
// 安全控件保存媒体资源至图库
async saveRecorderAsset() {
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(this.context);
// 需要确保uriPath对应的资源存在
this.uriPath = fileUri.getUriFromPath(this.filePath); // 获取录制文件的uri，用于安全控件保存至图库
let assetChangeRequest: photoAccessHelper.MediaAssetChangeRequest =
photoAccessHelper.MediaAssetChangeRequest.createVideoAssetRequest(this.context, this.uriPath);
await phAccessHelper.applyChanges(assetChangeRequest);
}
// 一个完整的【开始录制-暂停录制-恢复录制-停止录制】示例
async videoRecorderDemo() {
await this.startRecordingProcess();         // 开始录制
// 用户此处可以自行设置录制时长，例如通过设置休眠阻止代码执行
await this.pauseRecordingProcess();         //暂停录制
await this.resumeRecordingProcess();        // 恢复录制
await this.stopRecordingProcess();          // 停止录制
// 安全控件保存媒体资源至图库
await this.saveRecorderAsset();
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/using-avscreencapture-arkts-V14
爬取时间: 2025-04-28 20:35:28
来源: Huawei Developer
屏幕录制主要为主屏幕录屏功能。
开发者可以调用录屏（AVScreenCaptureRecorder）模块的ArkTs接口，完成屏幕录制，采集设备内、麦克风等的音视频源数据。可以调用录屏模块获取音视频文件，然后通过文件的形式流转到其他模块进行播放或处理，达成文件形式分享屏幕内容的场景。
录屏模块和窗口（Window）、图形（Graphic）等模块协同完成整个视频采集的流程。
使用AVScreenCaptureRecorder录制屏幕涉及到AVScreenCaptureRecorder实例的创建、音视频采集参数的配置、采集的开始与停止、资源的释放等。
开始屏幕录制时正在通话中或者屏幕录制过程中来电，录屏将自动停止。因通话中断的录屏会上报SCREENCAPTURE_STATE_STOPPED_BY_CALL状态。
本开发指导将以完成一次屏幕数据录制的过程为例，向开发者讲解如何使用AVScreenCaptureRecorder进行屏幕录制，详细的API声明请参考AVScreenCaptureRecoder API参考。
如果配置了采集麦克风音频数据，需对应配置麦克风权限ohos.permission.MICROPHONE和申请长时任务，配置方式请参见向用户申请权限、申请长时任务。
申请权限
在开发此功能前，开发者应根据实际需求申请相关权限：
仅应用需要克隆、备份或同步用户公共目录的图片、视频类文件时，可申请ohos.permission.READ_IMAGEVIDEO、ohos.permission.WRITE_IMAGEVIDEO权限来读写音频文件，申请方式请参考申请受控权限，通过AGC审核后才能使用。为避免应用的上架申请被驳回，开发者应优先使用Picker/控件等替代方案，仅少量符合特殊场景的应用被允许申请受限权限。
开发步骤及注意事项
使用AVScreenCaptureRecorder时要明确其状态的变化，在创建实例后，调用对应的方法可以进入指定的状态实现对应的行为。在确定的状态下执行不合适的方法会导致AVScreenCaptureRecorder发生错误，开发者需要在调用状态转换的方法前进行状态检查，避免程序运行异常。
1.  添加头文件。
2.  创建AVScreenCaptureRecorder类型的成员变量screenCapture。
3.  对成员变量screenCapture设置监听函数，分别监听不同状态和异常情况。
4.  配置屏幕录制参数。 ​创建AVScreenCaptureRecorder实例screenCapture后，可以设置屏幕录制所需要的参数。 ​参数videoBitrate、audioSampleRate、audioChannelCount、audioBitrate、preset为可选参数，若不设置则可按默认值进行设置，如下示例中提供了可选参数的默认值。麦克风和系统音的音频流共用一套音频参数，分别是音频采样率、音频通道数和音频比特率，对应audioSampleRate、audioChannelCount和audioBitrate参数。
5.  基于预先配置的屏幕录制参数，调用init()方法初始化screenCapture。
6.  创建豁免隐私窗口，这里填写的是子窗口id和主窗口id，具体开发步骤可参见窗口API
7.  调用startRecording()方法开始进行屏幕录制，并通过监听函数监听状态。
8.  停止录屏。 点击录屏胶囊中的结束按钮停止录制：基于回调函数实现，录屏对象实例screenCapture会触发SCREENCAPTURE_STATE_STOPPED_BY_USER的回调，通知应用此次录屏已停止，不需要开发者主动调用stopRecording()方法。 应用主动调用stopRecording()方法，停止录屏。
9.  点击录屏胶囊中的结束按钮停止录制：基于回调函数实现，录屏对象实例screenCapture会触发SCREENCAPTURE_STATE_STOPPED_BY_USER的回调，通知应用此次录屏已停止，不需要开发者主动调用stopRecording()方法。
10.  应用主动调用stopRecording()方法，停止录屏。
11.  调用release()方法销毁实例，释放资源。
-  点击录屏胶囊中的结束按钮停止录制：基于回调函数实现，录屏对象实例screenCapture会触发SCREENCAPTURE_STATE_STOPPED_BY_USER的回调，通知应用此次录屏已停止，不需要开发者主动调用stopRecording()方法。
-  应用主动调用stopRecording()方法，停止录屏。
完整示例
下面展示了使用AVScreenCaptureRecorder屏幕录屏存文件的完整示例代码。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/media-info-arkts-V14
爬取时间: 2025-04-28 20:35:44
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/avimagegenerator-V14
爬取时间: 2025-04-28 20:37:09
来源: Huawei Developer
使用AVImageGenerator可以实现从原始媒体资源中获取视频指定时间的缩略图，本开发指导将以获取一个视频资源的缩略图作为示例，向开发者讲解AVImageGenerator相关功能。
获取视频资源的缩略图的全流程包含：创建AVImageGenerator对象，设置资源，获取缩略图，销毁资源。
开发步骤及注意事项
详细的API说明请参考AVImageGenerator API参考。
1.  使用createAVImageGenerator()创建实例。
2.  设置资源：需要设置属性fdSrc（表示文件描述符）。 开发者需根据实际情况，确认资源有效性并设置fdSrc： 可以使用ResourceManager.getRawFd打开HAP资源文件描述符，使用方法可参考ResourceManager API参考。 也可以使用应用沙箱路径访问对应资源（必须确认资源文件可用），参考获取应用文件路径。应用沙箱的介绍及如何向应用沙箱推送文件，请参考文件管理。 不同AVImageGenerator或者AVMetadataExtractor，如果需要操作同一资源，需要多次打开文件描述符，不要共用同一文件描述符。
3.  可以使用ResourceManager.getRawFd打开HAP资源文件描述符，使用方法可参考ResourceManager API参考。
4.  也可以使用应用沙箱路径访问对应资源（必须确认资源文件可用），参考获取应用文件路径。应用沙箱的介绍及如何向应用沙箱推送文件，请参考文件管理。
5.  不同AVImageGenerator或者AVMetadataExtractor，如果需要操作同一资源，需要多次打开文件描述符，不要共用同一文件描述符。
6.  获取指定时间图像：调用fetchFrameByTime()，可以获取到一个PixelMap对象，该对象可用于图片显示。
7.  释放资源：调用release()销毁实例，释放资源。
-  可以使用ResourceManager.getRawFd打开HAP资源文件描述符，使用方法可参考ResourceManager API参考。
-  也可以使用应用沙箱路径访问对应资源（必须确认资源文件可用），参考获取应用文件路径。应用沙箱的介绍及如何向应用沙箱推送文件，请参考文件管理。
-  不同AVImageGenerator或者AVMetadataExtractor，如果需要操作同一资源，需要多次打开文件描述符，不要共用同一文件描述符。
完整示例
参考以下示例，设置文件描述符，获取一个视频指定时间的缩略图。
```typescript
import { media } from '@kit.MediaKit';
import { image } from '@kit.ImageKit';
const TAG = 'MetadataDemo';
@Entry
@Component
struct Index {
@State message: string = 'Hello World';
// pixelMap对象声明，用于图片显示
@State pixelMap: image.PixelMap | undefined = undefined;
build() {
Row() {
Column() {
Text(this.message).fontSize(50).fontWeight(FontWeight.Bold)
Button() {
Text('TestButton')
.fontSize(30)
.fontWeight(FontWeight.Bold)
}
.type(ButtonType.Capsule)
.margin({
top: 20
})
.backgroundColor('#0D9FFB')
.width('60%')
.height('5%')
.onClick(() => {
// 设置fdSrc, 获取视频的缩略图
this.testFetchFrameByTime();
})
Image(this.pixelMap).width(300).height(300)
.margin({
top: 20
})
}
.width('100%')
}
.height('100%')
}
// 在以下demo中，使用资源管理接口获取打包在HAP内的视频文件，通过设置fdSrc属性，
// 获取视频指定时间的缩略图，并通过Image控件显示在屏幕上。
async testFetchFrameByTime() {
// 创建AVImageGenerator对象
let avImageGenerator: media.AVImageGenerator = await media.createAVImageGenerator();
// 设置fdSrc
avImageGenerator.fdSrc = await getContext(this).resourceManager.getRawFd('demo.mp4');
// 初始化入参
let timeUs = 0;
let queryOption = media.AVImageQueryOptions.AV_IMAGE_QUERY_NEXT_SYNC;
let param: media.PixelMapParams = {
width : 300,
height : 300
};
// 获取缩略图（promise模式）
this.pixelMap = await avImageGenerator.fetchFrameByTime(timeUs, queryOption, param);
// 释放资源（promise模式）
avImageGenerator.release();
console.info(TAG, `release success.`);
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/media-transcoder-arkts-V14
爬取时间: 2025-04-28 20:37:23
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/using-avtranscoder-for-transcodering-V14
爬取时间: 2025-04-28 20:37:37
来源: Huawei Developer
使用AVTranscoder可以实现视频转码功能，从API 12开始，转码功能可在手机、平板、2in1设备上作为系统提供的基础能力使用。可以通过调用canIUse接口来判断当前设备是否支持AVTranscoder，canIUse("SystemCapability.Multimedia.Media.AVTranscoder")返回值为true，表示可以使用转码能力。
本开发指导将以“开始转码-暂停转码-恢复转码-转码完成”的一次流程为示例，向开发者讲解AVTranscoder视频转码相关功能。
开发步骤及注意事项
详细的API说明请参考AVTranscoder API参考。
1.  创建AVTranscoder实例。
```typescript
import { media } from '@kit.MediaKit';
import { BusinessError } from '@kit.BasicServicesKit';
let avTranscoder: media.AVTranscoder;
media.createAVTranscoder().then((transcoder: media.AVTranscoder) => {
avTranscoder = transcoder;
// 需要在avTranscoder完成赋值后，再进行其他操作。
}, (error: BusinessError) => {
console.error(`createAVTranscoder failed`);
});
```
2.  设置业务需要的监听事件，监听状态变化及错误上报。
```typescript
import { BusinessError } from '@kit.BasicServicesKit';
// 转码完成回调函数
avTranscoder.on('complete', () => {
console.log(`transcoder is completed`);
// 用户可以在此监听转码完成事件
});
// 错误上报回调函数
avTranscoder.on('error', (err: BusinessError) => {
console.error(`avTranscoder failed, code is ${err.code}, message is ${err.message}`);
});
```
3.  设置源视频文件fd：设置属性fdSrc。 下面代码示例中的fdSrc仅作示意使用，开发者需根据实际情况，确认资源有效性并设置： 如果使用本地资源转码，必须确认资源文件可用，并使用应用沙箱路径访问对应资源，参考获取应用文件路径。应用沙箱的介绍及如何向应用沙箱推送文件，请参考文件管理。 如果使用ResourceManager.getRawFd()打开HAP资源文件描述符，使用方法可参考ResourceManager API参考。
```typescript
import resourceManager from '@ohos.resourceManager';
import { common } from '@kit.AbilityKit';
let context = getContext(this) as common.UIAbilityContext;
let fileDescriptor = await context.resourceManager.getRawFd('H264_AAC.mp4');
// 设置转码的源文件属性fdSrc
this.avTranscoder.fdSrc = fileDescriptor;
```
4.  如果使用本地资源转码，必须确认资源文件可用，并使用应用沙箱路径访问对应资源，参考获取应用文件路径。应用沙箱的介绍及如何向应用沙箱推送文件，请参考文件管理。
5.  如果使用ResourceManager.getRawFd()打开HAP资源文件描述符，使用方法可参考ResourceManager API参考。
6.  设置目标视频文件fd：设置属性fdDst。 转码输出文件fd（即示例里fdDst），形式为number。需要调用基础文件操作接口（Core File Kit的ohos.file.fs）实现应用文件访问能力，获取方式参考应用文件访问与管理。
```typescript
// 设置转码的目标文件属性fdDst
this.avTranscoder.fdDst = 55; // 参考应用文件访问与管理中的开发示例获取创建的视频文件fd填入此处
```
7.  配置视频转码参数，调用prepare()接口。 写入配置参数时需要注意，prepare()接口的入参avConfig中仅设置转码相关的配置参数。 受限于解析/封装/编解码能力，只能使用支持的转码格式。
```typescript
import { media } from '@kit.MediaKit';
import { BusinessError } from '@kit.BasicServicesKit';
let avConfig: media.AVTranscoderConfig = {
audioBitrate: 100000, // 音频比特率
audioCodec: media.CodecMimeType.AUDIO_AAC, // 音频编码格式
fileFormat: media.ContainerFormatType.CFT_MPEG_4, // 封装格式
videoBitrate: 2000000, // 视频比特率
videoCodec: media.CodecMimeType.VIDEO_AVC, // 视频编码格式
videoFrameWidth: 640, // 视频分辨率的宽为640
videoFrameHeight: 480, // 视频分辨率的高为480
};
avTranscoder.prepare(avConfig).then(() => {
console.log('Invoke prepare succeeded.');
}, (err: BusinessError) => {
console.error(`Invoke prepare failed, code is ${err.code}, message is ${err.message}`);
});
```
8.  开始转码，调用start()接口。
```typescript
// 开始转码
avTranscoder.start();
```
9.  暂停转码，调用pause()接口。
```typescript
// 暂停转码
avTranscoder.pause();
```
10.  恢复转码，调用resume()接口。
```typescript
// 恢复转码
avTranscoder.resume();
```
11.  取消转码，调用cancel()接口。
```typescript
// 取消转码
avTranscoder.cancel();
```
12.  销毁实例，调用release()接口，退出转码。
```typescript
// 销毁实例
avTranscoder.release();
```
| 事件类型 | 说明 |
| --- | --- |
| complete | 必要事件，监听AVTranscoder的转码完成 |
| error | 必要事件，监听AVTranscoder的错误信息 |
-  如果使用本地资源转码，必须确认资源文件可用，并使用应用沙箱路径访问对应资源，参考获取应用文件路径。应用沙箱的介绍及如何向应用沙箱推送文件，请参考文件管理。
-  如果使用ResourceManager.getRawFd()打开HAP资源文件描述符，使用方法可参考ResourceManager API参考。
完整示例
参考以下示例，完成“开始转码-暂停转码-恢复转码-转码完成”的完整流程。
```typescript
import { media } from '@kit.MediaKit';
import { BusinessError } from '@kit.BasicServicesKit';
import { common } from '@kit.AbilityKit';
export class AVTranscoderDemo {
private avTranscoder: media.AVTranscoder | undefined = undefined;
private avConfig: media.AVTranscoderConfig = {
audioBitrate: 100000, // 音频比特率
audioCodec: media.CodecMimeType.AUDIO_AAC, // 音频编码格式
fileFormat: media.ContainerFormatType.CFT_MPEG_4, // 封装格式
videoBitrate: 200000, // 视频比特率
videoCodec: media.CodecMimeType.VIDEO_AVC, // 视频编码格式
videoFrameWidth: 640, // 视频分辨率的宽
videoFrameHeight: 480, // 视频分辨率的高
};
// 注册avTranscoder回调函数
setAVTranscoderCallback() {
if (canIUse("SystemCapability.Multimedia.Media.AVTranscoder")) {
if (this.avTranscoder != undefined) {
// 转码完成回调函数
this.avTranscoder.on('complete', async () => {
console.log(`AVTranscoder is completed`);
await this.releaseTranscoderingProcess();
});
// 错误上报回调函数
this.avTranscoder.on('error', (err: BusinessError) => {
console.error(`AVTranscoder failed, code is ${err.code}, message is ${err.message}`);
});
}
}
}
// 开始转码对应的流程
async startTranscoderingProcess() {
if (canIUse("SystemCapability.Multimedia.Media.AVTranscoder")) {
if (this.avTranscoder != undefined) {
await this.avTranscoder.release();
this.avTranscoder = undefined;
}
// 1.创建转码实例
this.avTranscoder = await media.createAVTranscoder();
this.setAVTranscoderCallback();
// 2.获取转码源文件fd和目标文件fd赋予avTranscoder；参考FilePicker文档
let context = getContext(this) as common.UIAbilityContext;
let fileDescriptor = await context.resourceManager.getRawFd('H264_AAC.mp4');
this.avTranscoder.fdSrc = fileDescriptor;
this.avTranscoder.fdDst = 55;
// 3.配置转码参数完成准备工作
await this.avTranscoder.prepare(this.avConfig);
// 4.开始转码
await this.avTranscoder.start();
}
}
// 暂停转码对应的流程
async pauseTranscoderingProcess() {
if (canIUse("SystemCapability.Multimedia.Media.AVTranscoder")) {
if (this.avTranscoder != undefined) { // 仅在调用start返回后调用pause为合理调用
await this.avTranscoder.pause();
}
}
}
// 恢复对应的转码流程
async resumeTranscoderingProcess() {
if (canIUse("SystemCapability.Multimedia.Media.AVTranscoder")) {
if (this.avTranscoder != undefined) { // 仅在调用pause返回后调用resume为合理调用
await this.avTranscoder.resume();
}
}
}
// 释放转码流程
async releaseTranscoderingProcess() {
if (canIUse("SystemCapability.Multimedia.Media.AVTranscoder")) {
if (this.avTranscoder != undefined) {
// 1.释放转码实例
await this.avTranscoder.release();
this.avTranscoder = undefined;
// 2.关闭转码目标文件fd
}
}
}
// 一个完整的【开始转码-暂停转码-恢复转码-转码完成】示例
async avTranscoderDemo() {
await this.startTranscoderingProcess(); // 开始转码
await this.pauseTranscoderingProcess(); //暂停转码
await this.resumeTranscoderingProcess(); // 恢复转码
await this.releaseTranscoderingProcess(); // 释放转码
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/media-kit-dev--c-V14
爬取时间: 2025-04-28 20:37:57
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/media-playback-c-V14
爬取时间: 2025-04-28 20:38:10
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/using-ndk-avplayer-for-playback-V14
爬取时间: 2025-04-28 20:38:24
来源: Huawei Developer
使用AVPlayer可以实现端到端播放原始媒体资源，本开发指导将以完整地播放一首音乐作为示例，向开发者讲解AVPlayer音频播放相关功能。
播放的全流程包含：创建AVPlayer，设置回调监听函数，设置播放资源，设置播放参数（音量/倍速/焦点模式），播放控制（播放/暂停/跳转/停止），重置，销毁播放器实例。
在进行应用开发的过程中，开发者可以通过AVPlayer的信息监听回调函数OH_AVPlayerOnInfoCallback和错误监听回调函数OH_AVPlayerOnErrorCallback主动获取播放过程信息。如果应用在音频播放器处于错误状态时执行操作，系统可能会抛出异常或生成其他未定义的行为。
图1播放状态变化示意图
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165955.49978062319875874218312913906619:50001231000000:2800:6780E91400AD0167EF98AF257375402552EDFB7D9A286F0A9BE51681F8BD4818.png)
状态的详细说明请参考AVPlayerState。当播放处于prepared / playing / paused / completed状态时，播放引擎处于工作状态，这需要占用系统较多的运行内存。当客户端暂时不使用播放器时，调用reset()或release()回收内存资源，做好资源利用。
开发建议
当前指导仅介绍如何实现媒体资源播放，在应用开发过程中可能会涉及后台播放、播放冲突等情况，请根据实际需要参考以下说明。
开发步骤及注意事项
在 CMake 脚本中链接动态库：
使用OH_AVPlayer_SetOnInfoCallback()、OH_AVPlayer_SetOnErrorCallback()接口设置信息监听回调函数和错误监听回调函数，需要在 CMake 脚本中链接如下动态库：
开发者使用系统日志能力时，需引入如下头文件：
并需要在 CMake 脚本中链接如下动态库:
开发者通过引入avplayer.h、avpalyer_base.h和native_averrors.h头文件，使用音频播放相关API。
详细的API说明请参考AVPlayer API。
1.  创建AVPlayer实例：调用OH_AVPlayer_Create()，AVPlayer初始化为AV_IDLE状态。
2.  设置回调监听函数：使用OH_AVPlayer_SetOnInfoCallback()、OH_AVPlayer_SetOnErrorCallback()接口设置信息监听回调函数和错误监听回调函数，搭配全流程场景使用。支持的监听事件包括： 应用使用OH_AVPlayer_SetOnInfoCallback()、OH_AVPlayer_SetOnErrorCallback()接口设置信息监听回调函数和错误监听回调函数，可以获取更多信息，还可以通过设置 userData 区分不同播放实例。
3.  设置资源：调用OH_AVPlayer_SetURLSource()，设置属性url，AVPlayer进入AV_INITIALIZED状态。
4.  （可选）设置音频流类型：调用OH_AVPlayer_SetAudioRendererInfo()，设置AVPlayer音频流类型。
5.  （可选）设置音频打断模式：调用OH_AVPlayer_SetAudioInterruptMode()，设置AVPlayer音频流打断模式。
6.  准备播放：调用OH_AVPlayer_Prepare()，AVPlayer进入AV_PREPARED状态，此时可以获取时长，设置音量。
7.  （可选）设置音频音效模式：调用OH_AVPlayer_SetAudioEffectMode()，设置AVPlayer音频音效模式。
8.  音频播控：播放OH_AVPlayer_Play()，暂停OH_AVPlayer_Pause()，跳转OH_AVPlayer_Seek()，停止OH_AVPlayer_Stop()等操作。
9.  （可选）更换资源：调用OH_AVPlayer_Reset()重置资源，AVPlayer重新进入AV_IDLE状态，允许更换资源url。
10.  退出播放：调用OH_AVPlayer_Release()销毁实例，AVPlayer进入AV_RELEASED状态，退出播放。之后再操作AVPlayer实例则行为未知，可能导致应用进程崩溃，应用闪退。
| 事件类型 | 说明 |
| --- | --- |
| OH_AVPlayerOnInfoCallback | 必要事件，监听播放器的过程信息。 |
| OH_AVPlayerOnErrorCallback | 必要事件，监听播放器的错误信息。 |
完整示例

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/using-ndk-avplayer-for-video-playback-V14
爬取时间: 2025-04-28 20:38:38
来源: Huawei Developer
使用AVPlayer可以实现端到端播放原始媒体资源，本开发指导将以完整地播放一个视频作为示例，向开发者讲解AVPlayer视频播放相关功能。
播放的全流程包含：创建AVPlayer，设置回调监听函数，设置播放资源，设置播放参数（音量/倍速/焦点模式），设置播放窗口，播放控制（播放/暂停/跳转/停止），重置，销毁播放器实例。
在进行应用开发的过程中，开发者可以通过AVPlayer的信息监听回调函数OH_AVPlayerOnInfoCallback和错误监听回调函数OH_AVPlayerOnErrorCallback主动获取播放过程信息。如果应用在视频播放器处于错误状态时执行操作，系统可能会抛出异常或生成其他未定义的行为。
图1播放状态变化示意图
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165955.51624436929203318905108246136440:50001231000000:2800:31FB00870446418C1C28EADD59D0B3192AEC57DEA4B44E1BEA763BE03F44730D.png)
状态的详细说明请参考AVPlayerState。当播放处于prepared / playing / paused / completed状态时，播放引擎处于工作状态，这需要占用系统较多的运行内存。当客户端暂时不使用播放器时，调用reset()或release()回收内存资源，做好资源利用。
开发建议
当前指导仅介绍如何实现媒体资源播放，在应用开发过程中可能会涉及后台播放、播放冲突等情况，请根据实际需要参考以下说明。
开发步骤及注意事项
在 CMake 脚本中链接动态库：
使用OH_AVPlayer_SetOnInfoCallback()、OH_AVPlayer_SetOnErrorCallback()接口设置信息监听回调函数和错误监听回调函数，需要在 CMake 脚本中链接如下动态库：
开发者使用系统日志能力时，需引入如下头文件：
并需要在 CMake 脚本中链接如下动态库:
开发者通过引入avplayer.h、avpalyer_base.h和native_averrors.h头文件，使用视频播放相关API。
详细的API说明请参考AVPlayer API。
1.  创建AVPlayer实例：调用OH_AVPlayer_Create()，AVPlayer初始化为AV_IDLE状态。
2.  设置回调监听函数：使用OH_AVPlayer_SetOnInfoCallback()、OH_AVPlayer_SetOnErrorCallback()接口设置信息监听回调函数和错误监听回调函数，搭配全流程场景使用。支持的监听事件包括： 应用使用OH_AVPlayer_SetOnInfoCallback()、OH_AVPlayer_SetOnErrorCallback()接口设置信息监听回调函数和错误监听回调函数，可以获取更多信息，还可以通过设置 userData 区分不同播放实例。
3.  设置资源：调用OH_AVPlayer_SetURLSource()，设置属性url，AVPlayer进入AV_INITIALIZED状态。
4.  （可选）设置音频流类型：调用OH_AVPlayer_SetAudioRendererInfo()，设置AVPlayer音频流类型。
5.  （可选）设置音频打断模式：调用OH_AVPlayer_SetAudioInterruptMode()，设置AVPlayer音频流打断模式。
6.  设置播放画面窗口：调用OH_AVPlayer_SetVideoSurface()设置播放画面窗口。此函数必须在SetSource之后，Prepare之前调用。
7.  准备播放：调用OH_AVPlayer_Prepare()，AVPlayer进入AV_PREPARED状态，此时可以获取时长，设置音量。
8.  （可选）设置音频音效模式：调用OH_AVPlayer_SetAudioEffectMode()，设置AVPlayer音频音效模式。
9.  视频播控：播放OH_AVPlayer_Play()，暂停OH_AVPlayer_Pause()，跳转OH_AVPlayer_Seek()，停止OH_AVPlayer_Stop()等操作。
10.  （可选）更换资源：调用OH_AVPlayer_Reset()重置资源，AVPlayer重新进入AV_IDLE状态，允许更换资源url。
11.  退出播放：调用OH_AVPlayer_Release()销毁实例，AVPlayer进入AV_RELEASED状态，退出播放。之后再操作AVPlayer实例则行为未知，可能导致应用进程崩溃，应用闪退。
| 事件类型 | 说明 |
| --- | --- |
| OH_AVPlayerOnInfoCallback | 必要事件，监听播放器的过程信息。 |
| OH_AVPlayerOnErrorCallback | 必要事件，监听播放器的错误信息。 |
完整示例

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/media-recording-c-V14
爬取时间: 2025-04-28 20:38:52
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/using-avscreencapture-for-buffer-V14
爬取时间: 2025-04-28 20:39:46
来源: Huawei Developer
屏幕录制主要为主屏幕录屏功能。
开发者可以调用录屏（AVScreenCapture）模块的C API接口，完成屏幕录制，采集设备内、麦克风等的音视频源数据。当开发直播、办公等应用时，可以调用录屏模块获取音视频原始码流，然后通过流的方式流转到其他模块处理，达成直播时共享桌面的场景。
录屏模块和窗口（Window）、图形（Graphic）等模块协同完成整个视频采集的流程。
当前在进行屏幕录制时默认使用主屏，图形默认根据主屏生产录屏帧数据到显示数据缓冲队列，录屏框架从显示数据缓冲队列获取数据进行相应处理。
使用AVScreenCapture录制屏幕涉及到AVScreenCapture实例的创建、音视频采集参数的配置、采集的开始与停止、资源的释放等。
开始屏幕录制时正在通话中或者屏幕录制过程中来电，录屏将自动停止。因通话中断的录屏会上报OH_SCREEN_CAPTURE_STATE_STOPPED_BY_CALL状态。
屏幕录制过程中发生系统用户切换事件时，录屏将自动停止。因系统用户切换中断的录屏会上报OH_SCREEN_CAPTURE_STATE_STOPPED_BY_USER_SWITCHES状态。
本开发指导将以完成一次屏幕数据录制的过程为例，向开发者讲解如何使用AVScreenCapture进行屏幕录制，详细的API声明请参考AVScreenCapture API参考。
如果配置了采集麦克风音频数据，需对应配置麦克风权限ohos.permission.MICROPHONE和申请长时任务，配置方式请参见向用户申请权限、申请长时任务。
开发步骤及注意事项
使用AVScreenCapture时要明确其状态的变化，在创建实例后，调用对应的方法可以进入指定的状态实现对应的行为。
在确定的状态下执行不合适的方法会导致AVScreenCapture发生错误，开发者需要在调用状态转换的方法前进行状态检查，避免程序运行异常。
在 CMake 脚本中链接动态库
1.  添加头文件。
2.  判断当前是否存在未结束的录屏服务实例，若存在，则先停止并释放资源。
3.  创建AVScreenCapture实例capture。
4.  配置屏幕录制参数。 创建AVScreenCapture实例capture后，可以设置屏幕录制所需要的参数，音频信息和视频信息的具体参数配置可参考详细说明。
5.  设置麦克风开关。(可选)
6.  回调函数的设置，主要监听录屏过程中的错误事件的发生，音频流和视频流数据的产生事件，具体设计可参考详细说明。
7.  调用StartScreenCapture()方法开始进行屏幕录制。 或调用StartScreenCaptureWithSurface()方法以Surface模式进行屏幕录制。
8.  调用StopScreenCapture()方法停止录制，具体设计可参考详细说明。
9.  调用Release()方法销毁实例，释放资源。
2in1设备录屏窗口选择界面规格说明
基于录屏取码流接口提供了2in1设备录屏窗口选择界面，为兼容已有的接口设计，目前支持三方应用在指定屏幕模式(OH_CAPTURE_SPECIFIED_SCREEN)、传一个窗口Id的指定窗口模式(OH_CAPTURE_SPECIFIED_WINDOW)下，2in1设备弹出Picker选择弹窗并根据传入的窗口Id选中对应窗口。最终录屏内容以Picker弹出后，用户在弹窗上的选择为准。
2in1设备录屏窗口选择界面推荐在OH_CAPTURE_SPECIFIED_WINDOW模式下使用，需根据2in1设备分辨率配置录屏的高度和宽度值并传入屏幕Id（若有期望录制的某个窗口，可同时传入单个窗口Id）。
另外，2in1设备录屏窗口选择界面兼容以下几种模式的录屏：
1.  OH_CAPTURE_SPECIFIED_WINDOW模式，传入多个窗口Id。 2in1设备不弹Picker选择界面，弹出隐私允许/不允许弹窗，可同时录制多个窗口；
2.  OH_CAPTURE_SPECIFIED_SCREEN模式。 2in1设备弹出Picker选择弹窗，传入的有效屏幕Id作为Picker弹窗上被选中的默认屏幕；
3.  OH_CAPTURE_HOME_SCREEN模式。 2in1设备不弹Picker选择界面，弹出隐私允许/不允许弹窗；
详细说明
针对开发步骤及注意事项中屏幕录制参数配置、回调函数设置、停止录屏服务实例步骤进一步详细说明。
1.  屏幕录制参数配置。
2.  回调函数设置。 针对录屏过程中可能发生的错误事件、状态变化和数据获取，分别设置了相应的事件监听函数。
3.  停止录屏服务并释放资源。
完整示例
下面展示了使用AVScreenCapture屏幕录制的完整示例代码。
创建OH_AVBuffer，可参考视频解码Buffer模式。
使用Surface模式录屏，可参考视频编码Surface模式。
目前阶段流程结束后返回的buffer为原始码流，针对原始码流可以进行编码并以mp4等文件格式保存以供播放。
编码格式当前阶段仅作预留，待后续版本实现。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/using-avscreencapture-for-file-V14
爬取时间: 2025-04-28 20:40:04
来源: Huawei Developer
屏幕录制主要为主屏幕录屏功能。
开发者可以调用录屏（AVScreenCapture）模块的C API接口，完成屏幕录制，采集设备内、麦克风等的音视频源数据。可以调用录屏模块获取音视频文件，然后通过文件的形式流转到其他模块进行播放或处理，达成文件形式分享屏幕内容的场景。
录屏模块和窗口（Window）、图形（Graphic）等模块协同完成整个视频采集的流程。
使用AVScreenCapture录制屏幕涉及到AVScreenCapture实例的创建、音视频采集参数的配置、采集的开始与停止、资源的释放等。
开始屏幕录制时正在通话中或者屏幕录制过程中来电，录屏将自动停止。因通话中断的录屏会上报OH_SCREEN_CAPTURE_STATE_STOPPED_BY_CALL状态。
屏幕录制过程中发生系统用户切换事件时，录屏将自动停止。因系统用户切换中断的录屏会上报OH_SCREEN_CAPTURE_STATE_STOPPED_BY_USER_SWITCHES状态。
本开发指导将以完成一次屏幕数据录制的过程为例，向开发者讲解如何使用AVScreenCapture进行屏幕录制，详细的API声明请参考AVScreenCapture API参考。
如果配置了采集麦克风音频数据，需对应配置麦克风权限ohos.permission.MICROPHONE和申请长时任务，配置方式请参见向用户申请权限、申请长时任务。
开发步骤及注意事项
使用AVScreenCapture时要明确其状态的变化，在创建实例后，调用对应的方法可以进入指定的状态实现对应的行为。
在确定的状态下执行不合适的方法会导致AVScreenCapture发生错误，开发者需要在调用状态转换的方法前进行状态检查，避免程序运行异常。
在 CMake 脚本中链接动态库
1.  添加头文件。
2.  创建AVScreenCapture实例capture。
3.  配置屏幕录制参数。 创建AVScreenCapture实例capture后，可以设置屏幕录制所需要的参数。 其中，录屏存文件时默认录制内录，麦克风可以动态开关，可以同时内外录制。 同时，录屏存文件需要设置状态回调，感知录制状态。
4.  调用StartScreenRecording()方法开始进行屏幕录制。
5.  调用StopScreenRecording()方法停止录制。
6.  调用Release()方法销毁实例，释放资源。
完整示例
下面展示了使用AVScreenCapture屏幕录制存文件的完整示例代码。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/videoprocessing-guidelines-V14
爬取时间: 2025-04-28 20:40:18
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/generate-super-resolution-video-V14
爬取时间: 2025-04-28 20:40:31
来源: Huawei Developer
本模块提供视频细节增强的C API接口，通过调用本模块的接口，可以实现视频流图像内容的清晰度增强及缩放功能，处理后的数据可以用于送显、播放和录制。
典型应用场景如：视频解码 > 视频细节增强 > XComponent显示。
约束与限制
1.  档位 输入分辨率要求（单位：像素） 输出分辨率要求（单位：像素） 说明 NONE 宽：(32,2000] 高：(32,2000] 宽：(32,2000] 高：(32,2000] 仅适用于缩放场景，无清晰度增强效果。 LOW 宽：(32,2000] 高：(32,2000] 宽：(32,2000] 高：(32,2000] 仅适用于缩放场景，等比缩放时无清晰度增强效果。 缩放时会对图像进行低质量的清晰度增强，处理速度较快。 该档位为默认设置。 MEDIUM 宽：(32,2000] 高：(32,2000] 宽：(32,2000] 高：(32,2000] 仅适用于缩放场景，等比缩放时无清晰度增强效果。 缩放时会对图像进行中等质量的清晰度增强，处理速度适中。 HIGH 宽：[512,2000] 高：[512,2000] 宽：[512,2000] 高：[512,2000] 适用于缩放及清晰度增强场景，等比缩放时能进行清晰度增强。 缩放时会对图像进行高质量的清晰度增强，处理速度相对较慢。
| 档位  | 输入分辨率要求（单位：像素）  | 输出分辨率要求（单位：像素）  | 说明  |
| --- | --- | --- | --- |
| NONE  | 宽：(32,2000] 高：(32,2000]  | 宽：(32,2000] 高：(32,2000]  | 仅适用于缩放场景，无清晰度增强效果。  |
| LOW  | 宽：(32,2000] 高：(32,2000]  | 宽：(32,2000] 高：(32,2000]  | 仅适用于缩放场景，等比缩放时无清晰度增强效果。 缩放时会对图像进行低质量的清晰度增强，处理速度较快。 该档位为默认设置。  |
| MEDIUM  | 宽：(32,2000] 高：(32,2000]  | 宽：(32,2000] 高：(32,2000]  | 仅适用于缩放场景，等比缩放时无清晰度增强效果。 缩放时会对图像进行中等质量的清晰度增强，处理速度适中。  |
| HIGH  | 宽：[512,2000] 高：[512,2000]  | 宽：[512,2000] 高：[512,2000]  | 适用于缩放及清晰度增强场景，等比缩放时能进行清晰度增强。 缩放时会对图像进行高质量的清晰度增强，处理速度相对较慢。  |
开发指导
在 CMake 脚本中链接动态库
开发步骤
1.  应用可以通过视频处理引擎模块类型来创建细节增强模块。示例中的变量说明如下：

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/generate-video-dynamic-metadata-V14
爬取时间: 2025-04-28 20:40:45
来源: Huawei Developer
调用者可以调用本模块提供的C API接口，实现HDRVivid标准动态元数据生成。
该能力常用于视频编辑中，如下图所示：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165956.25236948336999388551599472001435:50001231000000:2800:81BAAC71CB244172FDE0D9124C523C59EDE7E752C5EE1232AF8D6F671E58FE2B.png)
规格说明
支持的数据输入格式：
| 元数据类型MetadataType  | 颜色空间ColorSpace  | 像素格式pixelFormat  |
| --- | --- | --- |
| OH_VIDEO_HDR_VIVID  | OH_COLORSPACE_BT2020_PQ_LIMIT  | NATIVEBUFFER_PIXEL_FMT_YCBCR_P010, NATIVEBUFFER_PIXEL_FMT_YCRCB_P010, NATIVEBUFFER_PIXEL_FMT_RGBA_1010102  |
| OH_COLORSPACE_BT2020_HLG_LIMIT  |
| OH_VIDEO_HDR_HLG  | OH_COLORSPACE_BT2020_HLG_LIMIT  |
| OH_VIDEO_HDR_HDR10  | OH_COLORSPACE_BT2020_PQ_LIMIT  |
元数据类型MetadataType
颜色空间ColorSpace
像素格式pixelFormat
OH_VIDEO_HDR_VIVID
OH_COLORSPACE_BT2020_PQ_LIMIT
NATIVEBUFFER_PIXEL_FMT_YCBCR_P010,
NATIVEBUFFER_PIXEL_FMT_YCRCB_P010,
NATIVEBUFFER_PIXEL_FMT_RGBA_1010102
OH_COLORSPACE_BT2020_HLG_LIMIT
OH_VIDEO_HDR_HLG
OH_COLORSPACE_BT2020_HLG_LIMIT
OH_VIDEO_HDR_HDR10
OH_COLORSPACE_BT2020_PQ_LIMIT
支持的分辨率规格：
| 最小分辨率（单位：像素）  | 最大分辨率（单位：像素）  |
| --- | --- |
| 32*32  | 8192*8192  |
最小分辨率（单位：像素）
最大分辨率（单位：像素）
32*32
8192*8192
约束与限制
开发指导
在 CMake 脚本中链接动态库
开发步骤
1.  应用可以通过视频处理引擎模块类型来创建动态元数据生成模块。示例中的变量说明如下：
2.  可以通过XComponent等其他方式获取OHNativeWindow实例，具体参见NativeWindows开发指导。 视频处理引擎的SetSurface的windowOut从XComponent的OnSurfaceCreatedCB回调函数获取，需要对windowOut设置元数据类型、数据格式和颜色空间等参数。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/medialibrary-kit-V14
爬取时间: 2025-04-28 20:42:11
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/photoaccesshelper-overview-V14
爬取时间: 2025-04-28 20:42:24
来源: Huawei Developer
Media Library Kit（媒体文件管理服务）提供了管理相册和媒体文件的能力，包括照片和视频，帮助你的应用快速构建图片视频展示和播放能力。
能力范围
通过Media Library Kit，开发者可以管理相册和媒体文件，包括创建相册以及访问、修改相册中的媒体信息等。
面向所有应用开放如下能力：
面向三方应用受限开放如下能力：
受限开放的能力需要申请相册管理模块的读写操作权限。这部分权限为受控开放，通常是不允许三方应用申请的。如果有特殊场景需要使用，请提供相关申请材料到应用市场（AGC）申请相应权限证书。
亮点/特征
框架原理
媒体库接收用户对媒体资产的获取与变更请求，进行请求合法性检查和权限校验，通过后与数据库进行交互，并返回请求结果。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/photoaccesshelper-photoviewpicker-V14
爬取时间: 2025-04-28 20:43:18
来源: Huawei Developer
用户有时需要分享图片、视频等用户文件，开发者可以通过特定接口拉起系统图库，用户自行选择待分享的资源，然后最终完成分享。此接口本身无需申请权限，目前适用于界面UIAbility，使用窗口组件触发。具体使用方式如下：
1.  导入选择器模块和文件管理模块。
```typescript
import { photoAccessHelper } from '@kit.MediaLibraryKit';
import { fileIo } from '@kit.CoreFileKit';
import { BusinessError } from '@kit.BasicServicesKit';
```
2.  创建图片-音频类型文件选择选项实例。
```typescript
const photoSelectOptions = new photoAccessHelper.PhotoSelectOptions();
```
3.  配置可选的媒体文件类型和媒体文件的最大数目。 以下示例以图片选择为例，媒体文件类型请参见PhotoViewMIMETypes。
```typescript
photoSelectOptions.MIMEType = photoAccessHelper.PhotoViewMIMETypes.IMAGE_TYPE; // 过滤选择媒体文件类型为IMAGE
photoSelectOptions.maxSelectNumber = 5; // 选择媒体文件的最大数目
```
4.  创建图库选择器实例，调用PhotoViewPicker.select接口拉起图库界面进行文件选择。文件选择成功后，返回PhotoSelectResult结果集。 select返回的uri权限是只读权限，可以根据结果集中uri进行读取文件数据操作。注意不能在picker的回调里直接使用此uri进行打开文件操作，需要定义一个全局变量保存uri，类似使用一个按钮去触发打开文件。可参考指定URI读取文件数据。 也可以通过返回的uri获取图片或视频资源。 如有获取元数据需求，可以通过文件管理接口和文件URI根据uri获取部分文件属性信息，比如文件大小、访问时间、修改时间、文件名、文件路径等。
```typescript
let uris: Array<string> = [];
const photoViewPicker = new photoAccessHelper.PhotoViewPicker();
photoViewPicker.select(photoSelectOptions).then((photoSelectResult: photoAccessHelper.PhotoSelectResult) => {
uris = photoSelectResult.photoUris;
console.info('photoViewPicker.select to file succeed and uris are:' + uris);
}).catch((err: BusinessError) => {
console.error(`Invoke photoViewPicker.select failed, code is ${err.code}, message is ${err.message}`);
})
```
指定URI读取文件数据
1.  待界面从图库返回后，再通过一个类似按钮的组件去调用其他函数，使用fileIo.openSync接口，通过媒体文件uri打开这个文件得到fd。这里需要注意接口权限参数是fileIo.OpenMode.READ_ONLY。
```typescript
let uri: string = '';
let file = fileIo.openSync(uri, fileIo.OpenMode.READ_ONLY);
console.info('file fd: ' + file.fd);
```
2.  通过fd使用fileIo.readSync接口读取这个文件内的数据，读取完成后关闭fd。
```typescript
let buffer = new ArrayBuffer(4096);
let readLen = fileIo.readSync(file.fd, buffer);
console.info('readSync data to file succeed and buffer size is:' + readLen);
fileIo.closeSync(file);
```
指定URI获取图片或视频资源
媒体库支持Picker选择媒体文件URI后，根据指定URI获取图片或视频资源，下面以查询指定URI为'file://media/Photo/1/IMG_datetime_0001/displayName.jpg'为例。
```typescript
import { photoAccessHelper } from '@kit.MediaLibraryKit';
import { dataSharePredicates } from '@kit.ArkData';
const context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
class MediaDataHandler implements photoAccessHelper.MediaAssetDataHandler<ArrayBuffer> {
onDataPrepared(data: ArrayBuffer) {
if (data === undefined) {
console.error('Error occurred when preparing data');
return;
}
console.info('on image data prepared');
// 应用自定义对资源数据的处理逻辑
}
}
async function example() {
let predicates: dataSharePredicates.DataSharePredicates = new dataSharePredicates.DataSharePredicates();
let uri = 'file://media/Photo/1/IMG_datetime_0001/displayName.jpg' // 需保证此uri已存在。
predicates.equalTo(photoAccessHelper.PhotoKeys.URI, uri.toString());
let fetchOptions: photoAccessHelper.FetchOptions = {
fetchColumns: [photoAccessHelper.PhotoKeys.TITLE],
predicates: predicates
};
try {
let fetchResult: photoAccessHelper.FetchResult<photoAccessHelper.PhotoAsset> = await phAccessHelper.getAssets(fetchOptions);
let photoAsset: photoAccessHelper.PhotoAsset = await fetchResult.getFirstObject();
console.info('getAssets photoAsset.uri : ' + photoAsset.uri);
// 获取属性值，以标题为例；对于非默认查询的属性，get前需要在fetchColumns中添加对应列名
console.info('title : ' + photoAsset.get(photoAccessHelper.PhotoKeys.TITLE));
// 请求图片资源数据
let requestOptions: photoAccessHelper.RequestOptions = {
deliveryMode: photoAccessHelper.DeliveryMode.HIGH_QUALITY_MODE,
}
await photoAccessHelper.MediaAssetManager.requestImageData(context, photoAsset, requestOptions, new MediaDataHandler());
console.info('requestImageData successfully');
fetchResult.close();
} catch (err) {
console.error('getAssets failed with err: ' + err);
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/photoaccesshelper-savebutton-V14
爬取时间: 2025-04-28 20:43:32
来源: Huawei Developer
当用户需要保存图片、视频等用户文件到图库时，无需在应用中申请相册管理模块权限'ohos.permission.WRITE_IMAGEVIDEO'，应用可以通过安全控件或授权弹窗的方式，将用户指定的媒体资源保存到图库中。
使用安全控件保存媒体库资源
安全控件的介绍可参考安全控件的保存控件。
下面以使用安全控件创建一张图片资源为例。
开发步骤
```typescript
import { photoAccessHelper } from '@kit.MediaLibraryKit';
@Entry
@Component
struct Index {
saveButtonOptions: SaveButtonOptions = {
icon: SaveIconStyle.FULL_FILLED,
text: SaveDescription.SAVE_IMAGE,
buttonType: ButtonType.Capsule
} // 设置安全控件按钮属性
build() {
Row() {
Column() {
SaveButton(this.saveButtonOptions) // 创建安全控件按钮
.onClick(async (event, result: SaveButtonOnClickResult) => {
if (result == SaveButtonOnClickResult.SUCCESS) {
try {
let context = getContext();
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
// 需要确保fileUri对应的资源存在
let fileUri = 'file://com.example.temptest/data/storage/el2/base/haps/entry/files/test.jpg';
let assetChangeRequest: photoAccessHelper.MediaAssetChangeRequest = photoAccessHelper.MediaAssetChangeRequest.createImageAssetRequest(context, fileUri);
await phAccessHelper.applyChanges(assetChangeRequest);
console.info('createAsset successfully, uri: ' + assetChangeRequest.getAsset().uri);
} catch (err) {
console.error(`create asset failed with error: ${err.code}, ${err.message}`);
}
} else {
console.error('SaveButtonOnClickResult create asset failed');
}
})
}
.width('100%')
}
.height('100%')
}
}
```
除了上述通过fileUri从应用沙箱指定资源内容的方式，开发者还可以通过ArrayBuffer的方式添加资源内容，详情请参考addResource接口。
使用弹窗授权保存媒体库资源
下面以弹窗授权的方式保存一张图片资源为例。
开发步骤
```typescript
import { photoAccessHelper } from '@kit.MediaLibraryKit';
import { fileIo } from '@kit.CoreFileKit';
let context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
async function example() {
try {
// 指定待保存到媒体库的位于应用沙箱的图片uri
let srcFileUri = 'file://com.example.temptest/data/storage/el2/base/haps/entry/files/test.jpg';
let srcFileUris: Array<string> = [
srcFileUri
];
// 指定待保存照片的创建选项，包括文件后缀和照片类型，标题和照片子类型可选
let photoCreationConfigs: Array<photoAccessHelper.PhotoCreationConfig> = [
{
title: 'test', // 可选
fileNameExtension: 'jpg',
photoType: photoAccessHelper.PhotoType.IMAGE,
subtype: photoAccessHelper.PhotoSubtype.DEFAULT, // 可选
}
];
// 基于弹窗授权的方式获取媒体库的目标uri
let desFileUris: Array<string> = await phAccessHelper.showAssetsCreationDialog(srcFileUris, photoCreationConfigs);
// 将来源于应用沙箱的照片内容写入媒体库的目标uri
let desFile: fileIo.File = await fileIo.open(desFileUris[0], fileIo.OpenMode.WRITE_ONLY);
let srcFile: fileIo.File = await fileIo.open(srcFileUri, fileIo.OpenMode.READ_ONLY);
await fileIo.copyFile(srcFile.fd, desFile.fd);
fileIo.closeSync(srcFile);
fileIo.closeSync(desFile);
console.info('create asset by dialog successfully');
} catch (err) {
console.error(`failed to create asset by dialog successfully errCode is: ${err.code}, ${err.message}`);
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/component-guidelines-photoviewpicker-V14
爬取时间: 2025-04-28 20:43:45
来源: Huawei Developer
当应用需要读取用户图片时，开发者可以在应用界面中嵌入PhotoPicker组件，在用户选择所需要的图片资源后，直接返回该图片资源，而不需要授予应用读取图片文件的权限，即可完成图片或视频文件的访问和读取。
界面效果如图所示。
| 宫格页  | 大图页  |
| --- | --- |
|   |   |
宫格页
大图页
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165956.08532731226063609102509794685716:50001231000000:2800:F79F2EDA5807AA496C2EF80A456E6438BBBDF298425A4AC49CB6C5A9BE97BB80.jpg)
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165957.69689675892154802774300710301166:50001231000000:2800:FCCD825658F4E1B372D08BFFA38FB063759A3B524287598A2BA213655DD31118.jpg)
开发步骤
1.  通过PickerOptions，开发者可配置Picker宫格的样式（如勾选框背景色、文本颜色等）、滑动预览方向、最大选择数量等参数，详见PickerOptions API参考。 通过PickerController，应用可向Picker组件发送数据。
2.  通过实现以下回调事件，可在用户在界面操作时，将相关信息报给应用进行处理。
3.  存在多种用法，详见PickerControllerAPI文档。
完整示例

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/component-guidelines-albumpicker-V14
爬取时间: 2025-04-28 20:43:59
来源: Huawei Developer
开发者可以在布局中嵌入AlbumPickerComponent组件，通过此组件，应用无需申请权限，即可访问公共目录中的相册列表。
需配合PhotoPickerComponent一起使用，用户通过AlbumPickerComponent组件选择对应相册并通知PhotoPickerComponent组件刷新成对应相册的图片和视频。
界面效果如图所示。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165957.34773381339112817316279413669111:50001231000000:2800:7029C0C8D09A065F70D86151E120BDFD768BA0F76BCBAE349B8EE7CEC460F3A2.png)
开发步骤
1.  导入相册组件模块文件。
```typescript
import { AlbumPickerComponent, AlbumPickerOptions, AlbumInfo} from '@kit.MediaLibraryKit';
```
2.  创建相册组件配置选项实例（AlbumPickerOptions）。
```typescript
// 用于相册组件初始化时设置参数信息
albumOptions: AlbumPickerOptions = new AlbumPickerOptions();
```
```typescript
/**
* 设置相册页颜色模式， 默认AUTO。
* AUTO：跟随系统的模式， LIGHT：浅色模式， DARK：深色模式
*/
this.albumOptions.themeColorMode = PickerColorMode.AUTO;
```
```typescript
AlbumPickerComponent({
// 设置组件选择选项实例
albumPickerOptions: this.albumOptions,
/**
*相册被选中回调，返回相册信息
* AlbumInfo（uri）
*/
onAlbumClick: (albumInfo: AlbumInfo): boolean => this.onAlbumClick(albumInfo),
})
```
```typescript
private onAlbumClick(albumInfo: AlbumInfo): boolean {
if (albumInfo?.uri) {
// 根据相册url更新宫格页内容
this.pickerController.setData(DataType.SET_ALBUM_URI, albumInfo.uri);
}
return true;
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/component-guidelines-recentphoto-V14
爬取时间: 2025-04-28 20:44:13
来源: Huawei Developer
应用可以在布局中嵌入最近图片组件，通过此组件，应用无需申请权限，即可指定配置访问公共目录中最近的一个图片或视频文件。授予的权限仅包含只读权限。
界面效果如图所示。
①为PhotoPicker宫格页，②为RecentPhoto最近图片的效果。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165957.51245371065428131074956234440311:50001231000000:2800:59E54D56C0AFDECD4D60FACE4DCAFA4FA64BFCFBDF4AB3B4E0FD8DC6A4A7B9D2.png)
开发步骤
1.  导入最近图片组件模块文件。
```typescript
import { BaseItemInfo } from '@ohos.file.PhotoPickerComponent';
import photoAccessHelper from '@ohos.file.photoAccessHelper';
import {
PhotoSource,
RecentPhotoCheckResultCallback,
RecentPhotoClickCallback,
RecentPhotoComponent,
RecentPhotoOptions
} from '@ohos.file.RecentPhotoComponent';
```
2.  创建最近图片组件选择选项实例（RecentPhotoOptions）。
```typescript
// 最近图片组件初始化
recentPhotoOptions: RecentPhotoOptions = new RecentPhotoOptions();
```
```typescript
// 设置数据类型， IMAGE_VIDEO_TYPE：图片和视频（默认值）  IMAGE_TYPE：图片   VIDEO_TYPE：视频  MOVING_PHOTO_IMAGE_TYPE 动态图片
this.recentPhotoOptions.MIMEType = photoAccessHelper.PhotoViewMIMETypes.IMAGE_VIDEO_TYPE;
// 设置最近图片的时间范围，单位（秒）， 0表示所有时间。
this.recentPhotoOptions.period = 0;
// 设置资源的来源 ALL：所有 CAMERA：相机  SCREENSHOT：截图
this.recentPhotoOptions.photoSource = PhotoSource.ALL;
```
```typescript
RecentPhotoComponent({
// 设置最近图片组件选择选项实例
recentPhotoOptions: this.recentPhotoOptions,
/**
* 最近图片点击事件
* BaseItemInfo（uri, mimeType, width, height, size, duration）
* return 返回值为true时才会给url授权，才可以显示
*/
onRecentPhotoClick: (recentPhotoInfo: BaseItemInfo): boolean => this.onRecentPhotoClick(recentPhotoInfo),
// 检查是否存在最近的资源
onRecentPhotoCheckResult: (recentPhotoExists: boolean) => this.onReceiveCheckResult(recentPhotoExists),
})
```
3.  实现onReceiveCheckResult回调，可查询是否存在最近图片，仅返回true时才可进一步实现控制是否显示最近图片。
```typescript
// 返回值为true时，才能获取uri的权限
private onRecentPhotoClick(recentPhotoInfo: BaseItemInfo): boolean {
if (!recentPhotoInfo) {
return false;
}
return true;
}
private onReceiveCheckResult(recentPhotoExists: boolean): void {
if (!recentPhotoExists) {
console.info('not exist recent photo');
}
// 存在最近图片的话，可以实现业务需求， 如去控制RecentPhotoComponent是否显示
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/smart-photopicker-V14
爬取时间: 2025-04-28 20:44:27
来源: Huawei Developer
应用在调用PhotoPicker接口时，如果配置了PhotoPicker图片推荐参数，当设备中有满足图片推荐参数的图片，且设备中的图片已经分析完成时，PhotoPicker界面除了展示全量的图片外，还会展示符合条件的推荐图片供用户参考选择，从而缩短用户筛选图片的时间。
-  以指定图片类型为二维码为例，PhotoPicker界面上将出现“二维码”的Tab页，展示图库中的二维码图片。
-  举例说明，如设置的推荐参数文本是“国庆节，带着女儿去了上海野生动物园，看到了凶猛的大象，漂亮的火烈鸟，还有她心心念念的大熊猫，小家伙可开心了。” 而且手机中有相应的图片，图片分析完成时，会在“推荐”的Tab页中展示出时间是国庆节，地点是上海野生动物园的大熊猫、火烈鸟、大象的图片。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165957.02467647195587468756531737686101:50001231000000:2800:2D0D350800BDAA2077603A4A2B724AF6B67C25CF47EF4E6A8347B7DCD8B6C5D1.jpg)
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165957.00017743722784604647472690198547:50001231000000:2800:0E6E93C26D27B60086C2A78CC5431E6449AF0BE498738C94555D0EE01EEEAB15.jpg)
约束与限制
-  当设备在灭屏充电时，在电量充足、温度正常的情况下，系统将自动进行图片分析。 如果图片带有地理位置信息，设备需要联网才可以正常解析图片中的位置信息。
开发方式
前提条件
开发者可以通过两种方式实现PhotoPicker，拉起系统图库。请选择其中一种方式，实现PhotoPicker。
-  在配置参数photoAccessHelper.PhotoSelectOptions时，媒体文件类型MIMEType（PhotoViewMIMETypes）需要配置为IMAGE_TYPE或IMAGE_VIDEO_TYPE。
-  在配置PhotoPickerComponent的属性pickerOptions时，媒体文件类型MIMEType（PhotoViewMIMETypes）需要配置为IMAGE_TYPE或IMAGE_VIDEO_TYPE。
接下来将以通过PhotoPickerComponent为例，指导开发者如何配置PhotoPicker推荐参数，从而实现PhotoPicker推荐图片。
根据特定类型推荐图片
当需要推荐特定类型的图片，如身份证、银行卡、驾驶证、行驶证、二维码等，可通过配置RecommendationOptions.recommendationType，指定推荐的图片类型。支持的图片类型可参考RecommendationType。
当前示例以通过Picker组件实现推荐图片为例。
如果使用Picker接口，需要将推荐参数赋值给 photoSelectOptions.recommendationOptions。
根据文本信息推荐图片
当需要在图文编辑时，根据文本信息推荐图片，可通过配置RecommendationOptions.textContextInfo。
如果RecommendationOptions同时配置了recommendationType和textContextInfo，仅textContextInfo生效。
当前示例以通过Picker组件实现推荐图片为例。
如果使用Picker接口，需要将推荐参数赋值给 photoSelectOptions.recommendationOptions。
示例代码

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/movingphoto-V14
爬取时间: 2025-04-28 20:44:40
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/photoaccesshelper-movingphoto-V14
爬取时间: 2025-04-28 20:44:54
来源: Huawei Developer
动态照片是一种结合了图片和视频的照片形式，可以显示一小段时间的动态画面和声音。可以帮助用户捕捉精彩的动态瞬间，提升创作空间，同时令拍照的容错率更高。
媒体库提供访问和管理动态照片资源的能力，包括：
拍摄动态照片的能力由Camera Kit提供，开发者可参考Camera Kit-动态照片。
保存动态照片资源
使用安全控件保存动态照片资源后，可用于获取MovingPhoto对象，从而完成播放动态照片等操作。
使用安全控件保存动态照片资源，无需申请相册管理模块权限'ohos.permission.WRITE_IMAGEVIDEO'，允许用户通过点击按钮临时获取存储权限，并将资源直接保存到指定的媒体库路径，使得操作更为便捷。
详情请参考安全控件的保存控件。
开发步骤
1.  设置安全控件按钮属性。
2.  创建安全控件按钮。
3.  调用MediaAssetChangeRequest.createAssetRequest接口新建一个创建资产的变更请求，指定待创建资产的子类型为动态照片。
4.  调用MediaAssetChangeRequest.addResource接口指定动态照片的图片和视频内容。 以下示例以从应用沙箱的应用文件fileUri指定动态照片的图片和视频内容为例。 开发者可根据实际情况，通过ArrayBuffer的方式指定资源内容，参考MediaAssetChangeRequest.addResource(type: ResourceType, data: ArrayBuffer)。
5.  调用PhotoAccessHelper.applyChanges接口提交创建资产的变更请求。
```typescript
import { photoAccessHelper } from '@kit.MediaLibraryKit';
@Entry
@Component
struct Index {
@State message: string = 'Hello World'
@State saveButtonOptions: SaveButtonOptions = {
icon: SaveIconStyle.FULL_FILLED,
text: SaveDescription.SAVE_IMAGE,
buttonType: ButtonType.Capsule
} // 设置安全控件按钮属性
build() {
Row() {
Column() {
Text(this.message)
.fontSize(50)
.fontWeight(FontWeight.Bold)
SaveButton(this.saveButtonOptions) // 创建安全控件按钮
.onClick(async (event, result: SaveButtonOnClickResult) => {
if (result == SaveButtonOnClickResult.SUCCESS) {
try {
let context = getContext();
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
// 需要确保imageFileUri和videoFileUri对应的资源存在，分别表示待创建到媒体库的动态照片的图片和视频。
let imageFileUri = 'file://com.example.temptest/data/storage/el2/base/haps/entry/files/create_moving_photo.jpg';
let videoFileUri = 'file://com.example.temptest/data/storage/el2/base/haps/entry/files/create_moving_photo.mp4';
let assetChangeRequest: photoAccessHelper.MediaAssetChangeRequest = photoAccessHelper.MediaAssetChangeRequest.createAssetRequest(context, photoAccessHelper.PhotoType.IMAGE, "jpg", {
title: "moving_photo",
subtype: photoAccessHelper.PhotoSubtype.MOVING_PHOTO
});
assetChangeRequest.addResource(photoAccessHelper.ResourceType.IMAGE_RESOURCE, imageFileUri);
assetChangeRequest.addResource(photoAccessHelper.ResourceType.VIDEO_RESOURCE, videoFileUri);
await phAccessHelper.applyChanges(assetChangeRequest);
console.info('create moving photo successfully, uri: ' + assetChangeRequest.getAsset().uri);
} catch (err) {
console.error(`create moving photo failed with error: ${err.code}, ${err.message}`);
}
} else {
console.error('SaveButtonOnClickResult create moving photo failed');
}
})
}
.width('100%')
}
.height('100%')
}
}
```
获取动态照片对象
-  应用可以通过Picker的方式获取用户媒体库里的动态照片对象，后续可用于在应用内播放动态照片，或是读取动态照片资源进行其他操作（如上传到应用共享给他人浏览等）。
-  应用也可以通过传入应用沙箱的应用文件图片和视频fileUri的方式构造应用本地的动态照片对象。
获取到动态照片对象后，如需播放动态照片请使用MovingPhotoView组件。
获取媒体库动态照片对象
```typescript
import { photoAccessHelper } from '@kit.MediaLibraryKit';
import { dataSharePredicates } from '@kit.ArkData';
let context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
async function example() {
try {
// picker选择动态照片uri
let photoSelectOptions = new photoAccessHelper.PhotoSelectOptions();
photoSelectOptions.MIMEType = photoAccessHelper.PhotoViewMIMETypes.MOVING_PHOTO_IMAGE_TYPE;
photoSelectOptions.maxSelectNumber = 9;
let photoViewPicker = new photoAccessHelper.PhotoViewPicker();
let photoSelectResult = await photoViewPicker.select(photoSelectOptions);
let uris = photoSelectResult.photoUris;
for (let i = 0; i < uris.length; i++) {
// 获取uri对应的PhotoAsset资产
let predicates: dataSharePredicates.DataSharePredicates = new dataSharePredicates.DataSharePredicates();
predicates.equalTo(photoAccessHelper.PhotoKeys.URI, uris[i]);
let fetchOption: photoAccessHelper.FetchOptions = {
fetchColumns: [],
predicates: predicates
};
let fetchResult: photoAccessHelper.FetchResult<photoAccessHelper.PhotoAsset> = await phAccessHelper.getAssets(fetchOption);
let photoAsset: photoAccessHelper.PhotoAsset = await fetchResult.getFirstObject();
// 获取PhotoAsset对应的动态照片对象
await photoAccessHelper.MediaAssetManager.requestMovingPhoto(context, photoAsset, {
deliveryMode: photoAccessHelper.DeliveryMode.FAST_MODE
}, {
async onDataPrepared(movingPhoto: photoAccessHelper.MovingPhoto) {
if (movingPhoto !== undefined) {
// 应用可自定义对movingPhoto的处理逻辑
console.info('request moving photo successfully, uri: ' + movingPhoto.getUri());
}
}
})
}
} catch (err) {
console.error(`request moving photo failed with error: ${err.code}, ${err.message}`);
}
}
```
获取应用沙箱动态照片对象
调用MediaAssetManager.loadMovingPhoto加载应用沙箱的动态照片对象（MovingPhoto）。
```typescript
import { photoAccessHelper } from '@kit.MediaLibraryKit';
let context = getContext(this);
async function example() {
try {
let imageFileUri = 'file://com.example.temptest/data/storage/el2/base/haps/entry/files/local_moving_photo.jpg';
let videoFileUri = 'file://com.example.temptest/data/storage/el2/base/haps/entry/files/local_moving_photo.mp4';
let movingPhoto = await photoAccessHelper.MediaAssetManager.loadMovingPhoto(context, imageFileUri, videoFileUri);
console.info('load moving photo successfully');
} catch (err) {
console.error(`load moving photo failed with error: ${err.code}, ${err.message}`);
}
}
```
读取动态照片资源
对于一个动态照片对象，应用可以通过MovingPhoto.requestContent导出图片和视频到应用沙箱，或者读取图片或视频的ArrayBuffer内容。
```typescript
import { photoAccessHelper } from '@kit.MediaLibraryKit';
async function example(movingPhoto: photoAccessHelper.MovingPhoto) {
try {
let imageFileUri = 'file://com.example.temptest/data/storage/el2/base/haps/entry/files/request_moving_photo.jpg';
let videoFileUri = 'file://com.example.temptest/data/storage/el2/base/haps/entry/files/request_moving_photo.mp4';
await movingPhoto.requestContent(imageFileUri, videoFileUri); // 将动态照片导出到应用沙箱
let imageData = await movingPhoto.requestContent(photoAccessHelper.ResourceType.IMAGE_RESOURCE); // 读取图片的ArrayBuffer内容
let videoData = await movingPhoto.requestContent(photoAccessHelper.ResourceType.VIDEO_RESOURCE); // 读取视频的ArrayBuffer内容
} catch (err) {
console.error(`request content of moving photo failed with error: ${err.code}, ${err.message}`);
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/movingphotoview-guidelines-V14
爬取时间: 2025-04-28 20:45:08
来源: Huawei Developer
系统提供了MovingPhotoView组件，在一些社交类、图库类应用中，可用于播放动态照片文件。
约束与限制
针对MovingPhotoView组件的使用，有以下约束与限制：
开发步骤
1.  导入动态照片模块。
```typescript
import { MovingPhotoView, MovingPhotoViewController, MovingPhotoViewAttribute } from '@kit.MediaLibraryKit';
```
2.  获取动态照片对象（MovingPhoto）。 MovingPhoto对象需要通过photoAccessHelper接口创建或获取，MovingPhotoView只接收构造完成的MovingPhoto对象。 创建、获取的方式可参考访问和管理动态照片资源。
```typescript
src: photoAccessHelper.MovingPhoto | undefined = undefined;
```
3.  创建动态照片控制器（MovingPhotoViewController），用于控制动态照片的播放状态（如播放、停止）。
```typescript
controller: MovingPhotoViewController = new MovingPhotoViewController();
```
4.  创建动态照片组件。 以下参数取值仅为举例，具体每个属性的取值范围，可参考API文档：@ohos.multimedia.movingphotoview。
```typescript
import { photoAccessHelper, MovingPhotoView, MovingPhotoViewController, MovingPhotoViewAttribute } from '@kit.MediaLibraryKit';
@Entry
@Component
struct Index {
@State src: photoAccessHelper.MovingPhoto | undefined = undefined
@State isMuted: boolean = false
controller: MovingPhotoViewController = new MovingPhotoViewController();
build() {
Column() {
MovingPhotoView({
movingPhoto: this.src,
controller: this.controller
// imageAIOptions: this.options
})
// 是否静音播放，此处由按钮控制，默认值为false非静音播放。
.muted(this.isMuted)
// 视频显示模式，默认值为Cover。
.objectFit(ImageFit.Cover)
// 播放时触发
.onStart(() => {
console.log('onStart');
})
// 播放结束触发
.onFinish(() => {
console.log('onFinish');
})
// 播放停止触发
.onStop(() => {
console.log('onStop')
})
// 出现错误触发
.onError(() => {
console.log('onError');
})
Row() {
// 按钮：开始播放
Button('start')
.onClick(() => {
this.controller.startPlayback()
})
.margin(5)
// 按钮：停止播放
Button('stop')
.onClick(() => {
this.controller.stopPlayback()
})
.margin(5)
// 按钮：是否静音播放
Button('mute')
.onClick(() => {
this.isMuted = !this.isMuted
})
.margin(5)
}
.alignItems(VerticalAlign.Center)
.justifyContent(FlexAlign.Center)
.height('15%')
}
}
}
```
效果展示
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165958.52775492479402881670101398310999:50001231000000:2800:B2CB81502F62B0F785E22B689564DC8D8182D68752C0248033D3D2DDC4518212.gif)

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/medialibrary-request-photouris-permission-V14
爬取时间: 2025-04-28 20:45:22
来源: Huawei Developer
应用在HarmonyOS 3.1 Release API 9及更低版本运行时，有图片/视频访问权限，并在应用内记录对应的图片/视频文件路径或uri，在进入应用特定界面时，可实时访问图片/视频显示内容。
但在设备从HarmonyOS 3.1 Release API 9及更低版本升级至HarmonyOS 5.0.2及以上版本时，图片、视频等媒体文件的访问方式发生变化，应用无法使用原来的文件路径或uri访问媒体文件，且新版本上应用默认没有权限直接访问图片/视频。在新版本上，应用需要向用户发起请求，用户同意后，可继承原有的媒体文件访问权限。
本指南将帮助开发者了解如何在升级后，继承媒体文件的访问权限。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165958.87527874775525672917662253919608:50001231000000:2800:8CD28E7B11C1512F367F530A6D7BEE0D4A36F7551E36404C37250D3E5C4E9C65.png)
应用在启动或是进入对应的业务界面之后，从应用数据中获取在HarmonyOS 3.1/4.0版本的应用上已有权限且需要继承权限的媒体文件uri，调用Scenario Fusion Kit的接口convertFileUris()，获取转换后的HarmonyOS 5.0可访问的uri。再调用Media Library Kit的接口requestPhotoUrisReadPermission()，输入需要继承访问权限的媒体文件uri，拉起授权界面。在授权界面，根据应用输入的uri，将显示对应图片/视频缩略图。用户可以勾选对应的图片/视频，并同意授权，此时应用将获取该图片/视频的访问权限。
在用户界面的效果如图所示。
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165958.68952608820182831834756896335434:50001231000000:2800:153C6D6797BDF15126D53E3F671350BAD89A26F8C302300EA931BE86A0D309F0.png)
开发步骤
此处仅展示如何调用Media Library Kit的接口requestPhotoUrisReadPermission()，输入需要继承访问权限的媒体文件uri，拉起授权界面。
调用Scenario Fusion Kit的接口convertFileUris()，获取转换后的HarmonyOS 5.0可访问的uri，请参考公共目录文件uri继承。
1.  导入相关接口模块文件。
```typescript
import { AlbumPickerComponent, AlbumPickerOptions, AlbumInfo} from '@kit.MediaLibraryKit';
import { photoAccessHelper } from '@kit.MediaLibraryKit'；
```
```typescript
// 用于初始化时接口类实例
let phAccessHelper: photoAccessHelper.PhotoAccessHelper = photoAccessHelper.getPhotoAccessHelper(getContext(this));
```
```typescript
private uris: Array<string> = new Array<string>();
// 自行对其赋值，输入需要授权的uri信息
this.uris = [];
```
```typescript
try {
phAccessHelper.requestPhotoUrisReadPermission(uri).then((result: Array<string>) => {
console.info("requestPhotoUrisReadPermission, result = " + JSON.stringify(result));
if (result) {
// 授权成功返回授权后单框架的uri列表
} else {
// 授权失败后的处理
}
})
} catch(error) {
console.error("requestPhotoUrisReadPermission error: " + JSON.stringify(error))
}
```
完整示例
```typescript
import { AlbumPickerComponent, AlbumPickerOptions, AlbumInfo} from '@kit.MediaLibraryKit';
import { photoAccessHelper } from '@kit.MediaLibraryKit'；
@Entry
@Component
struct Index{
private uris: Array<string> = new Array<string>();
build() {
Row() {
Column() {
Button("拉起授权界面").width('100%').height('10%').margin({top: 150})
.onClick(()=>{
// 自行对其赋值，输入需要授权的uri信息
this.uris = [];
let phAccessHelper: photoAccessHelper.PhotoAccessHelper = photoAccessHelper.getPhotoAccessHelper(getContext(this));
try {
phAccessHelper.requestPhotoUrisReadPermission(this.uris).then((result: Array<string>) => {
console.info("requestPhotoUrisReadPermission, result = " + JSON.stringify(result));
if (result) {
// 授权成功返回授权后单框架的uri列表
} else {
// 授权失败后的处理
}
})
} catch(error) {
console.error("requestPhotoUrisReadPermission error: " + JSON.stringify(error))
}
})
}
.width('100%')
}
.height('100%')
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/restricted-open-capabilities-V14
爬取时间: 2025-04-28 20:45:36
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/photoaccesshelper-preparation-V14
爬取时间: 2025-04-28 20:45:50
来源: Huawei Developer
应用需要先获取相册管理模块实例，才能访问和修改相册中的媒体数据信息。相册管理模块涉及用户个人数据信息，所以应用需要向用户申请相册管理模块读写操作权限才能保证功能的正常运行。在使用相册管理模块相关接口时如无其他注明则默认在工程代码的pages/index.ets或者其他自创的ets文件中使用。
获取相册管理模块实例
应用需要使用应用上下文Context通过接口getPhotoAccessHelper，获取相册管理模块实例，用于访问和修改相册中媒体数据信息（如图片、视频）。
开发步骤
```typescript
import { photoAccessHelper } from '@kit.MediaLibraryKit';
// 此处获取的photoAccessHelper实例为全局对象，后续文档中使用到的地方默认为使用此处获取的对象，如未添加此段代码报未定义的错误请自行添加。
const context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
```
申请相册管理模块功能相关权限
相册管理模块的读写操作需要相应权限，在申请权限前，请保证符合权限使用的基本原则。涉及的权限如下表。
| 权限名 | 说明 | 授权方式 |
| --- | --- | --- |
| ohos.permission.READ_IMAGEVIDEO | 允许应用读取媒体库的图片和视频媒体文件信息。 | user_grant |
| ohos.permission.WRITE_IMAGEVIDEO | 允许应用读写媒体库的图片和视频媒体文件信息。 | user_grant |
以上权限的授权方式均为user_grant（用户授权），即开发者在module.json5文件中配置对应的权限后，需要使用接口abilityAccessCtrl.requestPermissionsFromUser去校验当前用户是否已授权。如果是，应用可以直接访问/操作目标对象；否则需要弹框向用户申请授权。
开发步骤
即使用户曾经授予权限，应用在调用受此权限保护的接口前，也应该先检查是否有权限。不能把之前授予的状态持久化，因为用户在动态授予后还可以通过“设置”取消应用的权限。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/photoaccesshelper-resource-guidelines-V14
爬取时间: 2025-04-28 20:46:04
来源: Huawei Developer
应用可以通过photoAccessHelper的接口，对媒体资源（图片、视频）进行相关操作。
为了保证应用的运行效率，大部分photoAccessHelper的接口调用都是异步的。以下异步调用的API示例均采用Promise函数，更多方式可以查阅API参考。
获取指定媒体资源
开发者可以根据特定的条件查询媒体资源，如指定类型、指定日期、指定相册等。
应用通过调用PhotoAccessHelper.getAssets获取媒体资源，并传入FetchOptions对象指定检索条件。如无特别说明，文档中涉及的待获取的资源均视为已经预置且在数据库中存在相应数据。如出现按照示例代码执行出现获取资源为空的情况请确认文件是否已预置，数据库中是否存在该文件的数据。
如果只想获取某个位置的对象（如第一个、最后一个、指定索引等），可以通过FetchResult中的接口获取对应位置的媒体资源对象。
前提条件
指定媒体文件名获取图片或视频资源
下面以查询文件名为'test.jpg'的图片资源为例。
```typescript
import { dataSharePredicates } from '@kit.ArkData';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
const context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
async function example() {
let predicates: dataSharePredicates.DataSharePredicates = new dataSharePredicates.DataSharePredicates();
predicates.equalTo(photoAccessHelper.PhotoKeys.DISPLAY_NAME, 'test.jpg');
let fetchOptions: photoAccessHelper.FetchOptions = {
fetchColumns: [],
predicates: predicates
};
try {
let fetchResult: photoAccessHelper.FetchResult<photoAccessHelper.PhotoAsset> = await phAccessHelper.getAssets(fetchOptions);
let photoAsset: photoAccessHelper.PhotoAsset = await fetchResult.getFirstObject();
console.info('getAssets photoAsset.displayName : ' + photoAsset.displayName);
fetchResult.close();
} catch (err) {
console.error('getAssets failed with err: ' + err);
}
}
```
获取图片和视频缩略图
通过接口PhotoAsset.getThumbnail，传入缩略图尺寸，可以获取图片和视频缩略图。缩略图常用于UI界面展示。
前提条件
获取某张图片的缩略图
当需要在相册展示图片和视频、编辑预览，应用需要获取某张图片的缩略图。
参考以下示例，获取图片的文件描述符fd后，需要解码为统一的PixelMap，方便在应用中进行图片显示或图片处理，具体请参考图片解码。
下面以获取一张图片的缩略图为例，缩略图尺寸为720*720。
开发步骤
```typescript
import { dataSharePredicates } from '@kit.ArkData';
import { image } from '@kit.ImageKit';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
const context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
async function example() {
let predicates: dataSharePredicates.DataSharePredicates = new dataSharePredicates.DataSharePredicates();
let fetchOptions: photoAccessHelper.FetchOptions = {
fetchColumns: [],
predicates: predicates
};
try {
let fetchResult: photoAccessHelper.FetchResult<photoAccessHelper.PhotoAsset> = await phAccessHelper.getAssets(fetchOptions);
let photoAsset: photoAccessHelper.PhotoAsset = await fetchResult.getFirstObject();
console.info('getAssets photoAsset.displayName : ' + photoAsset.displayName);
let size: image.Size = { width: 720, height: 720 };
let pixelMap: image.PixelMap =  await photoAsset.getThumbnail(size);
let imageInfo: image.ImageInfo = await pixelMap.getImageInfo()
console.info('getThumbnail successful, pixelMap ImageInfo size: ' + JSON.stringify(imageInfo.size));
fetchResult.close();
} catch (err) {
console.error('getThumbnail failed with err: ' + err);
}
}
```
重命名媒体资源
重命名修改的是文件的PhotoAsset.displayName属性，即文件的显示文件名，包含文件后缀。
调用MediaAssetChangeRequest.setTitle重命名后再通过PhotoAccessHelper.applyChanges更新到数据库中完成修改。
在重命名文件之前，需要先获取文件对象，可以通过FetchResult中的接口获取对应位置的文件。
前提条件
下面以将获取的图片资源中第一个文件重命名为例。
开发步骤
```typescript
import { dataSharePredicates } from '@kit.ArkData';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
let context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
async function example() {
let predicates: dataSharePredicates.DataSharePredicates = new dataSharePredicates.DataSharePredicates();
let fetchOptions: photoAccessHelper.FetchOptions = {
fetchColumns: ['title'],
predicates: predicates
};
let newTitle: string = 'newTestPhoto';
try {
let fetchResult: photoAccessHelper.FetchResult<photoAccessHelper.PhotoAsset> = await phAccessHelper.getAssets(fetchOptions);
let photoAsset: photoAccessHelper.PhotoAsset = await fetchResult.getFirstObject();
let assetChangeRequest: photoAccessHelper.MediaAssetChangeRequest = new photoAccessHelper.MediaAssetChangeRequest(photoAsset);
assetChangeRequest.setTitle(newTitle);
await phAccessHelper.applyChanges(assetChangeRequest);
fetchResult.close();
} catch (err) {
console.error(`rename failed with error: ${err.code}, ${err.message}`);
}
}
```
将文件放入回收站
通过MediaAssetChangeRequest.deleteAssets可以将文件放入回收站。
放入回收站的文件将会保存30天，30天后会自动彻底删除。在此期间，应用用户可以通过系统应用“文件管理”或“图库”恢复文件。
前提条件
下面以将文件检索结果中第一个文件放入回收站为例。
开发步骤
```typescript
import { dataSharePredicates } from '@kit.ArkData';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
let context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
async function example() {
let predicates: dataSharePredicates.DataSharePredicates = new dataSharePredicates.DataSharePredicates();
let fetchOptions: photoAccessHelper.FetchOptions = {
fetchColumns: [],
predicates: predicates
};
try {
let fetchResult: photoAccessHelper.FetchResult<photoAccessHelper.PhotoAsset> = await phAccessHelper.getAssets(fetchOptions);
let photoAsset: photoAccessHelper.PhotoAsset = await fetchResult.getFirstObject();
await photoAccessHelper.MediaAssetChangeRequest.deleteAssets(context, [photoAsset]);
fetchResult.close();
} catch (err) {
console.error(`deleteAssets failed with error: ${err.code}, ${err.message}`);
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/photoaccesshelper-useralbum-guidelines-V14
爬取时间: 2025-04-28 20:46:17
来源: Huawei Developer
photoAccessHelper提供用户相册相关的接口，供开发者创建、删除用户相册，往用户相册中添加和删除图片和视频资源等。
在进行功能开发前，请开发者查阅开发准备，了解如何获取相册管理模块实例和如何申请相册管理模块功能开发相关权限。
文档中使用到photoAccessHelper的地方默认为使用开发准备中获取的对象，如未添加此段代码报photoAccessHelper未定义的错误请自行添加。
为了保证应用的运行效率，大部分photoAccessHelper的接口调用都是异步的。以下异步调用的API示例均采用Promise函数，更多方式可以查阅API参考。
如无特别说明，文档中涉及的待获取的资源均视为已经预置且在数据库中存在相应数据。如出现按照示例代码执行出现获取资源为空的情况请确认文件是否已预置，数据库中是否存在该文件的数据。
获取用户相册
通过PhotoAccessHelper.getAlbums接口获取用户相册。
前提条件
下面以获取一个相册名为'albumName'的用户相册为例。
开发步骤
```typescript
import { dataSharePredicates } from '@kit.ArkData';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
const context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
async function example() {
let predicates: dataSharePredicates.DataSharePredicates = new dataSharePredicates.DataSharePredicates();
let albumName: photoAccessHelper.AlbumKeys = photoAccessHelper.AlbumKeys.ALBUM_NAME;
predicates.equalTo(albumName, 'albumName');
let fetchOptions: photoAccessHelper.FetchOptions = {
fetchColumns: [],
predicates: predicates
};
try {
let fetchResult: photoAccessHelper.FetchResult<photoAccessHelper.Album> = await phAccessHelper.getAlbums(photoAccessHelper.AlbumType.USER, photoAccessHelper.AlbumSubtype.USER_GENERIC, fetchOptions);
let album: photoAccessHelper.Album = await fetchResult.getFirstObject();
console.info('getAlbums successfully, albumName: ' + album.albumName);
fetchResult.close();
} catch (err) {
console.error('getAlbums failed with err: ' + err);
}
}
```
重命名用户相册
重命名用户相册修改的是相册的Album.albumName属性。
调用MediaAlbumChangeRequest.setAlbumName重命名用户相册后再通过PhotoAccessHelper.applyChanges更新到数据库中完成修改。
在重命名用户相册之前，需要先获取相册对象，可以通过FetchResult中的接口获取对应位置的用户相册。
重命名的相册名参数规格为：
-  不允许出现的非法英文字符，包括： . \ / : * ? " ' ` < > | { } [ ]
前提条件
下面以将一个相册名为'albumName'的用户相册重命名为例。
开发步骤
```typescript
import { dataSharePredicates } from '@kit.ArkData';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
const context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
async function example() {
let predicates: dataSharePredicates.DataSharePredicates = new dataSharePredicates.DataSharePredicates();
let albumName: photoAccessHelper.AlbumKeys = photoAccessHelper.AlbumKeys.ALBUM_NAME;
predicates.equalTo(albumName, 'albumName');
let fetchOptions: photoAccessHelper.FetchOptions = {
fetchColumns: [],
predicates: predicates
};
try {
let fetchResult: photoAccessHelper.FetchResult<photoAccessHelper.Album> = await phAccessHelper.getAlbums(photoAccessHelper.AlbumType.USER, photoAccessHelper.AlbumSubtype.USER_GENERIC, fetchOptions);
let album: photoAccessHelper.Album = await fetchResult.getFirstObject();
console.info('getAlbums successfully, albumName: ' + album.albumName);
let albumChangeRequest: photoAccessHelper.MediaAlbumChangeRequest = new photoAccessHelper.MediaAlbumChangeRequest(album);
let newAlbumName: string = 'newAlbumName';
albumChangeRequest.setAlbumName(newAlbumName);
await phAccessHelper.applyChanges(albumChangeRequest);
console.info('setAlbumName successfully, new albumName: ' + album.albumName);
fetchResult.close();
} catch (err) {
console.error('setAlbumName failed with err: ' + err);
}
}
```
添加图片和视频到用户相册中
先获取用户相册对象和需要添加到相册中的图片或视频的对象数组，然后调用MediaAlbumChangeRequest.addAssets和PhotoAccessHelper.applyChanges接口往用户相册中添加图片或视频。
前提条件
下面以将往相册名为'albumName'的用户相册中添加一张图片为例。
开发步骤
```typescript
import { dataSharePredicates } from '@kit.ArkData';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
const context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
async function example() {
let albumPredicates: dataSharePredicates.DataSharePredicates = new dataSharePredicates.DataSharePredicates();
let albumName: photoAccessHelper.AlbumKeys = photoAccessHelper.AlbumKeys.ALBUM_NAME;
albumPredicates.equalTo(albumName, 'albumName');
let albumFetchOptions: photoAccessHelper.FetchOptions = {
fetchColumns: [],
predicates: albumPredicates
};
let photoPredicates: dataSharePredicates.DataSharePredicates = new dataSharePredicates.DataSharePredicates();
let photoFetchOptions: photoAccessHelper.FetchOptions = {
fetchColumns: [],
predicates: photoPredicates
};
try {
let albumFetchResult: photoAccessHelper.FetchResult<photoAccessHelper.Album> = await phAccessHelper.getAlbums(photoAccessHelper.AlbumType.USER, photoAccessHelper.AlbumSubtype.USER_GENERIC, albumFetchOptions);
let album: photoAccessHelper.Album = await albumFetchResult.getFirstObject();
console.info('getAlbums successfully, albumName: ' + album.albumName);
let photoFetchResult: photoAccessHelper.FetchResult<photoAccessHelper.PhotoAsset> = await phAccessHelper.getAssets(photoFetchOptions);
let photoAsset: photoAccessHelper.PhotoAsset = await photoFetchResult.getFirstObject();
console.info('getAssets successfully, albumName: ' + photoAsset.displayName);
let albumChangeRequest: photoAccessHelper.MediaAlbumChangeRequest = new photoAccessHelper.MediaAlbumChangeRequest(album);
albumChangeRequest.addAssets([photoAsset]);
await phAccessHelper.applyChanges(albumChangeRequest);
console.info('succeed to add ' + photoAsset.displayName + ' to ' + album.albumName);
albumFetchResult.close();
photoFetchResult.close();
} catch (err) {
console.error('addAssets failed with err: ' + err);
}
}
```
获取用户相册中的图片和视频
先获取用户相册对象，然后调用Album.getAssets接口获取用户相册中的图片资源。
前提条件
下面以获取相册名为'albumName'的用户相册中的一张图片为例。
开发步骤
```typescript
import { dataSharePredicates } from '@kit.ArkData';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
const context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
async function example() {
let albumPredicates: dataSharePredicates.DataSharePredicates = new dataSharePredicates.DataSharePredicates();
let albumName: photoAccessHelper.AlbumKeys = photoAccessHelper.AlbumKeys.ALBUM_NAME;
albumPredicates.equalTo(albumName, 'albumName');
let albumFetchOptions: photoAccessHelper.FetchOptions = {
fetchColumns: [],
predicates: albumPredicates
};
let photoPredicates: dataSharePredicates.DataSharePredicates = new dataSharePredicates.DataSharePredicates();
let photoFetchOptions: photoAccessHelper.FetchOptions = {
fetchColumns: [],
predicates: photoPredicates
};
try {
let albumFetchResult: photoAccessHelper.FetchResult<photoAccessHelper.Album> = await phAccessHelper.getAlbums(photoAccessHelper.AlbumType.USER, photoAccessHelper.AlbumSubtype.USER_GENERIC, albumFetchOptions);
let album: photoAccessHelper.Album = await albumFetchResult.getFirstObject();
console.info('getAlbums successfully, albumName: ' + album.albumName);
let photoFetchResult = await album.getAssets(photoFetchOptions);
let photoAsset = await photoFetchResult.getFirstObject();
console.info('album getAssets successfully, albumName: ' + photoAsset.displayName);
albumFetchResult.close();
photoFetchResult.close();
} catch (err) {
console.error('album getAssets failed with err: ' + err);
}
}
```
从用户相册中移除图片和视频
先获取用户相册对象，然后调用Album.getAssets接口获取用户相册中的资源。
选择其中要移除的资源，然后调用MediaAlbumChangeRequest.removeAssets和PhotoAccessHelper.applyChanges接口移除。
前提条件
下面以将往相册名为'albumName'的用户相册中移除一张图片为例。
开发步骤
```typescript
import { dataSharePredicates } from '@kit.ArkData';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
const context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
async function example() {
let albumPredicates: dataSharePredicates.DataSharePredicates = new dataSharePredicates.DataSharePredicates();
let albumName: photoAccessHelper.AlbumKeys = photoAccessHelper.AlbumKeys.ALBUM_NAME;
albumPredicates.equalTo(albumName, 'albumName');
let albumFetchOptions: photoAccessHelper.FetchOptions = {
fetchColumns: [],
predicates: albumPredicates
};
let photoPredicates: dataSharePredicates.DataSharePredicates = new dataSharePredicates.DataSharePredicates();
let photoFetchOptions: photoAccessHelper.FetchOptions = {
fetchColumns: [],
predicates: photoPredicates
};
try {
let albumFetchResult: photoAccessHelper.FetchResult<photoAccessHelper.Album> = await phAccessHelper.getAlbums(photoAccessHelper.AlbumType.USER, photoAccessHelper.AlbumSubtype.USER_GENERIC, albumFetchOptions);
let album: photoAccessHelper.Album = await albumFetchResult.getFirstObject();
console.info('getAlbums successfully, albumName: ' + album.albumName);
let photoFetchResult = await album.getAssets(photoFetchOptions);
let photoAsset = await photoFetchResult.getFirstObject();
console.info('album getAssets successfully, albumName: ' + photoAsset.displayName);
let albumChangeRequest: photoAccessHelper.MediaAlbumChangeRequest = new photoAccessHelper.MediaAlbumChangeRequest(album);
albumChangeRequest.removeAssets([photoAsset]);
await phAccessHelper.applyChanges(albumChangeRequest);
console.info('succeed to remove ' + photoAsset.displayName + ' from ' + album.albumName);
albumFetchResult.close();
photoFetchResult.close();
} catch (err) {
console.error('removeAssets failed with err: ' + err);
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/photoaccesshelper-systemalbum-guidelines-V14
爬取时间: 2025-04-28 20:46:31
来源: Huawei Developer
photoAccessHelper仅提供开发者对收藏夹、视频相册、截屏和录屏相册进行相关操作。
在进行功能开发前，请开发者查阅开发准备，了解如何获取相册管理模块实例和如何申请相册管理模块功能开发相关权限。
文档中使用到PhotoAccessHelper的地方默认为使用开发准备中获取的对象，如未添加此段代码报PhotoAccessHelper未定义的错误请自行添加。
为了保证应用的运行效率，大部分photoAccessHelper的接口调用都是异步的。以下异步调用的API示例均采用Promise函数，更多方式可以查阅API参考。
如无特别说明，文档中涉及的待获取的资源均视为已经预置且在数据库中存在相应数据。如出现按照示例代码执行出现获取资源为空的情况请确认文件是否已预置，数据库中是否存在该文件的数据。
收藏夹
收藏夹属于系统相册，对图片或视频设置收藏时会自动将其加入到收藏夹中，取消收藏则会从收藏夹中移除。
获取收藏夹对象
通过PhotoAccessHelper.getAlbums接口获取收藏夹对象。
前提条件
开发步骤
```typescript
import { photoAccessHelper } from '@kit.MediaLibraryKit';
const context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
async function example() {
try {
let fetchResult: photoAccessHelper.FetchResult<photoAccessHelper.Album> = await phAccessHelper.getAlbums(photoAccessHelper.AlbumType.SYSTEM, photoAccessHelper.AlbumSubtype.FAVORITE);
let album: photoAccessHelper.Album = await fetchResult.getFirstObject();
console.info('get favorite album successfully, albumUri: ' + album.albumUri);
fetchResult.close();
} catch (err) {
console.error('get favorite album failed with err: ' + err);
}
}
```
获取收藏夹中的图片和视频
先获取收藏夹对象。然后调用Album.getAssets接口获取收藏夹中的资源。
前提条件
下面以获取收藏夹中的一张图片为例。
开发步骤
```typescript
import { dataSharePredicates } from '@kit.ArkData';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
const context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
async function example() {
let predicates: dataSharePredicates.DataSharePredicates = new dataSharePredicates.DataSharePredicates();
let fetchOptions: photoAccessHelper.FetchOptions = {
fetchColumns: [],
predicates: predicates
};
try {
let albumFetchResult: photoAccessHelper.FetchResult<photoAccessHelper.Album> = await phAccessHelper.getAlbums(photoAccessHelper.AlbumType.SYSTEM, photoAccessHelper.AlbumSubtype.FAVORITE);
let album: photoAccessHelper.Album = await albumFetchResult.getFirstObject();
console.info('get favorite album successfully, albumUri: ' + album.albumUri);
let photoFetchResult: photoAccessHelper.FetchResult<photoAccessHelper.PhotoAsset> = await album.getAssets(fetchOptions);
let photoAsset: photoAccessHelper.PhotoAsset = await photoFetchResult.getFirstObject();
console.info('favorite album getAssets successfully, photoAsset displayName: ' + photoAsset.displayName);
photoFetchResult.close();
albumFetchResult.close();
} catch (err) {
console.error('favorite failed with err: ' + err);
}
}
```
视频相册
视频相册属于系统相册，用户文件中属于视频类型的媒体文件会自动加入到视频相册中。
获取视频相册对象
通过PhotoAccessHelper.getAlbums接口获取视频相册对象。
前提条件
开发步骤
```typescript
import { photoAccessHelper } from '@kit.MediaLibraryKit';
const context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
async function example() {
try {
let fetchResult: photoAccessHelper.FetchResult<photoAccessHelper.Album> = await phAccessHelper.getAlbums(photoAccessHelper.AlbumType.SYSTEM, photoAccessHelper.AlbumSubtype.VIDEO);
let album: photoAccessHelper.Album = await fetchResult.getFirstObject();
console.info('get video album successfully, albumUri: ' + album.albumUri);
fetchResult.close();
} catch (err) {
console.error('get video album failed with err: ' + err);
}
}
```
获取视频相册中的视频
先获取视频相册对象。然后调用Album.getAssets接口获取视频相册对象中的视频资源。
前提条件
下面以获取视频相册中的一个视频为例。
开发步骤
```typescript
import { dataSharePredicates } from '@kit.ArkData';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
const context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
async function example() {
let predicates: dataSharePredicates.DataSharePredicates = new dataSharePredicates.DataSharePredicates();
let fetchOptions: photoAccessHelper.FetchOptions = {
fetchColumns: [],
predicates: predicates
};
try {
let albumFetchResult: photoAccessHelper.FetchResult<photoAccessHelper.Album> = await phAccessHelper.getAlbums(photoAccessHelper.AlbumType.SYSTEM, photoAccessHelper.AlbumSubtype.VIDEO);
let album: photoAccessHelper.Album = await albumFetchResult.getFirstObject();
console.info('get video album successfully, albumUri: ' + album.albumUri);
let videoFetchResult: photoAccessHelper.FetchResult<photoAccessHelper.PhotoAsset> = await album.getAssets(fetchOptions);
let photoAsset: photoAccessHelper.PhotoAsset = await videoFetchResult.getFirstObject();
console.info('video album getAssets successfully, photoAsset displayName: ' + photoAsset.displayName);
videoFetchResult.close();
albumFetchResult.close();
} catch (err) {
console.error('video failed with err: ' + err);
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/photoaccesshelper-notify-guidelines-V14
爬取时间: 2025-04-28 20:46:45
来源: Huawei Developer
photoAccessHelper提供监听媒体资源变更的接口，供开发者对指定媒体资源变更进行监听。
在进行功能开发前，请开发者查阅开发准备，了解如何获取相册管理模块实例和如何申请相册管理模块功能开发相关权限。
文档中使用到photoAccessHelper的地方默认为使用开发准备中获取的对象，如未添加此段代码报photoAccessHelper未定义的错误请自行添加。
媒体资源变更通知相关接口的异步调用仅支持使用callback方式。以下只列出部分接口使用方式，其他使用方式可以查阅API参考。
如无特别说明，文档中涉及的待获取的资源均视为已经预置且在数据库中存在相应数据。如出现按照示例代码执行出现获取资源为空的情况请确认文件是否已预置，数据库中是否存在该文件的数据。
监听指定URI
通过调用registerChange接口监听指定uri。当被监听对象发生变更时返回监听器回调函数的值。
对指定PhotoAsset注册监听
对指定PhotoAsset注册监听，当被监听的PhotoAsset发生变更时，返回监听回调。
前提条件
下面以对一张图片注册监听，通过将这张图片删除触发监听回调为例。
开发步骤
```typescript
import { dataSharePredicates } from '@kit.ArkData';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
const context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
async function example() {
let predicates: dataSharePredicates.DataSharePredicates = new dataSharePredicates.DataSharePredicates();
predicates.equalTo(photoAccessHelper.PhotoKeys.DISPLAY_NAME, 'test.jpg');
let fetchOptions: photoAccessHelper.FetchOptions = {
fetchColumns: [],
predicates: predicates
};
try {
let fetchResult: photoAccessHelper.FetchResult<photoAccessHelper.PhotoAsset> = await phAccessHelper.getAssets(fetchOptions);
let photoAsset: photoAccessHelper.PhotoAsset = await fetchResult.getFirstObject();
console.info('getAssets photoAsset.uri : ' + photoAsset.uri);
let onCallback = (changeData: photoAccessHelper.ChangeData) => {
console.info('onCallback successfully, changData: ' + JSON.stringify(changeData));
}
phAccessHelper.registerChange(photoAsset.uri, false, onCallback);
await photoAccessHelper.MediaAssetChangeRequest.deleteAssets(context, [photoAsset]);
fetchResult.close();
} catch (err) {
console.error('onCallback failed with err: ' + err);
}
}
```
对指定Album注册监听
对指定Album注册监听，当被监听的Album发生变更时，返回监听回调。
前提条件
下面以对一个用户相册注册监听，通过重命名相册触发监听回调为例。
开发步骤
```typescript
import { dataSharePredicates } from '@kit.ArkData';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
const context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
async function example() {
let predicates: dataSharePredicates.DataSharePredicates = new dataSharePredicates.DataSharePredicates();
let albumName: photoAccessHelper.AlbumKeys = photoAccessHelper.AlbumKeys.ALBUM_NAME;
predicates.equalTo(albumName, 'albumName');
let fetchOptions: photoAccessHelper.FetchOptions = {
fetchColumns: [],
predicates: predicates
};
try {
let fetchResult: photoAccessHelper.FetchResult<photoAccessHelper.Album> = await phAccessHelper.getAlbums(photoAccessHelper.AlbumType.USER, photoAccessHelper.AlbumSubtype.USER_GENERIC, fetchOptions);
let album: photoAccessHelper.Album = await fetchResult.getFirstObject();
console.info('getAlbums successfully, albumUri: ' + album.albumUri);
let onCallback = (changeData: photoAccessHelper.ChangeData) => {
console.info('onCallback successfully, changData: ' + JSON.stringify(changeData));
}
phAccessHelper.registerChange(album.albumUri, false, onCallback);
album.albumName = 'newAlbumName' + Date.now();
await album.commitModify();
fetchResult.close();
} catch (err) {
console.error('onCallback failed with err: ' + err);
}
}
```
模糊监听
通过设置forChildUris值为true来注册模糊监听，uri为相册uri时，forChildUris为true能监听到相册中文件的变化，如果是false只能监听相册本身变化。uri为photoAsset时，forChildUris为true、false没有区别，uri为DefaultChangeUri时，forChildUris必须为true，如果为false将找不到该uri，收不到任何消息。
对所有PhotoAsset注册监听
对所有PhotoAsset注册监听，当有被监听的PhotoAsset发生变更时，返回监听回调。
前提条件
下面以对所有PhotoAsset注册监听，通过将被监听的PhotoAsset删除触发监听回调为例。
开发步骤
```typescript
import { dataSharePredicates } from '@kit.ArkData';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
const context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
async function example() {
let onCallback = (changeData: photoAccessHelper.ChangeData) => {
console.info('onCallback successfully, changData: ' + JSON.stringify(changeData));
}
phAccessHelper.registerChange(photoAccessHelper.DefaultChangeUri.DEFAULT_PHOTO_URI, true, onCallback);
let predicates: dataSharePredicates.DataSharePredicates = new dataSharePredicates.DataSharePredicates();
let fetchOptions: photoAccessHelper.FetchOptions = {
fetchColumns: [],
predicates: predicates
};
try {
let fetchResult: photoAccessHelper.FetchResult<photoAccessHelper.PhotoAsset> = await phAccessHelper.getAssets(fetchOptions);
let photoAsset: photoAccessHelper.PhotoAsset = await fetchResult.getFirstObject();
console.info('getAssets photoAsset.uri : ' + photoAsset.uri);
await photoAccessHelper.MediaAssetChangeRequest.deleteAssets(context, [photoAsset]);
fetchResult.close();
} catch (err) {
console.error('onCallback failed with err: ' + err);
}
}
```
取消对指定URI的监听
取消对指定uri的监听，通过调用unRegisterChange接口取消对指定uri的监听。一个uri可以注册多个监听，存在多个callback监听时，可以取消指定注册的callback的监听；不指定callback时取消该uri的所有监听。
前提条件
下面以取消对一张图片指定的监听为例，取消监听后，删除图片不再触发对应的监听回调。
开发步骤
```typescript
import { dataSharePredicates } from '@kit.ArkData';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
const context = getContext(this);
let phAccessHelper = photoAccessHelper.getPhotoAccessHelper(context);
async function example() {
let predicates: dataSharePredicates.DataSharePredicates = new dataSharePredicates.DataSharePredicates();
predicates.equalTo(photoAccessHelper.PhotoKeys.DISPLAY_NAME, 'test.jpg');
let fetchOptions: photoAccessHelper.FetchOptions = {
fetchColumns: [],
predicates: predicates
};
try {
let fetchResult: photoAccessHelper.FetchResult<photoAccessHelper.PhotoAsset> = await phAccessHelper.getAssets(fetchOptions);
let photoAsset: photoAccessHelper.PhotoAsset = await fetchResult.getFirstObject();
console.info('getAssets photoAsset.uri : ' + photoAsset.uri);
let onCallback1 = (changeData: photoAccessHelper.ChangeData) => {
console.info('onCallback1, changData: ' + JSON.stringify(changeData));
}
let onCallback2 = (changeData: photoAccessHelper.ChangeData) => {
console.info('onCallback2, changData: ' + JSON.stringify(changeData));
}
phAccessHelper.registerChange(photoAsset.uri, false, onCallback1);
phAccessHelper.registerChange(photoAsset.uri, false, onCallback2);
phAccessHelper.unRegisterChange(photoAsset.uri, onCallback1);
await photoAccessHelper.MediaAssetChangeRequest.deleteAssets(context, [photoAsset]);
fetchResult.close();
} catch (err) {
console.error('onCallback failed with err: ' + err);
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/using-ndk-mediaassetmanager-for-request-resource-V14
爬取时间: 2025-04-28 20:46:58
来源: Huawei Developer
使用MediaAssetManager可以实现请求媒体资源到目标沙箱路径，本开发指导将以请求一张图片作为示例，向开发者讲解MediaAssetManager相关功能。
请求图片资源的全流程包含：创建MediaAssetManager，设置请求资源，请求图片资源，取消本次请求(可选)。
开发步骤及注意事项
在CMake脚本中链接动态库
开发者通过引入media_asset_manager.h和media_asset_base_capi.h头文件，使用MediaAssetManager相关API。
详细的API说明请参考MediaAssetManager API。
开发前，需要参考开发准备，申请ohos.permission.READ_IMAGEVIDEO权限。
完整示例

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/ringtone-kit-guide-V14
爬取时间: 2025-04-28 20:47:12
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/ringtone-introduction-V14
爬取时间: 2025-04-28 20:47:26
来源: Huawei Developer
Ringtone Kit（铃声服务）是一个用于设置铃声的工具库。通过使用Ringtone Kit，开发者可以在HarmonyOS应用中提供铃声设置的功能，为用户提供简单一致、安全高品质的铃声设置体验。
场景介绍
Ringtone Kit支持将音频文件设置成多种铃声类型，满足各类铃声需求场景。
铃声设置组件效果图：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165959.11781699143551792400272802988123:50001231000000:2800:6BA25C05A8BFF0A83121D42FC4E4FF1A8BB04B73429EAC7CEE1DB5BF236C2257.jpg)
约束与限制
当不满足所需条件时，部分功能不可使用：

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/ringtone-preparations-V14
爬取时间: 2025-04-28 20:47:39
来源: Huawei Developer
1.
```typescript
import { common } from '@kit.AbilityKit';
import { ringtone } from '@kit.RingtoneKit'
import { uniformTypeDescriptor } from '@kit.ArkData';
import { JSON } from '@kit.ArkTS';
import { hilog } from '@kit.PerformanceAnalysisKit';
const APP_TAG = "Msc_Demo"
const DOMAIN = 0x0001
```
2.
3.
4.  通过callback异步方式：
```typescript
// 详细代码参考API参考
let audioPath: string = (getContext(this) as common.UIAbilityContext).filesDir + '/' + this.buttonText;
let fileName: string = audioPath.substring(audioPath.lastIndexOf('/') + 1, audioPath.lastIndexOf('.'));
await ringtone.startRingtoneSetting(this.context, audioPath, fileName).then(res => {
hilog.info(DOMAIN, APP_TAG,'setFlag :' + res);
});
```
5.

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-kit-guide-V14
爬取时间: 2025-04-28 20:47:53
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-introduction-V14
爬取时间: 2025-04-28 20:48:06
来源: Huawei Developer
Scan Kit（统一扫码服务）作为软硬协同的系统级扫码服务，创新性地推出了更简单的“扫码直达”接入能力。只需少量的接入工作，无需在应用中开发专门的扫码模块，即可通过系统级扫码入口实现扫码到应用的跳转。同时还为开发者提供了面向各种场景的码图识别和生成能力。
Scan Kit应用了多项计算机视觉技术和AI算法技术，不仅实现了远距离自动扫码，同时还针对多种复杂扫码场景（如暗光、污损、模糊、小角度、曲面码等）做了识别优化，提升扫码成功率与用户体验。
场景介绍
Scan Kit同时提供了系统“扫码直达”和开发者应用内扫码两种能力。优先接入“扫码直达”能力，通过少量的接入工作即可实现开发者应用服务的一步直达。
功能使用限制
| 能力  | 限制条件  |
| --- | --- |
| 扫码直达能力  | 当前只支持开发者配置HTTPS架构的网页链接接入扫码直达。其他方式接入如HTTP配置可以通过工单联系我们。  |
| 默认界面扫码能力  | 暂不支持悬浮屏、分屏场景；相册扫码只支持单码识别；不支持界面UX添加自定义设置。  |
| 自定义界面扫码能力  | 需要授权使用相机权限；需要开发者自行实现扫码的人机交互界面。  |
| 码图生成能力  | 在设置生成参数时，对参数有限制要求，详细信息请参见文本生成码图的约束与限制和字节数组生成码图的约束与限制。  |
能力
限制条件
扫码直达能力
当前只支持开发者配置HTTPS架构的网页链接接入扫码直达。其他方式接入如HTTP配置可以通过工单联系我们。
默认界面扫码能力
暂不支持悬浮屏、分屏场景；相册扫码只支持单码识别；不支持界面UX添加自定义设置。
自定义界面扫码能力
需要授权使用相机权限；需要开发者自行实现扫码的人机交互界面。
码图生成能力
在设置生成参数时，对参数有限制要求，详细信息请参见文本生成码图的约束与限制和字节数组生成码图的约束与限制。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-config-agc-V14
爬取时间: 2025-04-28 20:48:20
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-directservice-V14
爬取时间: 2025-04-28 20:48:34
来源: Huawei Developer
在日常生活中，人们会使用各种应用扫各式各样的码，而“扫码直达”服务则为用户带来一种全新的扫码体验。
开发者将域名注册到“扫码直达”服务后，用户可通过控制中心等系统级的常驻入口，扫应用的二维码、条形码并跳转到应用对应服务页，实现一步直达服务的体验。
开发者接入“扫码直达”服务，能为应用带来：
业务流程
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165959.46522271839614981553069217384424:50001231000000:2800:E3249B2A37C3B760C510E5616AC4CDE87C17479DDE22711B37324A2D77F144D1.png)
开发步骤
```typescript
import { router } from '@kit.ArkUI';
import { hilog } from '@kit.PerformanceAnalysisKit';
import { AbilityConstant, UIAbility, Want } from '@kit.AbilityKit';
export default class EntryAbility extends UIAbility {
// 冷启动场景通过onCreate回调获取码值信息
onCreate(want: Want, launchParam: AbilityConstant.LaunchParam): void {
hilog.info(0x0001, '[Scan Access]', `Succeeded in getting want in onCreate`);
// 从want中获取传入的链接信息。
// 如传入的url为：https://www.example.com/programs?router=Access
this.getRouterUri(want);
}
// 热启动场景通过onNewWant回调获取码值信息
onNewWant(want: Want, launchParam: AbilityConstant.LaunchParam): void {
hilog.info(0x0001, '[Scan Access]', `Succeeded in getting want in onNewWant`);
// 从want中获取传入的链接信息
this.getRouterUri(want);
}
// 解析扫码结果，跳转相应页面
private getRouterUri(want: Want) {
let uri: string | undefined = want?.uri;
if (uri) {
// 开发者根据解析的uri跳转至响应页面，例如需要跳转页面为"pages/Access"
let status: router.RouterState = router.getState();
if (status && status.name !== 'Access' && uri) {
// 根据uri参数做业务处理
router.pushUrl({
url: 'pages/Access'
});
}
}
}
}
```
1.
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314165959.22850910904862070794589267553152:50001231000000:2800:61E46BCC1739EA242DC9AF88F16EA5CE08AD70D12869D3EE1C9FD822AE0D2160.gif)

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-scanbarcode-V14
爬取时间: 2025-04-28 20:48:48
来源: Huawei Developer
基本概念
默认界面扫码能力提供系统级体验一致的扫码界面，包含相机预览流，相册扫码入口，暗光环境闪光灯开启提示。Scan Kit对系统相机权限进行了预授权，调用接口时，无需开发者再次申请相机权限。适用于不同扫码场景的应用开发。
通过默认界面扫码可以实现应用内的扫码功能，为了应用更好的体验，推荐同时接入“扫码直达”服务，应用可以同时支持系统扫码入口（控制中心扫一扫）和应用内扫码两种方式跳转到指定服务页面。
场景介绍
默认界面扫码能力提供了系统级体验一致的扫码界面以及相册扫码入口，支持单码和多码识别，支持多种识码类型，请参见ScanType。无需使用三方库就可帮助开发者的应用快速处理各种扫码场景。
默认扫码界面UX：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314170000.65164217560956803648877954799752:50001231000000:2800:7E8C51433D57D29B25868E95FB44297C8CFBEFFE3135592A98062085549D6BA8.jpg)
约束与限制
业务流程
使用默认界面扫码的主要业务流程如下：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314170000.53116061292099499706728741923936:50001231000000:2800:35E8C3B3ADC4241A6942BF193B8E14043131F42EF5F4B1F434EC9CFD54D6F422.png)
接口说明
接口返回值有两种返回形式：Callback和Promise回调。下表中为默认界面扫码Callback和Promise形式接口，Callback和Promise只是返回值方式不一样，功能相同。startScanForResult接口打开的是应用内呈现的扫码界面样式。具体API说明详见接口文档。
| 接口名 | 描述 |
| --- | --- |
| startScanForResult(context: common.Context, options?: ScanOptions): Promise<ScanResult> | 启动默认界面扫码，通过ScanOptions进行扫码参数设置，使用Promise异步回调返回扫码结果。 |
| startScanForResult(context: common.Context, options: ScanOptions, callback: AsyncCallback<ScanResult>): void | 启动默认界面扫码，通过ScanOptions进行扫码参数设置，使用Callback异步回调返回扫码结果。 |
| startScanForResult(context: common.Context, callback: AsyncCallback<ScanResult>): void | 启动默认界面扫码，使用Callback异步回调返回扫码结果。 |
接口名
描述
startScanForResult(context:common.Context, options?:ScanOptions): Promise<ScanResult>
启动默认界面扫码，通过ScanOptions进行扫码参数设置，使用Promise异步回调返回扫码结果。
startScanForResult(context: common.Context, options: ScanOptions, callback: AsyncCallback<ScanResult>): void
启动默认界面扫码，通过ScanOptions进行扫码参数设置，使用Callback异步回调返回扫码结果。
startScanForResult(context: common.Context, callback: AsyncCallback<ScanResult>): void
启动默认界面扫码，使用Callback异步回调返回扫码结果。
startScanForResult接口需要在页面和组件的生命周期内调用。若需要设置扫码页面为全屏或沉浸式，请参见开发应用沉浸式效果。
开发步骤
Scan Kit提供了默认界面扫码的能力，由扫码接口直接控制相机实现最优的相机放大控制、自适应的曝光调节、自适应对焦调节等操作，保障流畅的扫码体验，减少开发者的工作量。
以下示例为调用Scan Kit的startScanForResult接口跳转扫码页面。
```typescript
import { scanCore, scanBarcode } from '@kit.ScanKit';
// 导入默认界面需要的日志模块和错误码模块
import { hilog } from '@kit.PerformanceAnalysisKit';
import { BusinessError } from '@kit.BasicServicesKit';
```
```typescript
@Entry
@Component
struct ScanBarCodePage {
build() {
Column() {
Row() {
Button("Promise with options")
.backgroundColor('#0D9FFB')
.fontSize(20)
.fontColor('#FFFFFF')
.fontWeight(FontWeight.Normal)
.align(Alignment.Center)
.type(ButtonType.Capsule)
.width('90%')
.height(40)
.margin({ top: 5, bottom: 5 })
.onClick(() => {
// 定义扫码参数options
let options: scanBarcode.ScanOptions = {
scanTypes: [scanCore.ScanType.ALL],
enableMultiMode: true,
enableAlbum: true
};
try {
// 可调用getContext接口获取当前页面关联的UIAbilityContext
scanBarcode.startScanForResult(getContext(this), options).then((result: scanBarcode.ScanResult) => {
// 解析码值结果跳转应用服务页
hilog.info(0x0001, '[Scan CPSample]', `Succeeded in getting ScanResult by promise with options, result is ${JSON.stringify(result)}`);
}).catch((error: BusinessError) => {
hilog.error(0x0001, '[Scan CPSample]',
`Failed to get ScanResult by promise with options. Code:${error.code}, message: ${error.message}`);
});
} catch (error) {
hilog.error(0x0001, '[Scan CPSample]',
`Failed to start the scanning service. Code:${error.code}, message: ${error.message}`);
}
})
}
.height('100%')
}
.width('100%')
}
}
```
```typescript
@Entry
@Component
struct ScanBarCodePage {
build() {
Column() {
Row() {
Button("Promise with options")
.backgroundColor('#0D9FFB')
.fontSize(20)
.fontColor('#FFFFFF')
.fontWeight(FontWeight.Normal)
.align(Alignment.Center)
.type(ButtonType.Capsule)
.width('90%')
.height(40)
.margin({ top: 5, bottom: 5 })
.onClick(() => {
// 定义扫码参数options
let options: scanBarcode.ScanOptions = {
scanTypes: [scanCore.ScanType.ALL],
enableMultiMode: true,
enableAlbum: true
};
try {
// 可调用getContext接口获取当前页面关联的UIAbilityContext
scanBarcode.startScanForResult(getContext(this), options).then((result: scanBarcode.ScanResult) => {
// 解析码值结果跳转应用服务页
hilog.info(0x0001, '[Scan CPSample]', `Succeeded in getting ScanResult by promise with options, result is ${JSON.stringify(result)}`);
}).catch((error: BusinessError) => {
hilog.error(0x0001, '[Scan CPSample]',
`Failed to get ScanResult by promise with options. Code:${error.code}, message: ${error.message}`);
});
} catch (error) {
hilog.error(0x0001, '[Scan CPSample]',
`Failed to start the scanning service. Code:${error.code}, message: ${error.message}`);
}
})
}
.height('100%')
}
.width('100%')
}
}
```
```typescript
@Entry
@Component
struct ScanBarCodePage {
build() {
Column() {
Row() {
Button('Callback with options')
.backgroundColor('#0D9FFB')
.fontSize(20)
.fontColor('#FFFFFF')
.fontWeight(FontWeight.Normal)
.align(Alignment.Center)
.type(ButtonType.Capsule)
.width('90%')
.height(40)
.margin({ top: 5, bottom: 5 })
.onClick(() => {
// 定义扫码参数options
let options: scanBarcode.ScanOptions = {
scanTypes: [scanCore.ScanType.ALL],
enableMultiMode: true,
enableAlbum: true
};
try {
// 可调用getContext接口获取当前页面关联的UIAbilityContext
scanBarcode.startScanForResult(getContext(this), options,
(error: BusinessError, result: scanBarcode.ScanResult) => {
if (error) {
hilog.error(0x0001, '[Scan CPSample]',
`Failed to get ScanResult by callback with options. Code: ${error.code}, message: ${error.message}`);
return;
}
// 解析码值结果跳转应用服务页
hilog.info(0x0001, '[Scan CPSample]', `Succeeded in getting ScanResult by callback with options, result is ${JSON.stringify(result)}`);
})
} catch (error) {
hilog.error(0x0001, '[Scan CPSample]',
`Failed to start the scanning service. Code:${error.code}, message: ${error.message}`);
}
})
}
.height('100%')
}
.width('100%')
}
}
```
```typescript
@Entry
@Component
struct ScanBarCodePage {
build() {
Column() {
Row() {
Button("Promise with options")
.backgroundColor('#0D9FFB')
.fontSize(20)
.fontColor('#FFFFFF')
.fontWeight(FontWeight.Normal)
.align(Alignment.Center)
.type(ButtonType.Capsule)
.width('90%')
.height(40)
.margin({ top: 5, bottom: 5 })
.onClick(() => {
// 定义扫码参数options
let options: scanBarcode.ScanOptions = {
scanTypes: [scanCore.ScanType.ALL],
enableMultiMode: true,
enableAlbum: true
};
try {
// 可调用getContext接口获取当前页面关联的UIAbilityContext
scanBarcode.startScanForResult(getContext(this), options).then((result: scanBarcode.ScanResult) => {
// 解析码值结果跳转应用服务页
hilog.info(0x0001, '[Scan CPSample]', `Succeeded in getting ScanResult by promise with options, result is ${JSON.stringify(result)}`);
}).catch((error: BusinessError) => {
hilog.error(0x0001, '[Scan CPSample]',
`Failed to get ScanResult by promise with options. Code:${error.code}, message: ${error.message}`);
});
} catch (error) {
hilog.error(0x0001, '[Scan CPSample]',
`Failed to start the scanning service. Code:${error.code}, message: ${error.message}`);
}
})
}
.height('100%')
}
.width('100%')
}
}
```
```typescript
@Entry
@Component
struct ScanBarCodePage {
build() {
Column() {
Row() {
Button('Callback with options')
.backgroundColor('#0D9FFB')
.fontSize(20)
.fontColor('#FFFFFF')
.fontWeight(FontWeight.Normal)
.align(Alignment.Center)
.type(ButtonType.Capsule)
.width('90%')
.height(40)
.margin({ top: 5, bottom: 5 })
.onClick(() => {
// 定义扫码参数options
let options: scanBarcode.ScanOptions = {
scanTypes: [scanCore.ScanType.ALL],
enableMultiMode: true,
enableAlbum: true
};
try {
// 可调用getContext接口获取当前页面关联的UIAbilityContext
scanBarcode.startScanForResult(getContext(this), options,
(error: BusinessError, result: scanBarcode.ScanResult) => {
if (error) {
hilog.error(0x0001, '[Scan CPSample]',
`Failed to get ScanResult by callback with options. Code: ${error.code}, message: ${error.message}`);
return;
}
// 解析码值结果跳转应用服务页
hilog.info(0x0001, '[Scan CPSample]', `Succeeded in getting ScanResult by callback with options, result is ${JSON.stringify(result)}`);
})
} catch (error) {
hilog.error(0x0001, '[Scan CPSample]',
`Failed to start the scanning service. Code:${error.code}, message: ${error.message}`);
}
})
}
.height('100%')
}
.width('100%')
}
}
```
模拟器开发
暂不支持模拟器使用，调用会返回错误信息“Emulator is not supported.”

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-customscan-V14
爬取时间: 2025-04-28 20:49:02
来源: Huawei Developer
基本概念
自定义界面扫码能力提供了相机流控制接口，可根据自身需求自定义扫码界面，适用于对扫码界面有定制化需求的应用开发。
通过自定义页面扫码可以实现应用内的扫码功能，为了应用更好的体验，推荐同时接入“扫码直达”服务，应用可以同时支持系统扫码入口（控制中心扫一扫）和应用内扫码两种方式跳转到指定服务页面。
场景介绍
自定义界面扫码能力提供扫码相机流控制接口，支持相机流的初始化、开启、暂停、释放、重新扫码功能；支持闪光灯的状态获取、开启、关闭；支持变焦比的获取和设置；支持设置相机焦点和连续自动对焦；支持对条形码、二维码、MULTIFUNCTIONAL CODE进行扫码识别（具体类型参见ScanType），并获得码类型、码值、码位置信息、相机预览流（YUV）。该能力可用于单码和多码的扫描识别。
开发者集成自定义界面扫码能力可以自行定义扫码的界面样式，请按照业务流程完成扫码接口调用实现实时扫码功能。建议开发者基于Sample Code做个性化修改。
扫码页面UX设计规范：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314170000.27321335037973995196520790474858:50001231000000:2800:28159FD2EB72DBCC51C94DDCFBA6550E799EFCEECBC9EE2DBEDF20FA07A4B3D6.jpg)
YUV（相机预览流图像数据）适合于扫码和识物的综合识别场景，开发者需要自己控制相机流，普通扫码场景无需关注。
约束与限制
业务流程
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314170000.90355062457113325551948190777238:50001231000000:2800:D7AC6E59A854E647AF8950D6DEA79C7AE9CDB558C1568BBBC69D85CC653FEEF3.png)
接口说明
自定义界面扫码提供init、start、stop、release、getFlashLightStatus、openFlashLight、closeFlashLight、setZoom、getZoom、setFocusPoint、resetFocus、rescan、on('lightingFlash')、off('lightingFlash')接口，其中部分接口返回值有两种返回形式：Callback和Promise回调。Callback和Promise回调函数只是返回值方式不一样，功能相同。具体API说明详见接口文档。
| 接口名 | 描述 |
| --- | --- |
| init(options?: scanBarcode.ScanOptions): void | 初始化自定义界面扫码，加载资源。无返回结果。 |
| start(viewControl: ViewControl): Promise<Array<scanBarcode.ScanResult>> | 启动扫码相机流。使用Promise异步回调获取扫码结果。 |
| stop(): Promise<void> | 暂停扫码相机流。使用Promise异步回调返回执行结果。 |
| release(): Promise<void> | 释放扫码相机流。使用Promise异步回调返回执行结果。 |
| start(viewControl: ViewControl, callback: AsyncCallback<Array<scanBarcode.ScanResult>>, frameCallback?: AsyncCallback<ScanFrame>): void | 启动扫码相机流。使用Callback异步回调返回扫码结果以及YUV图像数据。 |
| getFlashLightStatus(): boolean | 获取闪光灯状态。返回结果为布尔值，true为打开状态，false为关闭状态。 |
| openFlashLight(): void | 开启闪光灯。无返回结果。 |
| closeFlashLight(): void | 关闭闪光灯。无返回结果。 |
| setZoom(zoomValue : number): void | 设置变焦比。无返回结果。 |
| getZoom(): number | 获取当前的变焦比。 |
| setFocusPoint(point: scanBarcode.Point): void | 设置相机焦点。 |
| resetFocus(): void | 设置连续自动对焦模式。 |
| rescan(): void | 触发一次重新扫码。仅对start接口Callback异步回调有效，Promise异步回调无效。 |
| stop(callback: AsyncCallback<void>): void | 暂停扫码相机流。使用Callback异步回调返回执行结果。 |
| release(callback: AsyncCallback<void>): void | 释放扫码相机流。使用Callback异步回调返回执行结果。 |
| on(type: 'lightingFlash', callback: AsyncCallback<boolean>): void | 注册闪光灯打开时机回调，使用Callback异步回调返回闪光灯打开时机。 |
| off(type: 'lightingFlash', callback?: AsyncCallback<boolean>): void | 注销闪光灯打开时机回调，使用Callback异步回调返回注销结果。 |
接口名
描述
init(options?: scanBarcode.ScanOptions): void
初始化自定义界面扫码，加载资源。无返回结果。
start(viewControl:ViewControl): Promise<Array<scanBarcode.ScanResult>>
启动扫码相机流。使用Promise异步回调获取扫码结果。
stop(): Promise<void>
暂停扫码相机流。使用Promise异步回调返回执行结果。
release(): Promise<void>
释放扫码相机流。使用Promise异步回调返回执行结果。
start(viewControl: ViewControl, callback: AsyncCallback<Array<scanBarcode.ScanResult>>, frameCallback?: AsyncCallback<ScanFrame>): void
启动扫码相机流。使用Callback异步回调返回扫码结果以及YUV图像数据。
getFlashLightStatus(): boolean
获取闪光灯状态。返回结果为布尔值，true为打开状态，false为关闭状态。
openFlashLight(): void
开启闪光灯。无返回结果。
closeFlashLight(): void
关闭闪光灯。无返回结果。
setZoom(zoomValue : number): void
设置变焦比。无返回结果。
getZoom(): number
获取当前的变焦比。
setFocusPoint(point: scanBarcode.Point): void
设置相机焦点。
resetFocus(): void
设置连续自动对焦模式。
rescan(): void
触发一次重新扫码。仅对start接口Callback异步回调有效，Promise异步回调无效。
stop(callback: AsyncCallback<void>): void
暂停扫码相机流。使用Callback异步回调返回执行结果。
release(callback: AsyncCallback<void>): void
释放扫码相机流。使用Callback异步回调返回执行结果。
on(type: 'lightingFlash', callback: AsyncCallback<boolean>): void
注册闪光灯打开时机回调，使用Callback异步回调返回闪光灯打开时机。
off(type: 'lightingFlash', callback?: AsyncCallback<boolean>): void
注销闪光灯打开时机回调，使用Callback异步回调返回注销结果。
开发步骤
自定义界面扫码接口支持自定义UI界面，识别相机流中的条形码，二维码以及MULTIFUNCTIONAL CODE，并返回码图的值、类型、码的位置信息（码图最小外接矩形左上角和右下角的坐标）以及相机预览流（YUV）。
以下示例为调用自定义界面扫码接口拉起相机流并返回扫码结果和相机预览流（YUV）。
1.  权限名 说明 授权方式 ohos.permission.CAMERA 允许应用使用相机扫码。 user_grant
```typescript
import { scanCore, scanBarcode, customScan } from '@kit.ScanKit';
// 导入功能涉及的权限申请、回调接口
import { router, promptAction, display } from '@kit.ArkUI';
import { AsyncCallback, BusinessError } from '@kit.BasicServicesKit';
import { hilog } from '@kit.PerformanceAnalysisKit';
import { common, abilityAccessCtrl } from '@kit.AbilityKit';
```
```typescript
const TAG: string = '[customScanPage]';
@Entry
@Component
struct CustomScanPage {
@State userGrant: boolean = false // 是否已申请相机权限
@State surfaceId: string = '' // xComponent组件生成id
@State isShowBack: boolean = false // 是否已经返回扫码结果
@State isFlashLightEnable: boolean = false // 是否开启了闪光灯
@State isSensorLight: boolean = false // 记录当前环境亮暗状态
@State cameraHeight: number = 640 // 设置预览流高度，默认单位：vp
@State cameraWidth: number = 360 // 设置预览流宽度，默认单位：vp
@State offsetX: number = 0 // 设置预览流x轴方向偏移量，默认单位：vp
@State offsetY: number = 0 // 设置预览流y轴方向偏移量，默认单位：vp
@State zoomValue: number = 1 // 预览流缩放比例
@State setZoomValue: number = 1 // 已设置的预览流缩放比例
@State scaleValue: number = 1 // 屏幕缩放比
@State pinchValue: number = 1 // 双指缩放比例
@State displayHeight: number = 0 // 屏幕高度，单位vp
@State displayWidth: number = 0 // 屏幕宽度，单位vp
@State scanResult: Array<scanBarcode.ScanResult> = [] // 扫码结果
private mXComponentController: XComponentController = new XComponentController()
async onPageShow() {
// 自定义启动第一步，用户申请权限
await this.requestCameraPermission();
// 多码扫码识别，enableMultiMode: true 单码扫码识别enableMultiMode: false
let options: scanBarcode.ScanOptions = {
scanTypes: [scanCore.ScanType.ALL],
enableMultiMode: true,
enableAlbum: true
}
// 自定义启动第二步：设置预览流布局尺寸
this.setDisplay();
try {
// 自定义启动第三步，初始化接口
customScan.init(options);
} catch (error) {
hilog.error(0x0001, TAG, `Failed to init customScan. Code: ${error.code}, message: ${error.message}`);
}
}
async onPageHide() {
// 页面消失或隐藏时，停止并释放相机流
this.userGrant = false;
this.isFlashLightEnable = false;
this.isSensorLight = false;
try {
customScan.off('lightingFlash');
} catch (error) {
hilog.error(0x0001, TAG, `Failed to off lightingFlash. Code: ${error.code}, message: ${error.message}`);
}
this.customScanStop();
try {
// 自定义相机流释放接口
customScan.release().then(() => {
hilog.info(0x0001, TAG, 'Succeeded in releasing customScan by promise.');
}).catch((error: BusinessError) => {
hilog.error(0x0001, TAG,
`Failed to release customScan by promise. Code: ${error.code}, message: ${error.message}`);
})
} catch (error) {
hilog.error(0x0001, TAG, `Failed to release customScan. Code: ${error.code}, message: ${error.message}`);
}
}
// 用户申请权限
async reqPermissionsFromUser(): Promise<number[]> {
hilog.info(0x0001, TAG, 'reqPermissionsFromUser start');
let context = getContext() as common.UIAbilityContext;
let atManager = abilityAccessCtrl.createAtManager();
let grantStatus = await atManager.requestPermissionsFromUser(context, ['ohos.permission.CAMERA']);
return grantStatus.authResults;
}
// 用户申请相机权限
async requestCameraPermission() {
let grantStatus = await this.reqPermissionsFromUser();
for (let i = 0; i < grantStatus.length; i++) {
if (grantStatus[i] === 0) {
// 用户授权，可以继续访问目标操作
hilog.info(0x0001, TAG, 'Succeeded in getting permissions.');
this.userGrant = true;
}
}
}
// 竖屏时获取屏幕尺寸，设置预览流全屏示例
setDisplay() {
try {
// 默认竖屏
let displayClass = display.getDefaultDisplaySync();
this.displayHeight = px2vp(displayClass.height);
this.displayWidth = px2vp(displayClass.width);
let maxLen: number = Math.max(this.displayWidth, this.displayHeight);
let minLen: number = Math.min(this.displayWidth, this.displayHeight);
const RATIO: number = 16 / 9;
this.cameraHeight = maxLen;
this.cameraWidth = maxLen / RATIO;
this.offsetX = (minLen - this.cameraWidth) / 2;
} catch (error) {
hilog.error(0x0001, TAG, `Failed to getDefaultDisplaySync. Code: ${error.code}, message: ${error.message}`);
}
}
// toast显示扫码结果
async showScanResult(result: scanBarcode.ScanResult) {
// 使用toast显示出扫码结果
promptAction.showToast({
message: JSON.stringify(result),
duration: 5000
});
}
initCamera() {
this.isShowBack = false;
this.scanResult = [];
let viewControl: customScan.ViewControl = {
width: this.cameraWidth,
height: this.cameraHeight,
surfaceId: this.surfaceId
};
try {
// 自定义启动第四步，请求扫码接口，通过Promise方式回调
customScan.start(viewControl)
.then(async (result: Array<scanBarcode.ScanResult>) => {
hilog.info(0x0001, TAG, `result: ${JSON.stringify(result)}`);
if (result.length) {
// 解析码值结果跳转应用服务页
this.scanResult = result;
this.isShowBack = true;
// 获取到扫描结果后暂停相机流
this.customScanStop();
}
});
} catch (error) {
hilog.error(0x0001, TAG, `Failed to start customScan. Code: ${error.code}, message: ${error.message}`);
}
}
customScanStop() {
try {
customScan.stop();
} catch (error) {
hilog.error(0x0001, TAG, `Failed to stop customScan. Code: ${error.code}, message: ${error.message}`);
}
}
// 自定义扫码界面的顶部返回按钮和扫码提示
@Builder
TopTool() {
Column() {
Flex({ direction: FlexDirection.Row, justifyContent: FlexAlign.SpaceBetween, alignItems: ItemAlign.Center }) {
Text('返回')
.onClick(async () => {
router.back();
})
}.padding({ left: 24, right: 24, top: 40 })
Column() {
Text('扫描二维码/条形码')
Text('对准二维码/条形码，即可自动扫描')
}.margin({ left: 24, right: 24, top: 24 })
}
.height(146)
.width('100%')
}
build() {
Stack() {
if (this.userGrant) {
Column() {
XComponent({
id: 'componentId',
type: XComponentType.SURFACE,
controller: this.mXComponentController
})
.onLoad(async () => {
hilog.info(0x0001, TAG, 'Succeeded in loading, onLoad is called.');
// 获取XComponent组件的surfaceId
this.surfaceId = this.mXComponentController.getXComponentSurfaceId();
hilog.info(0x0001, TAG, `Succeeded in getting surfaceId: ${this.surfaceId}`);
this.initCamera();
// 闪光灯监听接口
customScan.on('lightingFlash', (error, isLightingFlash) => {
if (error) {
hilog.error(0x0001, TAG,
`Failed to on lightingFlash. Code: ${error.code}, message: ${error.message}`);
return;
}
if (isLightingFlash) {
this.isFlashLightEnable = true;
} else {
try {
if (!customScan.getFlashLightStatus()) {
this.isFlashLightEnable = false;
}
} catch (error) {
hilog.error(0x0001, TAG,
`Failed to get FlashLightStatus. Code: ${error.code}, message: ${error.message}`);
}
}
this.isSensorLight = isLightingFlash;
});
})
.width(this.cameraWidth)
.height(this.cameraHeight)
.position({ x: this.offsetX, y: this.offsetY })
}
.height('100%')
.width('100%')
}
Column() {
this.TopTool()
Column() {
}
.layoutWeight(1)
.width('100%')
Column() {
Row() {
// 闪光灯按钮，启动相机流后才能使用
Button('FlashLight')
.onClick(() => {
let lightStatus: boolean = false;
try {
lightStatus = customScan.getFlashLightStatus();
} catch (error) {
hilog.error(0x0001, TAG,
`Failed to get flashLightStatus. Code: ${error.code}, message: ${error.message}`);
}
// 根据当前闪光灯状态，选择打开或关闭闪关灯
if (lightStatus) {
try {
customScan.closeFlashLight();
setTimeout(() => {
this.isFlashLightEnable = this.isSensorLight;
}, 200);
} catch (error) {
hilog.error(0x0001, TAG,
`Failed to close flashLight. Code: ${error.code}, message: ${error.message}`);
}
} else {
try {
customScan.openFlashLight();
} catch (error) {
hilog.error(0x0001, TAG,
`Failed to open flashLight. Code: ${error.code}, message: ${error.message}`);
}
}
})
.visibility((this.userGrant && this.isFlashLightEnable) ? Visibility.Visible : Visibility.None)
// 扫码成功后，点击按钮后重新扫码
Button('Scan')
.onClick(() => {
// 点击按钮重启相机流，重新扫码
this.initCamera();
})
.visibility(this.isShowBack ? Visibility.Visible : Visibility.None)
}
Row() {
// 预览流设置缩放比例
Button('缩放比例,当前比例:' + this.setZoomValue)
.onClick(() => {
// 设置相机缩放比例
if (!this.isShowBack) {
if (!this.zoomValue || this.zoomValue === this.setZoomValue) {
this.setZoomValue = this.customGetZoom();
} else {
this.zoomValue = this.zoomValue;
this.customSetZoom(this.zoomValue);
setTimeout(() => {
if (!this.isShowBack) {
this.setZoomValue = this.customGetZoom();
}
}, 1000);
}
}
})
}
.margin({ top: 10, bottom: 10 })
Row() {
// 输入要设置的预览流缩放比例
TextInput({ placeholder: '输入缩放倍数' })
.type(InputType.Number)
.borderWidth(1)
.backgroundColor(Color.White)
.onChange(value => {
this.zoomValue = Number(value);
})
}
}
.width('50%')
.height(180)
}
// 单码、多码扫描后，显示码图蓝点位置。点击toast码图信息
ForEach(this.scanResult, (item: scanBarcode.ScanResult, index: number) => {
if (item.scanCodeRect) {
Image($rawfile('scan_selected2.svg'))
.width(40)
.height(40)
.markAnchor({ x: 20, y: 20 })
.position({
x: (item.scanCodeRect.left + item?.scanCodeRect?.right) / 2 + this.offsetX,
y: (item.scanCodeRect.top + item?.scanCodeRect?.bottom) / 2 + this.offsetY
})
.onClick(() => {
this.showScanResult(item);
})
}
})
}
// 建议相机流设置为全屏
.width('100%')
.height('100%')
.onClick((event: ClickEvent) => {
// 是否已扫描到结果
if (this.isShowBack) {
return;
}
// 点击屏幕位置，获取点击位置(x,y)，设置相机焦点
let x1 = vp2px(event.displayY) / (this.displayHeight + 0.0);
let y1 = 1.0 - (vp2px(event.displayX) / (this.displayWidth + 0.0));
try {
customScan.setFocusPoint({ x: x1, y: y1 });
hilog.info(0x0001, TAG, `Succeeded in setting focusPoint x1: ${x1}, y1: ${y1}`);
} catch (error) {
hilog.error(0x0001, TAG, `Failed to set focusPoint. Code: ${error.code}, message: ${error.message}`);
}
hilog.info(0x0001, TAG, `Succeeded in setting focusPoint x1: ${x1}, y1: ${y1}`);
// 设置连续自动对焦模式
setTimeout(() => {
try {
customScan.resetFocus();
} catch (error) {
hilog.error(0x0001, TAG, `Failed to reset Focus. Code: ${error.code}, message: ${error.message}`);
}
}, 200);
}).gesture(PinchGesture({ fingers: 2 })
.onActionStart((event: GestureEvent) => {
hilog.info(0x0001, TAG, 'Pinch start');
})
.onActionUpdate((event: GestureEvent) => {
if (event) {
this.scaleValue = event.scale;
}
})
.onActionEnd((event: GestureEvent) => {
// 是否已扫描到结果
if (this.isShowBack) {
return;
}
// 获取双指缩放比例，设置变焦比
try {
let zoom = this.customGetZoom();
this.pinchValue = this.scaleValue * zoom;
this.customSetZoom(this.pinchValue);
hilog.info(0x0001, TAG, 'Pinch end');
} catch (error) {
hilog.error(0x0001, TAG, `Failed to setZoom. Code: ${error.code}, message: ${error.message}`);
}
}))
}
public customGetZoom(): number {
let zoom = 1;
try {
zoom = customScan.getZoom();
hilog.info(0x0001, TAG, `Succeeded in getting Zoom, zoom: ${zoom}`);
} catch (error) {
hilog.error(0x0001, TAG, `Failed to getZoom. Code: ${error.code}, message: ${error?.message}`);
}
return zoom;
}
public customSetZoom(pinchValue: number): void {
try {
customScan.setZoom(pinchValue);
hilog.info(0x0001, TAG, `Succeeded in setting Zoom.`);
} catch (error) {
hilog.error(0x0001, TAG, `Failed to setZoom. Code: ${error.code}, message: ${error?.message}`);
}
}
}
```
```typescript
const TAG: string = '[customScanPage]';
@Entry
@Component
struct CustomScanPage {
@State userGrant: boolean = false // 是否已申请相机权限
@State surfaceId: string = '' // xComponent组件生成id
@State isShowBack: boolean = false // 是否已经返回扫码结果
@State isFlashLightEnable: boolean = false // 是否开启了闪光灯
@State isSensorLight: boolean = false // 记录当前环境亮暗状态
@State cameraHeight: number = 640 // 设置预览流高度，默认单位：vp
@State cameraWidth: number = 360 // 设置预览流宽度，默认单位：vp
@State offsetX: number = 0 // 设置预览流x轴方向偏移量，默认单位：vp
@State offsetY: number = 0 // 设置预览流y轴方向偏移量，默认单位：vp
@State zoomValue: number = 1 // 预览流缩放比例
@State setZoomValue: number = 1 // 已设置的预览流缩放比例
@State scaleValue: number = 1 // 屏幕缩放比
@State pinchValue: number = 1 // 双指缩放比例
@State displayHeight: number = 0 // 屏幕高度，单位vp
@State displayWidth: number = 0 // 屏幕宽度，单位vp
@State scanResult: Array<scanBarcode.ScanResult> = [] // 扫码结果
private mXComponentController: XComponentController = new XComponentController()
async onPageShow() {
// 自定义启动第一步，用户申请权限
await this.requestCameraPermission();
// 多码扫码识别，enableMultiMode: true 单码扫码识别enableMultiMode: false
let options: scanBarcode.ScanOptions = {
scanTypes: [scanCore.ScanType.ALL],
enableMultiMode: true,
enableAlbum: true
}
// 自定义启动第二步：设置预览流布局尺寸
this.setDisplay();
try {
// 自定义启动第三步，初始化接口
customScan.init(options);
} catch (error) {
hilog.error(0x0001, TAG, `Failed to init customScan. Code: ${error.code}, message: ${error.message}`);
}
}
async onPageHide() {
// 页面消失或隐藏时，停止并释放相机流
this.userGrant = false;
this.isFlashLightEnable = false;
this.isSensorLight = false;
try {
customScan.off('lightingFlash');
} catch (error) {
hilog.error(0x0001, TAG, `Failed to off lightingFlash. Code: ${error.code}, message: ${error.message}`);
}
this.customScanStop();
try {
// 自定义相机流释放接口
customScan.release().then(() => {
hilog.info(0x0001, TAG, 'Succeeded in releasing customScan by promise.');
}).catch((error: BusinessError) => {
hilog.error(0x0001, TAG,
`Failed to release customScan by promise. Code: ${error.code}, message: ${error.message}`);
})
} catch (error) {
hilog.error(0x0001, TAG, `Failed to release customScan. Code: ${error.code}, message: ${error.message}`);
}
}
// 用户申请权限
async reqPermissionsFromUser(): Promise<number[]> {
hilog.info(0x0001, TAG, 'reqPermissionsFromUser start');
let context = getContext() as common.UIAbilityContext;
let atManager = abilityAccessCtrl.createAtManager();
let grantStatus = await atManager.requestPermissionsFromUser(context, ['ohos.permission.CAMERA']);
return grantStatus.authResults;
}
// 用户申请相机权限
async requestCameraPermission() {
let grantStatus = await this.reqPermissionsFromUser();
for (let i = 0; i < grantStatus.length; i++) {
if (grantStatus[i] === 0) {
// 用户授权，可以继续访问目标操作
hilog.info(0x0001, TAG, 'Succeeded in getting permissions.');
this.userGrant = true;
}
}
}
// 竖屏时获取屏幕尺寸，设置预览流全屏示例
setDisplay() {
try {
// 默认竖屏
let displayClass = display.getDefaultDisplaySync();
this.displayHeight = px2vp(displayClass.height);
this.displayWidth = px2vp(displayClass.width);
let maxLen: number = Math.max(this.displayWidth, this.displayHeight);
let minLen: number = Math.min(this.displayWidth, this.displayHeight);
const RATIO: number = 16 / 9;
this.cameraHeight = maxLen;
this.cameraWidth = maxLen / RATIO;
this.offsetX = (minLen - this.cameraWidth) / 2;
} catch (error) {
hilog.error(0x0001, TAG, `Failed to getDefaultDisplaySync. Code: ${error.code}, message: ${error.message}`);
}
}
// toast显示扫码结果
async showScanResult(result: scanBarcode.ScanResult) {
// 使用toast显示出扫码结果
promptAction.showToast({
message: JSON.stringify(result),
duration: 5000
});
}
initCamera() {
this.isShowBack = false;
this.scanResult = [];
let viewControl: customScan.ViewControl = {
width: this.cameraWidth,
height: this.cameraHeight,
surfaceId: this.surfaceId
};
try {
// 自定义启动第四步，请求扫码接口，通过Promise方式回调
customScan.start(viewControl)
.then(async (result: Array<scanBarcode.ScanResult>) => {
hilog.info(0x0001, TAG, `result: ${JSON.stringify(result)}`);
if (result.length) {
// 解析码值结果跳转应用服务页
this.scanResult = result;
this.isShowBack = true;
// 获取到扫描结果后暂停相机流
this.customScanStop();
}
});
} catch (error) {
hilog.error(0x0001, TAG, `Failed to start customScan. Code: ${error.code}, message: ${error.message}`);
}
}
customScanStop() {
try {
customScan.stop();
} catch (error) {
hilog.error(0x0001, TAG, `Failed to stop customScan. Code: ${error.code}, message: ${error.message}`);
}
}
// 自定义扫码界面的顶部返回按钮和扫码提示
@Builder
TopTool() {
Column() {
Flex({ direction: FlexDirection.Row, justifyContent: FlexAlign.SpaceBetween, alignItems: ItemAlign.Center }) {
Text('返回')
.onClick(async () => {
router.back();
})
}.padding({ left: 24, right: 24, top: 40 })
Column() {
Text('扫描二维码/条形码')
Text('对准二维码/条形码，即可自动扫描')
}.margin({ left: 24, right: 24, top: 24 })
}
.height(146)
.width('100%')
}
build() {
Stack() {
if (this.userGrant) {
Column() {
XComponent({
id: 'componentId',
type: XComponentType.SURFACE,
controller: this.mXComponentController
})
.onLoad(async () => {
hilog.info(0x0001, TAG, 'Succeeded in loading, onLoad is called.');
// 获取XComponent组件的surfaceId
this.surfaceId = this.mXComponentController.getXComponentSurfaceId();
hilog.info(0x0001, TAG, `Succeeded in getting surfaceId: ${this.surfaceId}`);
this.initCamera();
// 闪光灯监听接口
customScan.on('lightingFlash', (error, isLightingFlash) => {
if (error) {
hilog.error(0x0001, TAG,
`Failed to on lightingFlash. Code: ${error.code}, message: ${error.message}`);
return;
}
if (isLightingFlash) {
this.isFlashLightEnable = true;
} else {
try {
if (!customScan.getFlashLightStatus()) {
this.isFlashLightEnable = false;
}
} catch (error) {
hilog.error(0x0001, TAG,
`Failed to get FlashLightStatus. Code: ${error.code}, message: ${error.message}`);
}
}
this.isSensorLight = isLightingFlash;
});
})
.width(this.cameraWidth)
.height(this.cameraHeight)
.position({ x: this.offsetX, y: this.offsetY })
}
.height('100%')
.width('100%')
}
Column() {
this.TopTool()
Column() {
}
.layoutWeight(1)
.width('100%')
Column() {
Row() {
// 闪光灯按钮，启动相机流后才能使用
Button('FlashLight')
.onClick(() => {
let lightStatus: boolean = false;
try {
lightStatus = customScan.getFlashLightStatus();
} catch (error) {
hilog.error(0x0001, TAG,
`Failed to get flashLightStatus. Code: ${error.code}, message: ${error.message}`);
}
// 根据当前闪光灯状态，选择打开或关闭闪关灯
if (lightStatus) {
try {
customScan.closeFlashLight();
setTimeout(() => {
this.isFlashLightEnable = this.isSensorLight;
}, 200);
} catch (error) {
hilog.error(0x0001, TAG,
`Failed to close flashLight. Code: ${error.code}, message: ${error.message}`);
}
} else {
try {
customScan.openFlashLight();
} catch (error) {
hilog.error(0x0001, TAG,
`Failed to open flashLight. Code: ${error.code}, message: ${error.message}`);
}
}
})
.visibility((this.userGrant && this.isFlashLightEnable) ? Visibility.Visible : Visibility.None)
// 扫码成功后，点击按钮后重新扫码
Button('Scan')
.onClick(() => {
// 点击按钮重启相机流，重新扫码
this.initCamera();
})
.visibility(this.isShowBack ? Visibility.Visible : Visibility.None)
}
Row() {
// 预览流设置缩放比例
Button('缩放比例,当前比例:' + this.setZoomValue)
.onClick(() => {
// 设置相机缩放比例
if (!this.isShowBack) {
if (!this.zoomValue || this.zoomValue === this.setZoomValue) {
this.setZoomValue = this.customGetZoom();
} else {
this.zoomValue = this.zoomValue;
this.customSetZoom(this.zoomValue);
setTimeout(() => {
if (!this.isShowBack) {
this.setZoomValue = this.customGetZoom();
}
}, 1000);
}
}
})
}
.margin({ top: 10, bottom: 10 })
Row() {
// 输入要设置的预览流缩放比例
TextInput({ placeholder: '输入缩放倍数' })
.type(InputType.Number)
.borderWidth(1)
.backgroundColor(Color.White)
.onChange(value => {
this.zoomValue = Number(value);
})
}
}
.width('50%')
.height(180)
}
// 单码、多码扫描后，显示码图蓝点位置。点击toast码图信息
ForEach(this.scanResult, (item: scanBarcode.ScanResult, index: number) => {
if (item.scanCodeRect) {
Image($rawfile('scan_selected2.svg'))
.width(40)
.height(40)
.markAnchor({ x: 20, y: 20 })
.position({
x: (item.scanCodeRect.left + item?.scanCodeRect?.right) / 2 + this.offsetX,
y: (item.scanCodeRect.top + item?.scanCodeRect?.bottom) / 2 + this.offsetY
})
.onClick(() => {
this.showScanResult(item);
})
}
})
}
// 建议相机流设置为全屏
.width('100%')
.height('100%')
.onClick((event: ClickEvent) => {
// 是否已扫描到结果
if (this.isShowBack) {
return;
}
// 点击屏幕位置，获取点击位置(x,y)，设置相机焦点
let x1 = vp2px(event.displayY) / (this.displayHeight + 0.0);
let y1 = 1.0 - (vp2px(event.displayX) / (this.displayWidth + 0.0));
try {
customScan.setFocusPoint({ x: x1, y: y1 });
hilog.info(0x0001, TAG, `Succeeded in setting focusPoint x1: ${x1}, y1: ${y1}`);
} catch (error) {
hilog.error(0x0001, TAG, `Failed to set focusPoint. Code: ${error.code}, message: ${error.message}`);
}
hilog.info(0x0001, TAG, `Succeeded in setting focusPoint x1: ${x1}, y1: ${y1}`);
// 设置连续自动对焦模式
setTimeout(() => {
try {
customScan.resetFocus();
} catch (error) {
hilog.error(0x0001, TAG, `Failed to reset Focus. Code: ${error.code}, message: ${error.message}`);
}
}, 200);
}).gesture(PinchGesture({ fingers: 2 })
.onActionStart((event: GestureEvent) => {
hilog.info(0x0001, TAG, 'Pinch start');
})
.onActionUpdate((event: GestureEvent) => {
if (event) {
this.scaleValue = event.scale;
}
})
.onActionEnd((event: GestureEvent) => {
// 是否已扫描到结果
if (this.isShowBack) {
return;
}
// 获取双指缩放比例，设置变焦比
try {
let zoom = this.customGetZoom();
this.pinchValue = this.scaleValue * zoom;
this.customSetZoom(this.pinchValue);
hilog.info(0x0001, TAG, 'Pinch end');
} catch (error) {
hilog.error(0x0001, TAG, `Failed to setZoom. Code: ${error.code}, message: ${error.message}`);
}
}))
}
public customGetZoom(): number {
let zoom = 1;
try {
zoom = customScan.getZoom();
hilog.info(0x0001, TAG, `Succeeded in getting Zoom, zoom: ${zoom}`);
} catch (error) {
hilog.error(0x0001, TAG, `Failed to getZoom. Code: ${error.code}, message: ${error?.message}`);
}
return zoom;
}
public customSetZoom(pinchValue: number): void {
try {
customScan.setZoom(pinchValue);
hilog.info(0x0001, TAG, `Succeeded in setting Zoom.`);
} catch (error) {
hilog.error(0x0001, TAG, `Failed to setZoom. Code: ${error.code}, message: ${error?.message}`);
}
}
}
```
```typescript
import { bundleManager, PermissionRequestResult, Permissions } from '@kit.AbilityKit';
const TAG = '[YUV CPSample]';
let context = getContext(this) as common.UIAbilityContext;
// 用户申请权限
export class PermissionsUtil {
public static async checkAccessToken(permission: Permissions): Promise<abilityAccessCtrl.GrantStatus> {
let atManager = abilityAccessCtrl.createAtManager();
let grantStatus: abilityAccessCtrl.GrantStatus = -1;
// 获取应用程序的accessTokenID
let tokenId: number = 0;
let bundleInfo: bundleManager.BundleInfo =
await bundleManager.getBundleInfoForSelf(bundleManager.BundleFlag.GET_BUNDLE_INFO_WITH_APPLICATION);
let appInfo: bundleManager.ApplicationInfo = bundleInfo.appInfo;
tokenId = appInfo.accessTokenId;
// 校验应用是否被授予权限
grantStatus = await atManager.checkAccessToken(tokenId, permission);
return grantStatus;
}
// 申请相机权限
public static async reqPermissionsFromUser(): Promise<number[]> {
hilog.info(0x0001, TAG, 'Succeeded in getting permissions by promise.')
let atManager = abilityAccessCtrl.createAtManager();
let grantStatus: PermissionRequestResult = { permissions: [], authResults: [] }
grantStatus = await atManager.requestPermissionsFromUser(context, ['ohos.permission.CAMERA']);
return grantStatus.authResults;
}
}
@Extend(Column)
function mainStyle() {
.width('100%')
.height('100%')
.padding({
top: 40
})
.justifyContent(FlexAlign.Center)
}
@Entry
@Component
struct YUVScan {
@State userGrant: boolean = false // 是否已申请相机权限
@State surfaceId: string = '' // xComponent组件生成id
@State cameraHeight: number = 640 // 设置预览流高度，默认单位：vp
@State cameraWidth: number = 360 // 设置预览流宽度，默认单位：vp
@State zoomValue: number = 1 // 预览流缩放比例
@State setZoomValue: number = 1 // 已设置的预览流缩放比例
@State isReleaseCamera: boolean = false // 是否已释放相机流
@State scanWidth: number = 384 // xComponent宽度，默认设置384，单位vp
@State scanHeight: number = 682 // xComponent高度，默认设置682，单位vp
@State scanBottom: number = 220
@State offsetX: number = 0 // xComponent位置x轴偏移量，单位vp
@State offsetY: number = 0 // xComponent位置y轴偏移量，单位vp
@State scanCodeRect: Array<scanBarcode.ScanCodeRect> = [] // 扫码结果码图位置
@State scanFlag: boolean = false // 是否已经扫码到结果
@State scanFrameResult: string = ''
@State scaleValue: number = 1 // 屏幕缩放比
@State pinchValue: number = 1 // 双指缩放比例
@State displayHeight: number = 0 // 屏幕高度，单位vp
@State displayWidth: number = 0 // 屏幕宽度，单位vp
private mXComponentController: XComponentController = new XComponentController()
private viewControl: customScan.ViewControl = { width: 1920, height: 1080, surfaceId: this.surfaceId }
options: scanBarcode.ScanOptions = {
// 扫码类型，可选参数
scanTypes: [scanCore.ScanType.ALL],
// 是否开启多码识别，可选参数
enableMultiMode: true,
// 是否开启相册扫码，可选参数
enableAlbum: true,
}
// 返回自定义扫描结果的回调
private callback: AsyncCallback<scanBarcode.ScanResult[]> =
async (error: BusinessError, result: scanBarcode.ScanResult[]) => {
if (error && error.code) {
hilog.error(0x0001, TAG,
`Failed to get ScanResult by callback. Code: ${error.code}, message: ${error.message}`);
return;
}
// 解析码值结果跳转应用服务页
hilog.info(0x0001, TAG, `Succeeded in getting ScanResult by callback, result: ${JSON.stringify(result)}`);
}
// 返回相机帧的回调
private frameCallback: AsyncCallback<customScan.ScanFrame> =
async (error: BusinessError, frameResult: customScan.ScanFrame) => {
if (error) {
hilog.error(0x0001, TAG, `Failed to get ScanFrame by callback. Code: ${error.code}, message: ${error.message}`);
return;
}
// byteBuffer相机YUV图像数组
hilog.info(0x0001, TAG,
`Succeeded in getting ScanFrame.byteBuffer.byteLength: ${frameResult.byteBuffer.byteLength}`)
hilog.info(0x0001, TAG, `Succeeded in getting ScanFrame.width: ${frameResult.width}`)
hilog.info(0x0001, TAG, `Succeeded in getting ScanFrame.height: ${frameResult.height}`)
this.scanFrameResult = JSON.stringify(frameResult.scanCodeRects);
if (frameResult && frameResult.scanCodeRects && frameResult.scanCodeRects.length > 0 && !this.scanFlag) {
if (frameResult.scanCodeRects[0]) {
this.stopCamera();
this.scanCodeRect = [];
this.scanFlag = true;
// 码图位置信息转换
this.changeToXComponent(frameResult);
} else {
this.scanFlag = false;
}
}
}
// frameCallback横向码图位置信息转换为预览流xComponent对应码图位置信息
changeToXComponent(frameResult: customScan.ScanFrame) {
if (frameResult && frameResult.scanCodeRects) {
let frameHeight = frameResult.height;
let ratio = this.scanWidth / frameHeight;
frameResult.scanCodeRects.forEach((item) => {
this.scanCodeRect.push({
left: this.toFixedNumber((frameHeight - item.bottom) * ratio),
top: this.toFixedNumber(item.left * ratio),
right: this.toFixedNumber((frameHeight - item.top) * ratio),
bottom: this.toFixedNumber(item.right * ratio)
});
});
this.scanFrameResult = JSON.stringify(this.scanCodeRect);
}
}
toFixedNumber(no: number): number {
return Number((no).toFixed(1));
}
async onPageShow() {
// 自定义启动第一步，用户申请权限
const permissions: Array<Permissions> = ['ohos.permission.CAMERA'];
// 自定义启动第二步：设置预览流布局尺寸
this.setDisplay();
let grantStatus = await PermissionsUtil.checkAccessToken(permissions[0]);
if (grantStatus === abilityAccessCtrl.GrantStatus.PERMISSION_GRANTED) {
// 已经授权，可以继续访问目标操作
this.userGrant = true;
if (this.surfaceId) {
// 自定义启动第三步，初始化接口
this.initCamera();
}
} else {
// 申请相机权限
this.requestCameraPermission();
}
}
async onPageHide() {
this.releaseCamera();
}
// 用户申请权限
async requestCameraPermission() {
let grantStatus = await PermissionsUtil.reqPermissionsFromUser()
let length: number = grantStatus.length;
for (let i = 0; i < length; i++) {
if (grantStatus[i] === 0) {
// 用户授权，可以继续访问目标操作
this.userGrant = true;
} else {
// 用户拒绝授权，提示用户必须授权才能访问当前页面的功能，并引导用户到系统设置中打开相应的权限
this.userGrant = false;
}
}
}
// 竖屏时获取屏幕尺寸，设置预览流全屏示例
setDisplay() {
try {
// 以手机为例计算宽高
let displayClass = display.getDefaultDisplaySync();
this.displayHeight = px2vp(displayClass.height);
this.displayWidth = px2vp(displayClass.width);
if (displayClass !== null) {
this.scanWidth = px2vp(displayClass.width);
this.scanHeight = Math.round(this.scanWidth * this.viewControl.width / this.viewControl.height);
this.scanBottom = Math.max(220, px2vp(displayClass.height) - this.scanHeight);
this.offsetX= 0;
this.offsetY= 0;
}
} catch (error) {
hilog.error(0x0001, TAG, `Failed to getDefaultDisplaySync. Code: ${error.code}, message: ${error.message}`);
}
}
// 初始化相机流
async initCamera() {
this.isReleaseCamera = false;
try {
// 自定义启动第三步，初始化接口
customScan.init(this.options);
hilog.info(0x0001, TAG, 'Succeeded in initting customScan with options.');
} catch (error) {
hilog.error(0x0001, TAG, `Failed to init customScan. Code: ${error.code}, message: ${error.message}`);
}
this.scanCodeRect = [];
this.scanFlag = false;
try {
// 自定义启动第四步，请求扫码接口
customScan.start(this.viewControl, this.callback, this.frameCallback);
} catch (error) {
hilog.error(0x0001, TAG, `Failed to start customScan. Code: ${error.code}, message: ${error.message}`);
}
}
// 暂停相机流
async stopCamera() {
if (!this.isReleaseCamera) {
try {
customScan.stop();
} catch (error) {
hilog.error(0x0001, TAG, `Failed to stop customScan. Code: ${error.code}, message: ${error.message}`);
}
}
}
// 释放相机流
async releaseCamera() {
if (!this.isReleaseCamera) {
await this.stopCamera();
try {
await customScan.release();
} catch (error) {
hilog.error(0x0001, TAG, `Failed to release customScan. Code: ${error.code}, message: ${error.message}`);
}
this.isReleaseCamera = true;
}
}
build() {
Stack() {
// 相机预览流XComponent
if (this.userGrant) {
Column() {
XComponent({
id: 'componentId',
type: XComponentType.SURFACE,
controller: this.mXComponentController
})
.onLoad(() => {
hilog.info(0x0001, TAG, 'Succeeded in loading, onLoad is called.');
this.surfaceId = this.mXComponentController.getXComponentSurfaceId();
hilog.info(0x0001, TAG, `Succeeded in getting surfaceId is ${this.surfaceId}`);
this.viewControl = { width: this.scanWidth, height: this.scanHeight, surfaceId: this.surfaceId };
// 启动相机进行扫码
this.initCamera();
})
.height(this.scanHeight)
.width(this.scanWidth)
.position({ x: 0, y: 0 })
}
.height('100%')
.width('100%')
.position({ x: this.offsetX, y: this.offsetY })
}
Column() {
Column() {
}
.layoutWeight(1)
.width('100%')
Column() {
Row() {
// 闪光灯按钮，启动相机流后才能使用
Button('FlashLight')
.onClick(() => {
let lightStatus: boolean = false;
try {
lightStatus = customScan.getFlashLightStatus();
} catch (error) {
hilog.error(0x0001, TAG,
`Failed to get flashLightStatus. Code: ${error.code}, message: ${error.message}`);
}
// 根据当前闪光灯状态，选择打开或关闭闪关灯
if (lightStatus) {
try {
customScan.closeFlashLight();
} catch (error) {
hilog.error(0x0001, TAG,
`Failed to close flashLight. Code: ${error.code}, message: ${error.message}`);
}
} else {
try {
customScan.openFlashLight();
} catch (error) {
hilog.error(0x0001, TAG,
`Failed to open flashLight. Code: ${error.code}, message: ${error.message}`);
}
}
})
.visibility(this.scanFlag ? Visibility.None : Visibility.Visible)
}
Row() {
// 预览流设置缩放比例
Button('缩放比例,当前比例:' + this.setZoomValue)
.width(200)
.alignSelf(ItemAlign.Center)
.onClick(() => {
// 设置相机缩放比例
if (!this.scanFlag) {
if (!this.zoomValue || this.zoomValue === this.setZoomValue) {
this.setZoomValue = this.customGetZoom();
} else {
this.zoomValue = this.zoomValue;
this.customSetZoom(this.zoomValue);
setTimeout(() => {
if (!this.scanFlag) {
this.setZoomValue = this.customGetZoom();
}
}, 1000);
}
}
})
}
.margin({ top: 10, bottom: 10 })
.visibility(this.scanFlag ? Visibility.None : Visibility.Visible)
Row() {
// 输入要设置的预览流缩放比例
TextInput({ placeholder: '输入缩放倍数' })
.width(200)
.type(InputType.Number)
.borderWidth(1)
.backgroundColor(Color.White)
.onChange(value => {
this.zoomValue = Number(value);
})
}
.visibility(this.scanFlag ? Visibility.None : Visibility.Visible)
Text(this.scanFlag ? '继续扫码' : '扫码中')
.height(30)
.fontSize(16)
.fontColor(Color.White)
.onClick(() => {
if (this.scanFlag) {
this.scanFrameResult = '';
this.initCamera();
}
})
Text('扫码结果：' + this.scanFrameResult).fontColor(Color.White).fontSize(12)
}
.width('100%')
.height(this.scanBottom)
.backgroundColor(Color.Black)
}
.mainStyle()
Image($rawfile('scan_back.svg'))
.width(20)
.height(20)
.position({
x: 40,
y: 40
})
.onClick(() => {
router.back();
})
// 实时扫码码图中心点位置
if (this.scanFlag && this.scanCodeRect.length > 0) {
ForEach(this.scanCodeRect, (item: scanBarcode.ScanCodeRect, index: number) => {
Image($rawfile('scan_selected2.svg'))
.width(40)
.height(40)
.markAnchor({ x: 20, y: 20 })
.position({
x: (item.left + item.right) / 2 + this.offsetX,
y: (item.top + item.bottom) / 2 + this.offsetY
})
})
}
}
.width('100%')
.height('100%')
.backgroundColor(this.userGrant ? Color.Transparent : Color.Black)
.onClick((event: ClickEvent) => {
// 是否已扫描到结果
if (this.scanFlag) {
return;
}
// 点击屏幕位置，获取点击位置(x,y)，设置相机焦点
let x1 = vp2px(event.displayY) / (this.displayHeight + 0.0);
let y1 = 1.0 - (vp2px(event.displayX) / (this.displayWidth + 0.0));
try {
customScan.setFocusPoint({ x: x1, y: y1 });
hilog.info(0x0001, TAG, `Succeeded in setting focusPoint x1: ${x1}, y1: ${y1}`);
} catch (error) {
hilog.error(0x0001, TAG, `Failed to set focusPoint. Code: ${error.code}, message: ${error.message}`);
}
setTimeout(() => {
try {
customScan.resetFocus();
} catch (error) {
hilog.error(0x0001, TAG, `Failed to reset Focus. Code: ${error.code}, message: ${error.message}`);
}
}, 200);
})
.gesture(PinchGesture({ fingers: 2 })
.onActionStart((event: GestureEvent) => {
hilog.info(0x0001, TAG, 'Pinch start');
})
.onActionUpdate((event: GestureEvent) => {
if (event) {
this.scaleValue = event.scale;
}
})
.onActionEnd((event: GestureEvent) => {
// 是否已扫描到结果
if (this.scanFlag) {
return;
}
// 获取双指缩放比例，设置变焦比
try {
let zoom = this.customGetZoom();
this.pinchValue = this.scaleValue * zoom;
this.customSetZoom(this.pinchValue);
hilog.info(0x0001, TAG, 'Pinch end');
} catch (error) {
hilog.error(0x0001, TAG, `Failed to setZoom. Code: ${error.code}, message: ${error.message}`);
}
}))
}
public customGetZoom(): number {
let zoom = 1;
try {
zoom = customScan.getZoom();
hilog.info(0x0001, TAG, `Succeeded in getting Zoom, zoom: ${zoom}`);
} catch (error) {
hilog.error(0x0001, TAG, `Failed to getZoom. Code: ${error.code}, message: ${error?.message}`);
}
return zoom;
}
public customSetZoom(pinchValue: number): void {
try {
customScan.setZoom(pinchValue);
hilog.info(0x0001, TAG, `Succeeded in setting Zoom.`);
} catch (error) {
hilog.error(0x0001, TAG, `Failed to setZoom. Code: ${error.code}, message: ${error?.message}`);
}
}
}
```
| 权限名 | 说明 | 授权方式 |
| --- | --- | --- |
| ohos.permission.CAMERA | 允许应用使用相机扫码。 | user_grant |
```typescript
const TAG: string = '[customScanPage]';
@Entry
@Component
struct CustomScanPage {
@State userGrant: boolean = false // 是否已申请相机权限
@State surfaceId: string = '' // xComponent组件生成id
@State isShowBack: boolean = false // 是否已经返回扫码结果
@State isFlashLightEnable: boolean = false // 是否开启了闪光灯
@State isSensorLight: boolean = false // 记录当前环境亮暗状态
@State cameraHeight: number = 640 // 设置预览流高度，默认单位：vp
@State cameraWidth: number = 360 // 设置预览流宽度，默认单位：vp
@State offsetX: number = 0 // 设置预览流x轴方向偏移量，默认单位：vp
@State offsetY: number = 0 // 设置预览流y轴方向偏移量，默认单位：vp
@State zoomValue: number = 1 // 预览流缩放比例
@State setZoomValue: number = 1 // 已设置的预览流缩放比例
@State scaleValue: number = 1 // 屏幕缩放比
@State pinchValue: number = 1 // 双指缩放比例
@State displayHeight: number = 0 // 屏幕高度，单位vp
@State displayWidth: number = 0 // 屏幕宽度，单位vp
@State scanResult: Array<scanBarcode.ScanResult> = [] // 扫码结果
private mXComponentController: XComponentController = new XComponentController()
async onPageShow() {
// 自定义启动第一步，用户申请权限
await this.requestCameraPermission();
// 多码扫码识别，enableMultiMode: true 单码扫码识别enableMultiMode: false
let options: scanBarcode.ScanOptions = {
scanTypes: [scanCore.ScanType.ALL],
enableMultiMode: true,
enableAlbum: true
}
// 自定义启动第二步：设置预览流布局尺寸
this.setDisplay();
try {
// 自定义启动第三步，初始化接口
customScan.init(options);
} catch (error) {
hilog.error(0x0001, TAG, `Failed to init customScan. Code: ${error.code}, message: ${error.message}`);
}
}
async onPageHide() {
// 页面消失或隐藏时，停止并释放相机流
this.userGrant = false;
this.isFlashLightEnable = false;
this.isSensorLight = false;
try {
customScan.off('lightingFlash');
} catch (error) {
hilog.error(0x0001, TAG, `Failed to off lightingFlash. Code: ${error.code}, message: ${error.message}`);
}
this.customScanStop();
try {
// 自定义相机流释放接口
customScan.release().then(() => {
hilog.info(0x0001, TAG, 'Succeeded in releasing customScan by promise.');
}).catch((error: BusinessError) => {
hilog.error(0x0001, TAG,
`Failed to release customScan by promise. Code: ${error.code}, message: ${error.message}`);
})
} catch (error) {
hilog.error(0x0001, TAG, `Failed to release customScan. Code: ${error.code}, message: ${error.message}`);
}
}
// 用户申请权限
async reqPermissionsFromUser(): Promise<number[]> {
hilog.info(0x0001, TAG, 'reqPermissionsFromUser start');
let context = getContext() as common.UIAbilityContext;
let atManager = abilityAccessCtrl.createAtManager();
let grantStatus = await atManager.requestPermissionsFromUser(context, ['ohos.permission.CAMERA']);
return grantStatus.authResults;
}
// 用户申请相机权限
async requestCameraPermission() {
let grantStatus = await this.reqPermissionsFromUser();
for (let i = 0; i < grantStatus.length; i++) {
if (grantStatus[i] === 0) {
// 用户授权，可以继续访问目标操作
hilog.info(0x0001, TAG, 'Succeeded in getting permissions.');
this.userGrant = true;
}
}
}
// 竖屏时获取屏幕尺寸，设置预览流全屏示例
setDisplay() {
try {
// 默认竖屏
let displayClass = display.getDefaultDisplaySync();
this.displayHeight = px2vp(displayClass.height);
this.displayWidth = px2vp(displayClass.width);
let maxLen: number = Math.max(this.displayWidth, this.displayHeight);
let minLen: number = Math.min(this.displayWidth, this.displayHeight);
const RATIO: number = 16 / 9;
this.cameraHeight = maxLen;
this.cameraWidth = maxLen / RATIO;
this.offsetX = (minLen - this.cameraWidth) / 2;
} catch (error) {
hilog.error(0x0001, TAG, `Failed to getDefaultDisplaySync. Code: ${error.code}, message: ${error.message}`);
}
}
// toast显示扫码结果
async showScanResult(result: scanBarcode.ScanResult) {
// 使用toast显示出扫码结果
promptAction.showToast({
message: JSON.stringify(result),
duration: 5000
});
}
initCamera() {
this.isShowBack = false;
this.scanResult = [];
let viewControl: customScan.ViewControl = {
width: this.cameraWidth,
height: this.cameraHeight,
surfaceId: this.surfaceId
};
try {
// 自定义启动第四步，请求扫码接口，通过Promise方式回调
customScan.start(viewControl)
.then(async (result: Array<scanBarcode.ScanResult>) => {
hilog.info(0x0001, TAG, `result: ${JSON.stringify(result)}`);
if (result.length) {
// 解析码值结果跳转应用服务页
this.scanResult = result;
this.isShowBack = true;
// 获取到扫描结果后暂停相机流
this.customScanStop();
}
});
} catch (error) {
hilog.error(0x0001, TAG, `Failed to start customScan. Code: ${error.code}, message: ${error.message}`);
}
}
customScanStop() {
try {
customScan.stop();
} catch (error) {
hilog.error(0x0001, TAG, `Failed to stop customScan. Code: ${error.code}, message: ${error.message}`);
}
}
// 自定义扫码界面的顶部返回按钮和扫码提示
@Builder
TopTool() {
Column() {
Flex({ direction: FlexDirection.Row, justifyContent: FlexAlign.SpaceBetween, alignItems: ItemAlign.Center }) {
Text('返回')
.onClick(async () => {
router.back();
})
}.padding({ left: 24, right: 24, top: 40 })
Column() {
Text('扫描二维码/条形码')
Text('对准二维码/条形码，即可自动扫描')
}.margin({ left: 24, right: 24, top: 24 })
}
.height(146)
.width('100%')
}
build() {
Stack() {
if (this.userGrant) {
Column() {
XComponent({
id: 'componentId',
type: XComponentType.SURFACE,
controller: this.mXComponentController
})
.onLoad(async () => {
hilog.info(0x0001, TAG, 'Succeeded in loading, onLoad is called.');
// 获取XComponent组件的surfaceId
this.surfaceId = this.mXComponentController.getXComponentSurfaceId();
hilog.info(0x0001, TAG, `Succeeded in getting surfaceId: ${this.surfaceId}`);
this.initCamera();
// 闪光灯监听接口
customScan.on('lightingFlash', (error, isLightingFlash) => {
if (error) {
hilog.error(0x0001, TAG,
`Failed to on lightingFlash. Code: ${error.code}, message: ${error.message}`);
return;
}
if (isLightingFlash) {
this.isFlashLightEnable = true;
} else {
try {
if (!customScan.getFlashLightStatus()) {
this.isFlashLightEnable = false;
}
} catch (error) {
hilog.error(0x0001, TAG,
`Failed to get FlashLightStatus. Code: ${error.code}, message: ${error.message}`);
}
}
this.isSensorLight = isLightingFlash;
});
})
.width(this.cameraWidth)
.height(this.cameraHeight)
.position({ x: this.offsetX, y: this.offsetY })
}
.height('100%')
.width('100%')
}
Column() {
this.TopTool()
Column() {
}
.layoutWeight(1)
.width('100%')
Column() {
Row() {
// 闪光灯按钮，启动相机流后才能使用
Button('FlashLight')
.onClick(() => {
let lightStatus: boolean = false;
try {
lightStatus = customScan.getFlashLightStatus();
} catch (error) {
hilog.error(0x0001, TAG,
`Failed to get flashLightStatus. Code: ${error.code}, message: ${error.message}`);
}
// 根据当前闪光灯状态，选择打开或关闭闪关灯
if (lightStatus) {
try {
customScan.closeFlashLight();
setTimeout(() => {
this.isFlashLightEnable = this.isSensorLight;
}, 200);
} catch (error) {
hilog.error(0x0001, TAG,
`Failed to close flashLight. Code: ${error.code}, message: ${error.message}`);
}
} else {
try {
customScan.openFlashLight();
} catch (error) {
hilog.error(0x0001, TAG,
`Failed to open flashLight. Code: ${error.code}, message: ${error.message}`);
}
}
})
.visibility((this.userGrant && this.isFlashLightEnable) ? Visibility.Visible : Visibility.None)
// 扫码成功后，点击按钮后重新扫码
Button('Scan')
.onClick(() => {
// 点击按钮重启相机流，重新扫码
this.initCamera();
})
.visibility(this.isShowBack ? Visibility.Visible : Visibility.None)
}
Row() {
// 预览流设置缩放比例
Button('缩放比例,当前比例:' + this.setZoomValue)
.onClick(() => {
// 设置相机缩放比例
if (!this.isShowBack) {
if (!this.zoomValue || this.zoomValue === this.setZoomValue) {
this.setZoomValue = this.customGetZoom();
} else {
this.zoomValue = this.zoomValue;
this.customSetZoom(this.zoomValue);
setTimeout(() => {
if (!this.isShowBack) {
this.setZoomValue = this.customGetZoom();
}
}, 1000);
}
}
})
}
.margin({ top: 10, bottom: 10 })
Row() {
// 输入要设置的预览流缩放比例
TextInput({ placeholder: '输入缩放倍数' })
.type(InputType.Number)
.borderWidth(1)
.backgroundColor(Color.White)
.onChange(value => {
this.zoomValue = Number(value);
})
}
}
.width('50%')
.height(180)
}
// 单码、多码扫描后，显示码图蓝点位置。点击toast码图信息
ForEach(this.scanResult, (item: scanBarcode.ScanResult, index: number) => {
if (item.scanCodeRect) {
Image($rawfile('scan_selected2.svg'))
.width(40)
.height(40)
.markAnchor({ x: 20, y: 20 })
.position({
x: (item.scanCodeRect.left + item?.scanCodeRect?.right) / 2 + this.offsetX,
y: (item.scanCodeRect.top + item?.scanCodeRect?.bottom) / 2 + this.offsetY
})
.onClick(() => {
this.showScanResult(item);
})
}
})
}
// 建议相机流设置为全屏
.width('100%')
.height('100%')
.onClick((event: ClickEvent) => {
// 是否已扫描到结果
if (this.isShowBack) {
return;
}
// 点击屏幕位置，获取点击位置(x,y)，设置相机焦点
let x1 = vp2px(event.displayY) / (this.displayHeight + 0.0);
let y1 = 1.0 - (vp2px(event.displayX) / (this.displayWidth + 0.0));
try {
customScan.setFocusPoint({ x: x1, y: y1 });
hilog.info(0x0001, TAG, `Succeeded in setting focusPoint x1: ${x1}, y1: ${y1}`);
} catch (error) {
hilog.error(0x0001, TAG, `Failed to set focusPoint. Code: ${error.code}, message: ${error.message}`);
}
hilog.info(0x0001, TAG, `Succeeded in setting focusPoint x1: ${x1}, y1: ${y1}`);
// 设置连续自动对焦模式
setTimeout(() => {
try {
customScan.resetFocus();
} catch (error) {
hilog.error(0x0001, TAG, `Failed to reset Focus. Code: ${error.code}, message: ${error.message}`);
}
}, 200);
}).gesture(PinchGesture({ fingers: 2 })
.onActionStart((event: GestureEvent) => {
hilog.info(0x0001, TAG, 'Pinch start');
})
.onActionUpdate((event: GestureEvent) => {
if (event) {
this.scaleValue = event.scale;
}
})
.onActionEnd((event: GestureEvent) => {
// 是否已扫描到结果
if (this.isShowBack) {
return;
}
// 获取双指缩放比例，设置变焦比
try {
let zoom = this.customGetZoom();
this.pinchValue = this.scaleValue * zoom;
this.customSetZoom(this.pinchValue);
hilog.info(0x0001, TAG, 'Pinch end');
} catch (error) {
hilog.error(0x0001, TAG, `Failed to setZoom. Code: ${error.code}, message: ${error.message}`);
}
}))
}
public customGetZoom(): number {
let zoom = 1;
try {
zoom = customScan.getZoom();
hilog.info(0x0001, TAG, `Succeeded in getting Zoom, zoom: ${zoom}`);
} catch (error) {
hilog.error(0x0001, TAG, `Failed to getZoom. Code: ${error.code}, message: ${error?.message}`);
}
return zoom;
}
public customSetZoom(pinchValue: number): void {
try {
customScan.setZoom(pinchValue);
hilog.info(0x0001, TAG, `Succeeded in setting Zoom.`);
} catch (error) {
hilog.error(0x0001, TAG, `Failed to setZoom. Code: ${error.code}, message: ${error?.message}`);
}
}
}
```
```typescript
import { bundleManager, PermissionRequestResult, Permissions } from '@kit.AbilityKit';
const TAG = '[YUV CPSample]';
let context = getContext(this) as common.UIAbilityContext;
// 用户申请权限
export class PermissionsUtil {
public static async checkAccessToken(permission: Permissions): Promise<abilityAccessCtrl.GrantStatus> {
let atManager = abilityAccessCtrl.createAtManager();
let grantStatus: abilityAccessCtrl.GrantStatus = -1;
// 获取应用程序的accessTokenID
let tokenId: number = 0;
let bundleInfo: bundleManager.BundleInfo =
await bundleManager.getBundleInfoForSelf(bundleManager.BundleFlag.GET_BUNDLE_INFO_WITH_APPLICATION);
let appInfo: bundleManager.ApplicationInfo = bundleInfo.appInfo;
tokenId = appInfo.accessTokenId;
// 校验应用是否被授予权限
grantStatus = await atManager.checkAccessToken(tokenId, permission);
return grantStatus;
}
// 申请相机权限
public static async reqPermissionsFromUser(): Promise<number[]> {
hilog.info(0x0001, TAG, 'Succeeded in getting permissions by promise.')
let atManager = abilityAccessCtrl.createAtManager();
let grantStatus: PermissionRequestResult = { permissions: [], authResults: [] }
grantStatus = await atManager.requestPermissionsFromUser(context, ['ohos.permission.CAMERA']);
return grantStatus.authResults;
}
}
@Extend(Column)
function mainStyle() {
.width('100%')
.height('100%')
.padding({
top: 40
})
.justifyContent(FlexAlign.Center)
}
@Entry
@Component
struct YUVScan {
@State userGrant: boolean = false // 是否已申请相机权限
@State surfaceId: string = '' // xComponent组件生成id
@State cameraHeight: number = 640 // 设置预览流高度，默认单位：vp
@State cameraWidth: number = 360 // 设置预览流宽度，默认单位：vp
@State zoomValue: number = 1 // 预览流缩放比例
@State setZoomValue: number = 1 // 已设置的预览流缩放比例
@State isReleaseCamera: boolean = false // 是否已释放相机流
@State scanWidth: number = 384 // xComponent宽度，默认设置384，单位vp
@State scanHeight: number = 682 // xComponent高度，默认设置682，单位vp
@State scanBottom: number = 220
@State offsetX: number = 0 // xComponent位置x轴偏移量，单位vp
@State offsetY: number = 0 // xComponent位置y轴偏移量，单位vp
@State scanCodeRect: Array<scanBarcode.ScanCodeRect> = [] // 扫码结果码图位置
@State scanFlag: boolean = false // 是否已经扫码到结果
@State scanFrameResult: string = ''
@State scaleValue: number = 1 // 屏幕缩放比
@State pinchValue: number = 1 // 双指缩放比例
@State displayHeight: number = 0 // 屏幕高度，单位vp
@State displayWidth: number = 0 // 屏幕宽度，单位vp
private mXComponentController: XComponentController = new XComponentController()
private viewControl: customScan.ViewControl = { width: 1920, height: 1080, surfaceId: this.surfaceId }
options: scanBarcode.ScanOptions = {
// 扫码类型，可选参数
scanTypes: [scanCore.ScanType.ALL],
// 是否开启多码识别，可选参数
enableMultiMode: true,
// 是否开启相册扫码，可选参数
enableAlbum: true,
}
// 返回自定义扫描结果的回调
private callback: AsyncCallback<scanBarcode.ScanResult[]> =
async (error: BusinessError, result: scanBarcode.ScanResult[]) => {
if (error && error.code) {
hilog.error(0x0001, TAG,
`Failed to get ScanResult by callback. Code: ${error.code}, message: ${error.message}`);
return;
}
// 解析码值结果跳转应用服务页
hilog.info(0x0001, TAG, `Succeeded in getting ScanResult by callback, result: ${JSON.stringify(result)}`);
}
// 返回相机帧的回调
private frameCallback: AsyncCallback<customScan.ScanFrame> =
async (error: BusinessError, frameResult: customScan.ScanFrame) => {
if (error) {
hilog.error(0x0001, TAG, `Failed to get ScanFrame by callback. Code: ${error.code}, message: ${error.message}`);
return;
}
// byteBuffer相机YUV图像数组
hilog.info(0x0001, TAG,
`Succeeded in getting ScanFrame.byteBuffer.byteLength: ${frameResult.byteBuffer.byteLength}`)
hilog.info(0x0001, TAG, `Succeeded in getting ScanFrame.width: ${frameResult.width}`)
hilog.info(0x0001, TAG, `Succeeded in getting ScanFrame.height: ${frameResult.height}`)
this.scanFrameResult = JSON.stringify(frameResult.scanCodeRects);
if (frameResult && frameResult.scanCodeRects && frameResult.scanCodeRects.length > 0 && !this.scanFlag) {
if (frameResult.scanCodeRects[0]) {
this.stopCamera();
this.scanCodeRect = [];
this.scanFlag = true;
// 码图位置信息转换
this.changeToXComponent(frameResult);
} else {
this.scanFlag = false;
}
}
}
// frameCallback横向码图位置信息转换为预览流xComponent对应码图位置信息
changeToXComponent(frameResult: customScan.ScanFrame) {
if (frameResult && frameResult.scanCodeRects) {
let frameHeight = frameResult.height;
let ratio = this.scanWidth / frameHeight;
frameResult.scanCodeRects.forEach((item) => {
this.scanCodeRect.push({
left: this.toFixedNumber((frameHeight - item.bottom) * ratio),
top: this.toFixedNumber(item.left * ratio),
right: this.toFixedNumber((frameHeight - item.top) * ratio),
bottom: this.toFixedNumber(item.right * ratio)
});
});
this.scanFrameResult = JSON.stringify(this.scanCodeRect);
}
}
toFixedNumber(no: number): number {
return Number((no).toFixed(1));
}
async onPageShow() {
// 自定义启动第一步，用户申请权限
const permissions: Array<Permissions> = ['ohos.permission.CAMERA'];
// 自定义启动第二步：设置预览流布局尺寸
this.setDisplay();
let grantStatus = await PermissionsUtil.checkAccessToken(permissions[0]);
if (grantStatus === abilityAccessCtrl.GrantStatus.PERMISSION_GRANTED) {
// 已经授权，可以继续访问目标操作
this.userGrant = true;
if (this.surfaceId) {
// 自定义启动第三步，初始化接口
this.initCamera();
}
} else {
// 申请相机权限
this.requestCameraPermission();
}
}
async onPageHide() {
this.releaseCamera();
}
// 用户申请权限
async requestCameraPermission() {
let grantStatus = await PermissionsUtil.reqPermissionsFromUser()
let length: number = grantStatus.length;
for (let i = 0; i < length; i++) {
if (grantStatus[i] === 0) {
// 用户授权，可以继续访问目标操作
this.userGrant = true;
} else {
// 用户拒绝授权，提示用户必须授权才能访问当前页面的功能，并引导用户到系统设置中打开相应的权限
this.userGrant = false;
}
}
}
// 竖屏时获取屏幕尺寸，设置预览流全屏示例
setDisplay() {
try {
// 以手机为例计算宽高
let displayClass = display.getDefaultDisplaySync();
this.displayHeight = px2vp(displayClass.height);
this.displayWidth = px2vp(displayClass.width);
if (displayClass !== null) {
this.scanWidth = px2vp(displayClass.width);
this.scanHeight = Math.round(this.scanWidth * this.viewControl.width / this.viewControl.height);
this.scanBottom = Math.max(220, px2vp(displayClass.height) - this.scanHeight);
this.offsetX= 0;
this.offsetY= 0;
}
} catch (error) {
hilog.error(0x0001, TAG, `Failed to getDefaultDisplaySync. Code: ${error.code}, message: ${error.message}`);
}
}
// 初始化相机流
async initCamera() {
this.isReleaseCamera = false;
try {
// 自定义启动第三步，初始化接口
customScan.init(this.options);
hilog.info(0x0001, TAG, 'Succeeded in initting customScan with options.');
} catch (error) {
hilog.error(0x0001, TAG, `Failed to init customScan. Code: ${error.code}, message: ${error.message}`);
}
this.scanCodeRect = [];
this.scanFlag = false;
try {
// 自定义启动第四步，请求扫码接口
customScan.start(this.viewControl, this.callback, this.frameCallback);
} catch (error) {
hilog.error(0x0001, TAG, `Failed to start customScan. Code: ${error.code}, message: ${error.message}`);
}
}
// 暂停相机流
async stopCamera() {
if (!this.isReleaseCamera) {
try {
customScan.stop();
} catch (error) {
hilog.error(0x0001, TAG, `Failed to stop customScan. Code: ${error.code}, message: ${error.message}`);
}
}
}
// 释放相机流
async releaseCamera() {
if (!this.isReleaseCamera) {
await this.stopCamera();
try {
await customScan.release();
} catch (error) {
hilog.error(0x0001, TAG, `Failed to release customScan. Code: ${error.code}, message: ${error.message}`);
}
this.isReleaseCamera = true;
}
}
build() {
Stack() {
// 相机预览流XComponent
if (this.userGrant) {
Column() {
XComponent({
id: 'componentId',
type: XComponentType.SURFACE,
controller: this.mXComponentController
})
.onLoad(() => {
hilog.info(0x0001, TAG, 'Succeeded in loading, onLoad is called.');
this.surfaceId = this.mXComponentController.getXComponentSurfaceId();
hilog.info(0x0001, TAG, `Succeeded in getting surfaceId is ${this.surfaceId}`);
this.viewControl = { width: this.scanWidth, height: this.scanHeight, surfaceId: this.surfaceId };
// 启动相机进行扫码
this.initCamera();
})
.height(this.scanHeight)
.width(this.scanWidth)
.position({ x: 0, y: 0 })
}
.height('100%')
.width('100%')
.position({ x: this.offsetX, y: this.offsetY })
}
Column() {
Column() {
}
.layoutWeight(1)
.width('100%')
Column() {
Row() {
// 闪光灯按钮，启动相机流后才能使用
Button('FlashLight')
.onClick(() => {
let lightStatus: boolean = false;
try {
lightStatus = customScan.getFlashLightStatus();
} catch (error) {
hilog.error(0x0001, TAG,
`Failed to get flashLightStatus. Code: ${error.code}, message: ${error.message}`);
}
// 根据当前闪光灯状态，选择打开或关闭闪关灯
if (lightStatus) {
try {
customScan.closeFlashLight();
} catch (error) {
hilog.error(0x0001, TAG,
`Failed to close flashLight. Code: ${error.code}, message: ${error.message}`);
}
} else {
try {
customScan.openFlashLight();
} catch (error) {
hilog.error(0x0001, TAG,
`Failed to open flashLight. Code: ${error.code}, message: ${error.message}`);
}
}
})
.visibility(this.scanFlag ? Visibility.None : Visibility.Visible)
}
Row() {
// 预览流设置缩放比例
Button('缩放比例,当前比例:' + this.setZoomValue)
.width(200)
.alignSelf(ItemAlign.Center)
.onClick(() => {
// 设置相机缩放比例
if (!this.scanFlag) {
if (!this.zoomValue || this.zoomValue === this.setZoomValue) {
this.setZoomValue = this.customGetZoom();
} else {
this.zoomValue = this.zoomValue;
this.customSetZoom(this.zoomValue);
setTimeout(() => {
if (!this.scanFlag) {
this.setZoomValue = this.customGetZoom();
}
}, 1000);
}
}
})
}
.margin({ top: 10, bottom: 10 })
.visibility(this.scanFlag ? Visibility.None : Visibility.Visible)
Row() {
// 输入要设置的预览流缩放比例
TextInput({ placeholder: '输入缩放倍数' })
.width(200)
.type(InputType.Number)
.borderWidth(1)
.backgroundColor(Color.White)
.onChange(value => {
this.zoomValue = Number(value);
})
}
.visibility(this.scanFlag ? Visibility.None : Visibility.Visible)
Text(this.scanFlag ? '继续扫码' : '扫码中')
.height(30)
.fontSize(16)
.fontColor(Color.White)
.onClick(() => {
if (this.scanFlag) {
this.scanFrameResult = '';
this.initCamera();
}
})
Text('扫码结果：' + this.scanFrameResult).fontColor(Color.White).fontSize(12)
}
.width('100%')
.height(this.scanBottom)
.backgroundColor(Color.Black)
}
.mainStyle()
Image($rawfile('scan_back.svg'))
.width(20)
.height(20)
.position({
x: 40,
y: 40
})
.onClick(() => {
router.back();
})
// 实时扫码码图中心点位置
if (this.scanFlag && this.scanCodeRect.length > 0) {
ForEach(this.scanCodeRect, (item: scanBarcode.ScanCodeRect, index: number) => {
Image($rawfile('scan_selected2.svg'))
.width(40)
.height(40)
.markAnchor({ x: 20, y: 20 })
.position({
x: (item.left + item.right) / 2 + this.offsetX,
y: (item.top + item.bottom) / 2 + this.offsetY
})
})
}
}
.width('100%')
.height('100%')
.backgroundColor(this.userGrant ? Color.Transparent : Color.Black)
.onClick((event: ClickEvent) => {
// 是否已扫描到结果
if (this.scanFlag) {
return;
}
// 点击屏幕位置，获取点击位置(x,y)，设置相机焦点
let x1 = vp2px(event.displayY) / (this.displayHeight + 0.0);
let y1 = 1.0 - (vp2px(event.displayX) / (this.displayWidth + 0.0));
try {
customScan.setFocusPoint({ x: x1, y: y1 });
hilog.info(0x0001, TAG, `Succeeded in setting focusPoint x1: ${x1}, y1: ${y1}`);
} catch (error) {
hilog.error(0x0001, TAG, `Failed to set focusPoint. Code: ${error.code}, message: ${error.message}`);
}
setTimeout(() => {
try {
customScan.resetFocus();
} catch (error) {
hilog.error(0x0001, TAG, `Failed to reset Focus. Code: ${error.code}, message: ${error.message}`);
}
}, 200);
})
.gesture(PinchGesture({ fingers: 2 })
.onActionStart((event: GestureEvent) => {
hilog.info(0x0001, TAG, 'Pinch start');
})
.onActionUpdate((event: GestureEvent) => {
if (event) {
this.scaleValue = event.scale;
}
})
.onActionEnd((event: GestureEvent) => {
// 是否已扫描到结果
if (this.scanFlag) {
return;
}
// 获取双指缩放比例，设置变焦比
try {
let zoom = this.customGetZoom();
this.pinchValue = this.scaleValue * zoom;
this.customSetZoom(this.pinchValue);
hilog.info(0x0001, TAG, 'Pinch end');
} catch (error) {
hilog.error(0x0001, TAG, `Failed to setZoom. Code: ${error.code}, message: ${error.message}`);
}
}))
}
public customGetZoom(): number {
let zoom = 1;
try {
zoom = customScan.getZoom();
hilog.info(0x0001, TAG, `Succeeded in getting Zoom, zoom: ${zoom}`);
} catch (error) {
hilog.error(0x0001, TAG, `Failed to getZoom. Code: ${error.code}, message: ${error?.message}`);
}
return zoom;
}
public customSetZoom(pinchValue: number): void {
try {
customScan.setZoom(pinchValue);
hilog.info(0x0001, TAG, `Succeeded in setting Zoom.`);
} catch (error) {
hilog.error(0x0001, TAG, `Failed to setZoom. Code: ${error.code}, message: ${error?.message}`);
}
}
}
```
模拟器开发
暂不支持模拟器使用，调用会返回错误信息“Emulator is not supported.”

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-imagerecognition-V14
爬取时间: 2025-04-28 20:49:15
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-detectbarcode-V14
爬取时间: 2025-04-28 20:49:29
来源: Huawei Developer
基本概念
图片识码能力支持对图库中的码图进行扫描识别，并获取信息。
场景介绍
图片识码能力支持对图库中的条形码、二维码、MULTIFUNCTIONAL CODE进行识别，并获得码类型、码值、码位置信息。该能力可用于一图单码和一图多码的识别，比如条形码、付款码等。
业务流程
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314170001.64051163600977922449552840376029:50001231000000:2800:6C1F76639237B39B715DE71654A32D22A50D10B154B6D0FC832A2A5B2837E203.png)
接口说明
接口返回值有两种返回形式：Callback和Promise回调。下表中为启动图片识码Callback和Promise形式接口，Callback和Promise只是返回值方式不一样，功能相同。具体API说明详见接口文档。
| 接口名 | 描述 |
| --- | --- |
| decode(inputImage: InputImage, options?: scanBarcode.ScanOptions): Promise<Array<scanBarcode.ScanResult>> | 启动图片识码，通过InputImage传入图片信息，通过ScanOptions进行识码参数设置（options为可选参数），使用Promise异步回调返回识码结果。 |
| decode(inputImage: InputImage, options: scanBarcode.ScanOptions, callback: AsyncCallback<Array<scanBarcode.ScanResult>>): void | 启动图片识码，通过InputImage传入图片信息，通过ScanOptions进行识码参数设置，使用Callback异步回调返回识码结果。 |
| decode(inputImage: InputImage, callback: AsyncCallback<Array<scanBarcode.ScanResult>>): void | 启动图片识码，通过InputImage传入图片信息，使用Callback异步回调返回识码结果。 |
接口名
描述
decode(inputImage:InputImage, options?: scanBarcode.ScanOptions): Promise<Array<scanBarcode.ScanResult>>
启动图片识码，通过InputImage传入图片信息，通过ScanOptions进行识码参数设置（options为可选参数），使用Promise异步回调返回识码结果。
decode(inputImage: InputImage, options: scanBarcode.ScanOptions, callback: AsyncCallback<Array<scanBarcode.ScanResult>>): void
启动图片识码，通过InputImage传入图片信息，通过ScanOptions进行识码参数设置，使用Callback异步回调返回识码结果。
decode(inputImage: InputImage, callback: AsyncCallback<Array<scanBarcode.ScanResult>>): void
启动图片识码，通过InputImage传入图片信息，使用Callback异步回调返回识码结果。
开发步骤
图片识码接口支持识别图库中的条形码，二维码以及MULTIFUNCTIONAL CODE，并返回图片中码图的值，类型以及码的位置信息（码图最小外接矩形左上角和右下角的坐标）。
以下示例为调用图片识码的detectBarcode.decode接口获取码图信息。
```typescript
// 导入图片识码需要的日志和picker模块
import { scanCore, scanBarcode, detectBarcode } from '@kit.ScanKit';
import { photoAccessHelper } from '@kit.MediaLibraryKit';
import { hilog } from '@kit.PerformanceAnalysisKit';
import { BusinessError } from '@kit.BasicServicesKit';
```
```typescript
@Entry
@Component
struct DetectPage {
build() {
Column() {
Button('Promise with options')
.backgroundColor('#0D9FFB')
.fontSize(20)
.fontColor('#FFFFFF')
.fontWeight(FontWeight.Normal)
.align(Alignment.Center)
.type(ButtonType.Capsule)
.width('90%')
.height(40)
.margin({ top: 5, bottom: 5 })
.onClick(() => {
// 定义识码参数options
let options: scanBarcode.ScanOptions = {
scanTypes: [scanCore.ScanType.ALL],
enableMultiMode: true,
}
// 通过picker拉起图库的图片
let photoOption = new photoAccessHelper.PhotoSelectOptions();
photoOption.MIMEType = photoAccessHelper.PhotoViewMIMETypes.IMAGE_TYPE;
photoOption.maxSelectNumber = 1;
let photoPicker = new photoAccessHelper.PhotoViewPicker();
photoPicker.select(photoOption).then((result) => {
// 定义识码参数inputImage，其中uri为picker选择图片
let inputImage: detectBarcode.InputImage = { uri: result.photoUris[0] };
try {
// 调用图片识码接口
detectBarcode.decode(inputImage, options).then((result: Array<scanBarcode.ScanResult>) => {
hilog.info(0x0001, '[Scan Sample]',
`Succeeded in getting ScanResult by promise with options, result is ${JSON.stringify(result)}`);
}).catch((error: BusinessError) => {
hilog.error(0x0001, '[Scan Sample]',
`Failed to get ScanResult by promise with options. Code: ${error.code}, message: ${error.message}`);
});
} catch (error) {
hilog.error(0x0001, '[Scan Sample]',
`Failed to detectBarcode. Code: ${error.code}, message: ${error.message}`);
}
})
});
}
.width('100%')
.height('100%')
.alignItems(HorizontalAlign.Center)
.justifyContent(FlexAlign.Center)
}
}
```
```typescript
@Entry
@Component
struct DetectPage {
build() {
Column() {
Button('Promise with options')
.backgroundColor('#0D9FFB')
.fontSize(20)
.fontColor('#FFFFFF')
.fontWeight(FontWeight.Normal)
.align(Alignment.Center)
.type(ButtonType.Capsule)
.width('90%')
.height(40)
.margin({ top: 5, bottom: 5 })
.onClick(() => {
// 定义识码参数options
let options: scanBarcode.ScanOptions = {
scanTypes: [scanCore.ScanType.ALL],
enableMultiMode: true,
}
// 通过picker拉起图库的图片
let photoOption = new photoAccessHelper.PhotoSelectOptions();
photoOption.MIMEType = photoAccessHelper.PhotoViewMIMETypes.IMAGE_TYPE;
photoOption.maxSelectNumber = 1;
let photoPicker = new photoAccessHelper.PhotoViewPicker();
photoPicker.select(photoOption).then((result) => {
// 定义识码参数inputImage，其中uri为picker选择图片
let inputImage: detectBarcode.InputImage = { uri: result.photoUris[0] };
try {
// 调用图片识码接口
detectBarcode.decode(inputImage, options).then((result: Array<scanBarcode.ScanResult>) => {
hilog.info(0x0001, '[Scan Sample]',
`Succeeded in getting ScanResult by promise with options, result is ${JSON.stringify(result)}`);
}).catch((error: BusinessError) => {
hilog.error(0x0001, '[Scan Sample]',
`Failed to get ScanResult by promise with options. Code: ${error.code}, message: ${error.message}`);
});
} catch (error) {
hilog.error(0x0001, '[Scan Sample]',
`Failed to detectBarcode. Code: ${error.code}, message: ${error.message}`);
}
})
});
}
.width('100%')
.height('100%')
.alignItems(HorizontalAlign.Center)
.justifyContent(FlexAlign.Center)
}
}
```
```typescript
@Entry
@Component
struct DetectPage {
build() {
Column() {
Button('Callback with options')
.backgroundColor('#0D9FFB')
.fontSize(20)
.fontColor('#FFFFFF')
.fontWeight(FontWeight.Normal)
.align(Alignment.Center)
.type(ButtonType.Capsule)
.width('90%')
.height(40)
.margin({ top: 5, bottom: 5 })
.onClick(() => {
// 定义识码参数options
let options: scanBarcode.ScanOptions = {
scanTypes: [scanCore.ScanType.ALL],
enableMultiMode: true,
enableAlbum: true
}
// 通过选择模式拉起photoPicker界面，用户可以选择一个图片
let photoOption = new photoAccessHelper.PhotoSelectOptions();
photoOption.MIMEType = photoAccessHelper.PhotoViewMIMETypes.IMAGE_TYPE;
photoOption.maxSelectNumber = 1;
let photoPicker = new photoAccessHelper.PhotoViewPicker();
photoPicker.select(photoOption).then((result) => {
// 定义识码参数inputImage，其中uri为picker选择图片
let inputImage: detectBarcode.InputImage = { uri: result.photoUris[0] };
try {
// 调用图片识码接口
detectBarcode.decode(inputImage, options,
(error: BusinessError, result: Array<scanBarcode.ScanResult>) => {
if (error && error.code) {
hilog.error(0x0001, '[Scan Sample]',
`Failed to get ScanResult by callback with options. Code: ${error.code}, message: ${error.message}`);
return;
}
hilog.info(0x0001, '[Scan Sample]',
`Succeeded in getting ScanResult by callback with options, result is ${JSON.stringify(result)}`);
});
} catch (error) {
hilog.error(0x0001, '[Scan Sample]',
`Failed to detectBarcode. Code: ${error.code}, message: ${error.message}`);
}
})
});
}
.width('100%')
.height('100%')
.alignItems(HorizontalAlign.Center)
.justifyContent(FlexAlign.Center)
}
}
```
```typescript
@Entry
@Component
struct DetectPage {
build() {
Column() {
Button('Promise with options')
.backgroundColor('#0D9FFB')
.fontSize(20)
.fontColor('#FFFFFF')
.fontWeight(FontWeight.Normal)
.align(Alignment.Center)
.type(ButtonType.Capsule)
.width('90%')
.height(40)
.margin({ top: 5, bottom: 5 })
.onClick(() => {
// 定义识码参数options
let options: scanBarcode.ScanOptions = {
scanTypes: [scanCore.ScanType.ALL],
enableMultiMode: true,
}
// 通过picker拉起图库的图片
let photoOption = new photoAccessHelper.PhotoSelectOptions();
photoOption.MIMEType = photoAccessHelper.PhotoViewMIMETypes.IMAGE_TYPE;
photoOption.maxSelectNumber = 1;
let photoPicker = new photoAccessHelper.PhotoViewPicker();
photoPicker.select(photoOption).then((result) => {
// 定义识码参数inputImage，其中uri为picker选择图片
let inputImage: detectBarcode.InputImage = { uri: result.photoUris[0] };
try {
// 调用图片识码接口
detectBarcode.decode(inputImage, options).then((result: Array<scanBarcode.ScanResult>) => {
hilog.info(0x0001, '[Scan Sample]',
`Succeeded in getting ScanResult by promise with options, result is ${JSON.stringify(result)}`);
}).catch((error: BusinessError) => {
hilog.error(0x0001, '[Scan Sample]',
`Failed to get ScanResult by promise with options. Code: ${error.code}, message: ${error.message}`);
});
} catch (error) {
hilog.error(0x0001, '[Scan Sample]',
`Failed to detectBarcode. Code: ${error.code}, message: ${error.message}`);
}
})
});
}
.width('100%')
.height('100%')
.alignItems(HorizontalAlign.Center)
.justifyContent(FlexAlign.Center)
}
}
```
```typescript
@Entry
@Component
struct DetectPage {
build() {
Column() {
Button('Callback with options')
.backgroundColor('#0D9FFB')
.fontSize(20)
.fontColor('#FFFFFF')
.fontWeight(FontWeight.Normal)
.align(Alignment.Center)
.type(ButtonType.Capsule)
.width('90%')
.height(40)
.margin({ top: 5, bottom: 5 })
.onClick(() => {
// 定义识码参数options
let options: scanBarcode.ScanOptions = {
scanTypes: [scanCore.ScanType.ALL],
enableMultiMode: true,
enableAlbum: true
}
// 通过选择模式拉起photoPicker界面，用户可以选择一个图片
let photoOption = new photoAccessHelper.PhotoSelectOptions();
photoOption.MIMEType = photoAccessHelper.PhotoViewMIMETypes.IMAGE_TYPE;
photoOption.maxSelectNumber = 1;
let photoPicker = new photoAccessHelper.PhotoViewPicker();
photoPicker.select(photoOption).then((result) => {
// 定义识码参数inputImage，其中uri为picker选择图片
let inputImage: detectBarcode.InputImage = { uri: result.photoUris[0] };
try {
// 调用图片识码接口
detectBarcode.decode(inputImage, options,
(error: BusinessError, result: Array<scanBarcode.ScanResult>) => {
if (error && error.code) {
hilog.error(0x0001, '[Scan Sample]',
`Failed to get ScanResult by callback with options. Code: ${error.code}, message: ${error.message}`);
return;
}
hilog.info(0x0001, '[Scan Sample]',
`Succeeded in getting ScanResult by callback with options, result is ${JSON.stringify(result)}`);
});
} catch (error) {
hilog.error(0x0001, '[Scan Sample]',
`Failed to detectBarcode. Code: ${error.code}, message: ${error.message}`);
}
})
});
}
.width('100%')
.height('100%')
.alignItems(HorizontalAlign.Center)
.justifyContent(FlexAlign.Center)
}
}
```
模拟器开发
支持模拟器开发，使用指导请参见使用模拟器运行应用/元服务。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-decodeimage-V14
爬取时间: 2025-04-28 20:49:43
来源: Huawei Developer
基本概念
图像数据识码能力支持对相机预览流数据中的码图进行扫描识别，并获取信息。
场景介绍
图像数据识码能力支持对相机预览流数据中的条形码、二维码、MULTIFUNCTIONAL CODE进行识别，并获得码类型、码值、码位置信息和相机变焦比。该能力可用于一图单码和一图多码的识别，比如条形码、付款码等。
业务流程
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314170001.56847401740651142625747903044710:50001231000000:2800:7CA8372CFB30B77D4C34547223E89D646F5B2A8E7942606E5D49266AE5A00EA6.png)
接口说明
识别图像数据中的码图，以Promise形式返回识别结果。具体API说明详见接口文档。
| 接口名 | 描述 |
| --- | --- |
| decodeImage(image: ByteImage, options?: scanBarcode.ScanOptions): Promise<DetectResult> | 启动图像识码，通过ByteImage传入图像数据信息，使用Promise异步回调返回识码结果。 |
接口名
描述
decodeImage(image:ByteImage, options?: scanBarcode.ScanOptions): Promise<DetectResult>
启动图像识码，通过ByteImage传入图像数据信息，使用Promise异步回调返回识码结果。
开发步骤
图像数据识码能力支持对相机预览流数据中的条形码、二维码、MULTIFUNCTIONAL CODE进行识别，并返回码图的值、类型、码的位置信息（码图最小外接矩形左上角和右下角的坐标，QR码支持返回四个点坐标）和相机变焦比。
以下示例为调用detectBarcode.decodeImage接口获取码图信息。
```typescript
import { detectBarcode, scanBarcode, scanCore } from '@kit.ScanKit';
import { BusinessError } from '@kit.BasicServicesKit';
import { camera } from '@kit.CameraKit';
import { image } from '@kit.ImageKit';
import { hilog } from '@kit.PerformanceAnalysisKit';
```
```typescript
// 从ImageReceiver获取imgComponent: image.Component，预览流设置的宽高: width, height
function decodeImageBuffer(imgComponent: image.Component, width: number, height: number) {
let byteImg: detectBarcode.ByteImage = {
byteBuffer: imgComponent.byteBuffer,
// 相机预览流数据旋转90°
width: height,
height: width,
format: detectBarcode.ImageFormat.NV21
};
let options: scanBarcode.ScanOptions = {
scanTypes: [scanCore.ScanType.ALL],
enableMultiMode: true,
enableAlbum: false
};
try {
detectBarcode.decodeImage(byteImg, options).then((result: detectBarcode.DetectResult) => {
hilog.info(0x0001, '[Scan Sample]',
`Succeeded in getting DetectResult by promise with options, result is ${JSON.stringify(result)}`);
}).catch((error: BusinessError) => {
hilog.error(0x0001, '[Scan Sample]',
`Failed to get DetectResult by promise with options. Code: ${error.code}, message: ${error.message}`);
})
} catch (error) {
hilog.error(0x0001, '[Scan Sample]', `Failed to detectBarcode. Code: ${error.code}, message: ${error.message}`);
}
}
```
模拟器开发
暂不支持模拟器使用，调用会返回错误信息“Emulator is not supported.”

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-generate-V14
爬取时间: 2025-04-28 20:49:56
来源: Huawei Developer

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-barcodegenerate-V14
爬取时间: 2025-04-28 20:50:09
来源: Huawei Developer
基本概念
码图生成能力支持将字符串转换为自定义格式的码图。
场景介绍
码图生成能力支持将字符串转换为自定义格式的码图，包含条形码、二维码生成。
可以将字符串转成联系人码图，手机克隆码图，例如将"HUAWEI"字符串生成码图使用。
约束与限制
支持十三种码图生成，每种码图对生成参数有不同的要求，码图限制可见下表，具体接口参数限制信息请参见CreateOptions。
| 生成码类型 | 参数建议内容 |
| --- | --- |
| QR Code | 支持中文，不超过512字符长度，如果内容过长会导致码复杂，影响识别。 |
| Aztec | 支持中文，不超过512字符长度，如果内容过长会导致码复杂，影响识别。 |
| PDF417 | 支持中文，不超过512字符长度，如果内容过长会导致码复杂，影响识别。 |
| Data Matrix | 不超过512字符长度，如果内容过长会导致码复杂，影响识别。 |
| UPC-A | 支持11位数字输入，只支持数字，生成包含12位数字的码图，包含最后一位校验数字。 |
| UPC-E | 支持7位数字输入，只支持数字，首位需要是0或1，生成包含8位数字的码图，包含最后一位校验数字。 |
| ITF-14 | 支持80位以内数字输入，并且需要是偶数位，只支持数字，生成包含偶数位数字的码图，如果内容过长会导致码复杂，影响识别。 |
| EAN-8 | 支持7位数字输入，只支持数字，生成包含8位数字的码图，包含最后一位校验数字。 |
| EAN-13 | 支持12位数字输入，只支持数字，首位不可以是0，生成包含13位数字的码图，包含最后一位校验数字 |
| Code 39 | 不超过80字节长度，字符集可以是数字、大小写字母和- . $ / + % * SPACE英文格式符号（请注意：一个小写字母占用2个字节）。 |
| Code 93 | 不超过80字节长度，字符集可以是数字、大小写字母和- . $ / + % * SPACE英文格式符号（请注意：一个小写字母占用2个字节）。 |
| Code 128 | 不超过80字节长度，字符集可以是数字、大小写字母和- . $ / + % * SPACE英文格式符号（请注意：一个小写字母占用1个字节）。 |
| Codabar | 不超过512字符长度，起始/终止符可以是ABCD中的任一个（特殊情况下，TN*E也会编码成ABCD，推荐使用ABCD）。其他字符可以是数字和- . $ / : +英文格式符号。 |
生成码类型
参数建议内容
QR Code
支持中文，不超过512字符长度，如果内容过长会导致码复杂，影响识别。
Aztec
支持中文，不超过512字符长度，如果内容过长会导致码复杂，影响识别。
PDF417
支持中文，不超过512字符长度，如果内容过长会导致码复杂，影响识别。
Data Matrix
不超过512字符长度，如果内容过长会导致码复杂，影响识别。
UPC-A
支持11位数字输入，只支持数字，生成包含12位数字的码图，包含最后一位校验数字。
UPC-E
支持7位数字输入，只支持数字，首位需要是0或1，生成包含8位数字的码图，包含最后一位校验数字。
ITF-14
支持80位以内数字输入，并且需要是偶数位，只支持数字，生成包含偶数位数字的码图，如果内容过长会导致码复杂，影响识别。
EAN-8
支持7位数字输入，只支持数字，生成包含8位数字的码图，包含最后一位校验数字。
EAN-13
支持12位数字输入，只支持数字，首位不可以是0，生成包含13位数字的码图，包含最后一位校验数字
Code 39
不超过80字节长度，字符集可以是数字、大小写字母和- . $ / + % * SPACE英文格式符号（请注意：一个小写字母占用2个字节）。
Code 93
不超过80字节长度，字符集可以是数字、大小写字母和- . $ / + % * SPACE英文格式符号（请注意：一个小写字母占用2个字节）。
Code 128
不超过80字节长度，字符集可以是数字、大小写字母和- . $ / + % * SPACE英文格式符号（请注意：一个小写字母占用1个字节）。
Codabar
不超过512字符长度，起始/终止符可以是ABCD中的任一个（特殊情况下，TN*E也会编码成ABCD，推荐使用ABCD）。其他字符可以是数字和- . $ / : +英文格式符号。
生成码参数建议：
-  建议使用默认颜色和背景：黑色码图、白色背景。如果码图颜色和背景对比度较小会影响识别率。
-  建议使用默认边距1，单位：px，取值范围：[1, 10]。
业务流程
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314170001.28154381067848968896653056951543:50001231000000:2800:24722B60E6AC4F2C4186E726BAFEAEF01BE95F276AD7E56513475A9DA9C0B1FC.png)
接口说明
接口返回值有两种返回形式：Callback和Promise回调。下表中为码图生成能力的Callback和Promise形式接口，Callback和Promise只是返回值方式不一样，功能相同。具体API说明详见接口文档。
| 接口名 | 接口描述 |
| --- | --- |
| createBarcode(content: string, options: CreateOptions): Promise<image.PixelMap> | 码图生成接口，返回image.PixelMap类型的参数，可以使用Image组件渲染成图片。使用Promise异步回调返回生成的码图。 |
| createBarcode(content: string, options: CreateOptions, callback: AsyncCallback<image.PixelMap>): void | 码图生成接口，返回image.PixelMap类型的参数，可以使用Image组件渲染成图片。使用Callback异步回调返回生成的码图。 |
接口名
接口描述
createBarcode(content: string, options:CreateOptions): Promise<image.PixelMap>
码图生成接口，返回image.PixelMap类型的参数，可以使用Image组件渲染成图片。使用Promise异步回调返回生成的码图。
createBarcode(content: string, options: CreateOptions, callback: AsyncCallback<image.PixelMap>): void
码图生成接口，返回image.PixelMap类型的参数，可以使用Image组件渲染成图片。使用Callback异步回调返回生成的码图。
开发步骤
码图生成根据传参内容直接生成所需码图，需要传入固定参数和可选参数。
以下示例为调用码图生成能力的createBarcode接口实现码图生成。
```typescript
// 导入码图生成需要的图片模块、错误码模块
import { scanCore, generateBarcode } from '@kit.ScanKit';
import { BusinessError } from '@kit.BasicServicesKit';
import { image } from '@kit.ImageKit';
import { hilog } from '@kit.PerformanceAnalysisKit';
```
```typescript
@Entry
@Component
struct Index {
@State pixelMap: image.PixelMap | undefined = undefined
build() {
Flex({ direction: FlexDirection.Column, alignItems: ItemAlign.Center, justifyContent: FlexAlign.Center }) {
Button('generateBarcode Promise').onClick(() => {
// 以QR码为例，码图生成参数
this.pixelMap = undefined;
let content: string = 'huawei';
let options: generateBarcode.CreateOptions = {
scanType: scanCore.ScanType.QR_CODE,
height: 400,
width: 400
}
try {
// 码图生成接口，成功返回PixelMap格式图片
generateBarcode.createBarcode(content, options).then((pixelMap: image.PixelMap) => {
this.pixelMap = pixelMap;
}).catch((error: BusinessError) => {
hilog.error(0x0001, '[generateBarcode]',
`Failed to get PixelMap by promise with options. Code: ${error.code}, message: ${error.message}`);
})
} catch (error) {
hilog.error(0x0001, '[generateBarcode]',
`Failed to createBarcode by promise with options. Code: ${error.code}, message: ${error.message}`);
}
})
// 获取生成码后显示
if (this.pixelMap) {
Image(this.pixelMap).width(300).height(300).objectFit(ImageFit.Contain)
}
}
.width('100%')
.height('100%')
}
}
```
```typescript
@Entry
@Component
struct Index {
@State pixelMap: image.PixelMap | undefined = undefined
build() {
Flex({ direction: FlexDirection.Column, alignItems: ItemAlign.Center, justifyContent: FlexAlign.Center }) {
Button('generateBarcode Promise').onClick(() => {
// 以QR码为例，码图生成参数
this.pixelMap = undefined;
let content: string = 'huawei';
let options: generateBarcode.CreateOptions = {
scanType: scanCore.ScanType.QR_CODE,
height: 400,
width: 400
}
try {
// 码图生成接口，成功返回PixelMap格式图片
generateBarcode.createBarcode(content, options).then((pixelMap: image.PixelMap) => {
this.pixelMap = pixelMap;
}).catch((error: BusinessError) => {
hilog.error(0x0001, '[generateBarcode]',
`Failed to get PixelMap by promise with options. Code: ${error.code}, message: ${error.message}`);
})
} catch (error) {
hilog.error(0x0001, '[generateBarcode]',
`Failed to createBarcode by promise with options. Code: ${error.code}, message: ${error.message}`);
}
})
// 获取生成码后显示
if (this.pixelMap) {
Image(this.pixelMap).width(300).height(300).objectFit(ImageFit.Contain)
}
}
.width('100%')
.height('100%')
}
}
```
```typescript
@Entry
@Component
struct Index {
@State pixelMap: image.PixelMap | undefined = undefined
build() {
Flex({ direction: FlexDirection.Column, alignItems: ItemAlign.Center, justifyContent: FlexAlign.Center }) {
Button('generateBarcode Callback').onClick(() => {
// 以QR码为例，码图生成参数
let content = 'huawei';
let options: generateBarcode.CreateOptions = {
scanType: scanCore.ScanType.QR_CODE,
height: 400,
width: 400
}
try {
// 码图生成接口，成功返回PixelMap格式图片
generateBarcode.createBarcode(content, options, (error: BusinessError, pixelMap: image.PixelMap) => {
if (error) {
hilog.error(0x0001, '[generateBarcode]',
`Failed to get PixelMap by callback with options. Code: ${error.code}, message: ${error.message}`);
return;
}
this.pixelMap = pixelMap;
})
} catch (error) {
hilog.error(0x0001, '[generateBarcode]',
`Failed to createBarcode by callback with options. Code: ${error.code}, message: ${error.message}`);
}
})
// 获取生成码后显示
if (this.pixelMap) {
Image(this.pixelMap).width(300).height(300).objectFit(ImageFit.Contain)
}
}
.width('100%')
.height('100%')
}
}
```
```typescript
@Entry
@Component
struct Index {
@State pixelMap: image.PixelMap | undefined = undefined
build() {
Flex({ direction: FlexDirection.Column, alignItems: ItemAlign.Center, justifyContent: FlexAlign.Center }) {
Button('generateBarcode Promise').onClick(() => {
// 以QR码为例，码图生成参数
this.pixelMap = undefined;
let content: string = 'huawei';
let options: generateBarcode.CreateOptions = {
scanType: scanCore.ScanType.QR_CODE,
height: 400,
width: 400
}
try {
// 码图生成接口，成功返回PixelMap格式图片
generateBarcode.createBarcode(content, options).then((pixelMap: image.PixelMap) => {
this.pixelMap = pixelMap;
}).catch((error: BusinessError) => {
hilog.error(0x0001, '[generateBarcode]',
`Failed to get PixelMap by promise with options. Code: ${error.code}, message: ${error.message}`);
})
} catch (error) {
hilog.error(0x0001, '[generateBarcode]',
`Failed to createBarcode by promise with options. Code: ${error.code}, message: ${error.message}`);
}
})
// 获取生成码后显示
if (this.pixelMap) {
Image(this.pixelMap).width(300).height(300).objectFit(ImageFit.Contain)
}
}
.width('100%')
.height('100%')
}
}
```
```typescript
@Entry
@Component
struct Index {
@State pixelMap: image.PixelMap | undefined = undefined
build() {
Flex({ direction: FlexDirection.Column, alignItems: ItemAlign.Center, justifyContent: FlexAlign.Center }) {
Button('generateBarcode Callback').onClick(() => {
// 以QR码为例，码图生成参数
let content = 'huawei';
let options: generateBarcode.CreateOptions = {
scanType: scanCore.ScanType.QR_CODE,
height: 400,
width: 400
}
try {
// 码图生成接口，成功返回PixelMap格式图片
generateBarcode.createBarcode(content, options, (error: BusinessError, pixelMap: image.PixelMap) => {
if (error) {
hilog.error(0x0001, '[generateBarcode]',
`Failed to get PixelMap by callback with options. Code: ${error.code}, message: ${error.message}`);
return;
}
this.pixelMap = pixelMap;
})
} catch (error) {
hilog.error(0x0001, '[generateBarcode]',
`Failed to createBarcode by callback with options. Code: ${error.code}, message: ${error.message}`);
}
})
// 获取生成码后显示
if (this.pixelMap) {
Image(this.pixelMap).width(300).height(300).objectFit(ImageFit.Contain)
}
}
.width('100%')
.height('100%')
}
}
```
模拟器开发
暂不支持模拟器使用，调用会返回错误信息“Emulator is not supported.”

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-generatearray-V14
爬取时间: 2025-04-28 20:50:23
来源: Huawei Developer
基本概念
码图生成能力支持将字节数组转换为自定义格式的码图。
场景介绍
码图生成能力支持将字节数组转换为自定义格式的码图。
例如：调用码图生成能力, 将字节数组转换成交通一卡通二维码使用。
约束与限制
只支持QR Code生成，根据纠错水平不同对生成参数有不同的要求，参数限制可见下表，具体接口参数限制信息请参见CreateOptions。
Scan Kit识别该码图内容显示内容为乱码，这种字节数组需要专门的解码器解析，例如地铁闸机。
| 纠错水平 | 参数内容限制 |
| --- | --- |
| LEVEL_L | 字节数组长度限制建议不超过2048。 |
| LEVEL_M | 字节数组长度限制建议不超过2048。 |
| LEVEL_Q | 字节数组长度限制建议不超过1536。 |
| LEVEL_H | 字节数组长度限制建议不超过1024。 |
纠错水平
参数内容限制
LEVEL_L
字节数组长度限制建议不超过2048。
LEVEL_M
字节数组长度限制建议不超过2048。
LEVEL_Q
字节数组长度限制建议不超过1536。
LEVEL_H
字节数组长度限制建议不超过1024。
生成码参数建议：
-  建议使用默认颜色和背景：黑色码图、白色背景。如果码图颜色和背景对比度较小会影响识别率。
-  建议使用默认边距1，单位：px，取值范围：[1, 10]。
-  输入的width和height值相同且均大于等于200小于等于4096，单位：px，否则生成的码图过小会影响识别。
业务流程
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314170001.10414546373947103873377240727297:50001231000000:2800:CB42480E5FAD70E71767D75D4C6880659B87DAF7926C1E792B7FFE8553CB6014.png)
接口说明
通过字节数组生成码图，以Promise形式生成码图。具体API说明详见接口文档。
| 接口名 | 接口描述 |
| --- | --- |
| createBarcode(content: ArrayBuffer, options: CreateOptions): Promise<image.PixelMap> | 码图生成接口，返回image.PixelMap类型的参数，可以使用Image组件渲染成图片。使用Promise异步回调返回生成的码图。 |
接口名
接口描述
createBarcode(content: ArrayBuffer, options:CreateOptions): Promise<image.PixelMap>
码图生成接口，返回image.PixelMap类型的参数，可以使用Image组件渲染成图片。使用Promise异步回调返回生成的码图。
开发步骤
码图生成根据传参内容直接生成所需码图，需要传入固定参数和可选参数。
以下示例为调用码图生成能力的createBarcode接口实现码图生成。
```typescript
// 导入码图生成需要的图片模块、错误码模块
import { scanCore, generateBarcode } from '@kit.ScanKit';
import { BusinessError } from '@kit.BasicServicesKit';
import { image } from '@kit.ImageKit';
import { hilog } from '@kit.PerformanceAnalysisKit';
import { buffer } from '@kit.ArkTS';
```
```typescript
const TAG: string = 'Create barcode';
@Entry
@Component
struct Index {
@State pixelMap: image.PixelMap | undefined = undefined
build() {
Flex({ direction: FlexDirection.Column, alignItems: ItemAlign.Center, justifyContent: FlexAlign.Center }) {
Button('generateBarcode Promise').onClick(() => {
this.pixelMap = undefined;
let content: string =
'0177C10DD10F7768600202312110000063458FD14112345678FFFFD381012610b746365409210201b66636540ad0200020000000000110e617003201000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000006645fbec664358ECF657CB40693c92da';
let contentBuffer: ArrayBuffer = buffer.from(content, 'hex').buffer; // 通过包含十六进制字符的字符串创建Buffer
let options: generateBarcode.CreateOptions = {
scanType: scanCore.ScanType.QR_CODE,
height: 400,
width: 400
}
try {
// 码图生成接口，成功返回PixelMap格式图片
generateBarcode.createBarcode(contentBuffer, options).then((pixelMap: image.PixelMap) => {
this.pixelMap = pixelMap;
hilog.info(0x0001, TAG, 'Succeeded in creating barCode.');
}).catch((error: BusinessError) => {
hilog.error(0x0001, TAG, `Failed to createBarCode. Code: ${error.code}, message: ${error.message}`);
})
} catch (error) {
hilog.error(0x0001, TAG,
`Failed to createBarcode by Promise with options. Code: ${error.code}, message: ${error.message}`);
}
})
// 获取生成码后显示
if (this.pixelMap) {
Image(this.pixelMap).width(300).height(300).objectFit(ImageFit.Contain)
}
}
.width('100%')
.height('100%')
}
}
```
```typescript
const TAG: string = 'Create barcode';
@Entry
@Component
struct Index {
@State pixelMap: image.PixelMap | undefined = undefined
build() {
Flex({ direction: FlexDirection.Column, alignItems: ItemAlign.Center, justifyContent: FlexAlign.Center }) {
Button('generateBarcode Promise').onClick(() => {
this.pixelMap = undefined;
let content: string =
'0177C10DD10F7768600202312110000063458FD14112345678FFFFD381012610b746365409210201b66636540ad0200020000000000110e617003201000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000006645fbec664358ECF657CB40693c92da';
let contentBuffer: ArrayBuffer = buffer.from(content, 'hex').buffer; // 通过包含十六进制字符的字符串创建Buffer
let options: generateBarcode.CreateOptions = {
scanType: scanCore.ScanType.QR_CODE,
height: 400,
width: 400
}
try {
// 码图生成接口，成功返回PixelMap格式图片
generateBarcode.createBarcode(contentBuffer, options).then((pixelMap: image.PixelMap) => {
this.pixelMap = pixelMap;
hilog.info(0x0001, TAG, 'Succeeded in creating barCode.');
}).catch((error: BusinessError) => {
hilog.error(0x0001, TAG, `Failed to createBarCode. Code: ${error.code}, message: ${error.message}`);
})
} catch (error) {
hilog.error(0x0001, TAG,
`Failed to createBarcode by Promise with options. Code: ${error.code}, message: ${error.message}`);
}
})
// 获取生成码后显示
if (this.pixelMap) {
Image(this.pixelMap).width(300).height(300).objectFit(ImageFit.Contain)
}
}
.width('100%')
.height('100%')
}
}
```
```typescript
const TAG: string = 'Create barcode';
@Entry
@Component
struct Index {
@State pixelMap: image.PixelMap | undefined = undefined
build() {
Flex({ direction: FlexDirection.Column, alignItems: ItemAlign.Center, justifyContent: FlexAlign.Center }) {
Button('generateBarcode Promise').onClick(() => {
this.pixelMap = undefined;
let content: string =
'0177C10DD10F7768600202312110000063458FD14112345678FFFFD381012610b746365409210201b66636540ad0200020000000000110e617003201000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000006645fbec664358ECF657CB40693c92da';
let contentBuffer: ArrayBuffer = buffer.from(content, 'hex').buffer; // 通过包含十六进制字符的字符串创建Buffer
let options: generateBarcode.CreateOptions = {
scanType: scanCore.ScanType.QR_CODE,
height: 400,
width: 400
}
try {
// 码图生成接口，成功返回PixelMap格式图片
generateBarcode.createBarcode(contentBuffer, options).then((pixelMap: image.PixelMap) => {
this.pixelMap = pixelMap;
hilog.info(0x0001, TAG, 'Succeeded in creating barCode.');
}).catch((error: BusinessError) => {
hilog.error(0x0001, TAG, `Failed to createBarCode. Code: ${error.code}, message: ${error.message}`);
})
} catch (error) {
hilog.error(0x0001, TAG,
`Failed to createBarcode by Promise with options. Code: ${error.code}, message: ${error.message}`);
}
})
// 获取生成码后显示
if (this.pixelMap) {
Image(this.pixelMap).width(300).height(300).objectFit(ImageFit.Contain)
}
}
.width('100%')
.height('100%')
}
}
```
模拟器开发
暂不支持模拟器使用，调用会返回错误信息“Emulator is not supported.”

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-faq-1-V14
爬取时间: 2025-04-28 20:52:29
来源: Huawei Developer
问题现象
有些用户编辑过控制中心，删除了默认存在的“扫一扫”入口，后续如何添加。
解决措施
控制中心编辑添加“扫一扫”入口：手机下滑菜单栏，打开控制中心，在编辑区域中添加选择“扫一扫”后保存，即可在下拉控制列表中找到。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-faq-2-V14
爬取时间: 2025-04-28 20:52:42
来源: Huawei Developer
问题现象
扫码直达跳转失败。
解决措施
请检查App Linking配置是否正确：
详情参考：App Linking的FAQ。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-faq-3-V14
爬取时间: 2025-04-28 20:52:56
来源: Huawei Developer
问题现象
实时扫描多个码图时，只返回一个码图结果。
解决措施

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-faq-4-V14
爬取时间: 2025-04-28 20:53:10
来源: Huawei Developer
问题现象
在进行应用上架操作中，上传软件包时，AGC平台提示“上传的软件包与声明支持设备不一致，请重新上传或修改可支持设备”。
解决措施

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-faq-5-V14
爬取时间: 2025-04-28 20:53:23
来源: Huawei Developer
问题现象
启动默认界面进行扫码，当开启相册扫码识别多码时，识别失败。
解决措施
相册扫码只支持单码识别。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-faq-7-V14
爬取时间: 2025-04-28 20:54:50
来源: Huawei Developer
问题现象
条形码识别场景下，存在识别成功后，返回位置信息为空的现象。
解决措施
因为条形码识别逻辑，算法返回的位置信息可能在同一行或者同一列，无法返回外接矩形，该场景下需要开发者判断位置信息是否为空，进行对应处理。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-faq-8-V14
爬取时间: 2025-04-28 20:55:04
来源: Huawei Developer
问题现象
XComponent的宽高比与自定义界面扫码接口中ViewControl的宽高比不一致，导致自定义界面扫码预览画面出现拉伸。
解决措施
ViewControl的宽高比需要与XComponent的宽高比保持一致，会消除画面拉伸现象。当前支持的分辨率比例为16:9、4:3、1:1。
例如：XComponent中width为1080(px)，height为1920(px)，则ViewControl宽度设置为1080，高度设置为1920。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-faq-9-V14
爬取时间: 2025-04-28 20:55:17
来源: Huawei Developer
问题现象
自定义启动相机却显示黑屏现象。
解决措施
1.  切换至后台时，在页面生命周期onPageHide中先暂停并释放相机流（customScan.stop、customScan.release）。重新切换到前台后，在页面生命周期onPageShow中再重启相机流（customScan.start）。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-faq-10-V14
爬取时间: 2025-04-28 20:55:31
来源: Huawei Developer
问题现象
自定义界面扫码扫到码值后，如何连续扫码？
解决措施
customScan.rescan可以重新触发一次扫码，必须在customScan.start(viewControl, callback)方法Callback接口回调中有效，Promise方式无效。
示例：
```typescript
import { AsyncCallback, BusinessError } from '@kit.BasicServicesKit';
import { hilog } from '@kit.PerformanceAnalysisKit';
import { customScan, scanBarcode } from '@kit.ScanKit';
@Entry
@Component
struct Index {
private callback: AsyncCallback<Array<scanBarcode.ScanResult>> =
async (error: BusinessError, result: Array<scanBarcode.ScanResult>) => {
if (error) {
hilog.error(0x0001, '[Scan Sample]',
`Failed to get ScanResult by callback. Code: ${error.code}, message: ${error.message}`);
return;
}
hilog.info(0x0001, '[Scan Sample]',
`Succeeded in getting ScanResult by callback, result is ${JSON.stringify(result)}`);
try {
// 重新触发扫码：不需要重启相机并重新触发一次扫码，可以在start接口的Callback异步回调中，调用rescan接口。
customScan.rescan();
} catch (error) {
hilog.error(0x0001, '[Scan Sample]', `Failed to rescan. Code: ${error.code}, message: ${error.message}`);
}
}
build() {
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-faq-11-V14
爬取时间: 2025-04-28 20:55:45
来源: Huawei Developer
问题现象
Scan Kit识别该码图内容显示内容为乱码，无法解析。
解决措施
通过字节数组生成码图，Scan Kit识别该码图内容显示内容为乱码，这种字节数组需要专门的解码器解析，例如地铁闸机。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-faq-12-V14
爬取时间: 2025-04-28 20:55:58
来源: Huawei Developer
问题现象
调用默认界面扫码功能，没有扫码直接关闭，如何在逻辑中判断？
解决措施
开启扫码，却没有进行任何扫码操作而直接取消扫码，可以从回调中获取返回错误码：1000500002，用户取消扫码，据此自行修改逻辑操作。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-faq-13-V14
爬取时间: 2025-04-28 20:56:12
来源: Huawei Developer
问题现象
Scan Kit没有提供H5的方案接入扫码。
解决措施
开发者可参考前端页面调用应用侧函数方式调用扫码接口，实现网页扫码功能。

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-faq-14-V14
爬取时间: 2025-04-28 20:56:26
来源: Huawei Developer
问题现象
自定义界面扫码如何主动通过手势缩放相机流。
解决措施
通过组合手势接口设置变焦比setZoom(zoomValue : number): void。
参考下面示例代码，手势缩放跟随和手势缩放结束接口中都可以设置变焦比变化：
```typescript
import { hilog } from '@kit.PerformanceAnalysisKit';
@Entry
@Component
struct Index {
build() {
Column() {
// 绑定手势
}.gesture(PinchGesture({ fingers: 2 })
.onActionStart((event: GestureEvent) => {
hilog.info(0x0001, '[Scan Sample]', 'Pinch start');
})
.onActionUpdate((event: GestureEvent) => {
if (event) {
// 手势缩放比例
let scaleValue = event.scale;
// 1、手势跟随过程中设置变焦比
}
})
.onActionEnd((event: GestureEvent) => {
// 2、手势结束抬起后，设置变焦比
})
)
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-faq-15-V14
爬取时间: 2025-04-28 20:56:39
来源: Huawei Developer
问题现象
扫码界面没有类似扫码框呈现。
解决措施
示例代码（仅供参考）：
```typescript
import { customScan, scanBarcode } from '@kit.ScanKit';
import { hilog } from '@kit.PerformanceAnalysisKit';
import { BusinessError } from '@kit.BasicServicesKit';
// 例如XComponent设置的宽高为cameraWidth = 1080px, cameraHeight = 1920px
let cameraWidth = 1080;
let cameraHeight = 1920;
// 自定义扫码框在屏幕中间 scanBox 为800px*800px，则扫码框相对xComponent的坐标left: 140px, top: 560px, right: 940px, bottom: 1360px
let scanBoxWidth = 800;
let scanBoxHeight = 800;
let scanBox: scanBarcode.ScanCodeRect = {
left: (cameraWidth - scanBoxWidth) / 2,
top: (cameraHeight - scanBoxHeight) / 2,
right: (cameraWidth + scanBoxWidth) / 2,
bottom: (cameraHeight + scanBoxHeight) / 2
}
// 设置ViewControl参数
let viewControl: customScan.ViewControl = {
width: cameraWidth,
height: cameraHeight,
surfaceId: '123' // mock数据，实际需要从组件生成获取
};
try {
customScan.start(viewControl, async (error: BusinessError, result: Array<scanBarcode.ScanResult>) => {
if (error) {
// 扫码识别失败
return;
}
// 例如：mock扫码结果返回的结果为第一个result[0] = { left: 150px, top: 400px, right: 450px, bottom: 700px }
let scanResult: scanBarcode.ScanCodeRect = {
left: 150,
top: 400,
right: 450,
bottom: 700
};
// 判断码图位置是否位于扫码框范围内
if (scanResult.left >= scanBox.left && scanResult.top >= scanBox.top && scanResult.right <= scanBox.right &&
scanResult.bottom <= scanBox.bottom) {
// 扫码成功，码图位置位于扫码框范围，根据业务需求处理扫码结果
} else {
// 码图位置不在扫码框范围，继续扫码
try {
customScan.rescan();
} catch (error) {
hilog.error(0x0001, '[Scan Sample]', `Failed to rescan. Code: ${error.code}, message: ${error.message}`);
}
}
});
} catch (error) {
hilog.error(0x0001, '[Scan Sample]', `Failed to start customScan. Code: ${error.code}, message: ${error.message}`);
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-faq-16-V14
爬取时间: 2025-04-28 20:56:53
来源: Huawei Developer
问题现象
点击按钮后，无法确认是否正在拉起扫码功能。
解决措施
点击按钮拉起默认扫码界面或自定义扫码界面后，将按钮置灰，说明正在拉起扫码功能，以“默认界面扫码”按钮为例：
![Image](https://alliance-communityfile-drcn.dbankcdn.com/FileServer/getFile/cmtyPub/011/111/111/0000000000011111111.20250314170002.07101781088386577695626773455764:50001231000000:2800:B191D495C0795E1A2066F6CF989AE9D79C231D8F71DA92D3E41D52DEB26D4B7E.png)

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-faq-17-V14
爬取时间: 2025-04-28 20:57:07
来源: Huawei Developer
问题现象
调用customScan.init成功后，调用customScan.start启动相机流时抛出1000500001内部错误。
解决措施
可以尝试增加扫码相机流重试机制。
先暂停并释放相机流（customScan.stop、customScan.release），再重启相机流（customScan.init、customScan.start）。
示例代码（仅供参考）：
```typescript
import { customScan, scanBarcode, scanCore } from '@kit.ScanKit';
import { AsyncCallback, BusinessError } from '@kit.BasicServicesKit';
import { hilog } from '@kit.PerformanceAnalysisKit';
@Entry
@Component
struct Index {
@State viewControl: customScan.ViewControl = {
width: 1080,
height: 1080,
surfaceId: ''
};
private retryScanTimes = 0;
private options: scanBarcode.ScanOptions = {
scanTypes: [scanCore.ScanType.ALL],
enableMultiMode: true,
enableAlbum: true
};
private customScanCallbackScan: AsyncCallback<scanBarcode.ScanResult[]> =
async (error: BusinessError, result: scanBarcode.ScanResult[]) => {
if (error && error.code !== 0) {
hilog.error(0x0001, '[Scan Sample]',
`An error is returned by customScan.start->CallbackScan. Code: ${error.code}`);
// start回调，出现1000500001内部错误时触发重启相机流
if (error.code === scanCore.ScanErrorCode.INTERNAL_ERROR) {
this.retryCamera(error);
}
}
// 识码处理逻辑
// ...
}
// 重启相机流
retryCamera(error: BusinessError) {
if (this.retryScanTimes < 3 && error.code === scanCore.ScanErrorCode.INTERNAL_ERROR) {
this.retryScanTimes++;
let timeId = setTimeout(async () => {
hilog.info(0x0001, '[Scan Sample]',
`Retry camera start. Times: ${this.retryScanTimes}.`);
// 先暂停并释放相机流
await this.releaseCamera();
// 重启相机流
this.startCamera();
hilog.info(0x0001, '[Scan Sample]', 'Retry camera end.');
clearTimeout(timeId);
}, 100)
}
}
// 启动相机流
startCamera() {
try {
customScan.init(this.options);
hilog.info(0x0001, '[Scan Sample]', 'customScan->init end');
try {
customScan.start(this.viewControl, this.customScanCallbackScan);
hilog.info(0x0001, '[Scan Sample]', 'customScan->start end');
} catch (error) {
hilog.error(0x0001, '[Scan Sample]',
`Failed to customScan->start. Code: ${error.code}, message: ${error.message}`);
}
} catch (error) {
hilog.error(0x0001, '[Scan Sample]',
`Failed to customScan->init. Code: ${error.code}, message: ${error.message}`);
}
}
// 暂停并释放相机流
async releaseCamera() {
try {
await customScan.stop();
hilog.info(0x0001, '[Scan Sample]', 'customScan->stop end');
try {
await customScan.release();
hilog.info(0x0001, '[Scan Sample]', 'customScan->release end');
} catch (error) {
hilog.error(0x0001, '[Scan Sample]',
`Failed to customScan->release. Code: ${error.code}, message: ${error.message}`);
}
} catch (error) {
hilog.error(0x0001, '[Scan Sample]',
`Failed to customScan->stop. Code: ${error.code}, message: ${error.message}`);
}
}
build() {
}
}
```

URL: https://developer.huawei.com/consumer/cn/doc/harmonyos-guides-V14/scan-personal-data-V14
爬取时间: 2025-04-28 20:57:20
来源: Huawei Developer
最后修改时间：2024/6/25
此文档针对华为作为最终用户数据处理者，开发者作为最终用户数据控制者的数据处理进行说明，包括：
华为处理的个人数据清单
| 个人数据清单  | 使用目的  | 存留期  |
| --- | --- | --- |
| 您主动提交的图片  | 在使用扫码功能的图片扫码场景时，用于读取需要处理的图片。在使用扫码功能的相机扫码场景时，用于拍取包含码值的图片。  | 读取需要处理的图片不涉及存储。拍取包含码值的图片不涉及存储。  |
| 您主动提交的文本  | 在使用码图生成功能的场景时，用于读取需要生成码图的文本。  | 读取生成码图的文本不涉及存储。  |
| 传感器信息（加速度传感器、光照传感器）  | 加速度传感器用于感知您设备横竖屏切换的状态，为您调整扫码界面的方向，光照传感器用于暗光下打开手电筒，为您提供更好的扫码体验，以上个人信息均在端侧使用。  | 传感器信息不涉及存储。  |
个人数据清单
使用目的
存留期
您主动提交的图片
您主动提交的文本
在使用码图生成功能的场景时，用于读取需要生成码图的文本。
读取生成码图的文本不涉及存储。
传感器信息（加速度传感器、光照传感器）
加速度传感器用于感知您设备横竖屏切换的状态，为您调整扫码界面的方向，光照传感器用于暗光下打开手电筒，为您提供更好的扫码体验，以上个人信息均在端侧使用。
传感器信息不涉及存储。
指导开发者如何帮助最终用户实现对数据的控制
-  不涉及，Scan Kit API皆不涉及存储。
-  不涉及，Scan Kit API皆不涉及存储。

